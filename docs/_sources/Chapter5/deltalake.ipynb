{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442fd2ba",
   "metadata": {},
   "source": [
    "## Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccae85e",
   "metadata": {},
   "source": [
    "### Version Your Pandas DataFrame with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee7cba",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deltalake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2bb4d7",
   "metadata": {},
   "source": [
    "Versioning your data is essential to undoing mistakes, preventing data loss, and ensuring reproducibility. Delta Lake makes it easy to version pandas DataFrames and review past changes for auditing and debugging purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e54654",
   "metadata": {},
   "source": [
    "To version a pandas DataFrame with Delta Lake, start with writing out a pandas DataFrame to a Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e58706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from deltalake.writer import write_deltalake\n",
    "\n",
    "df = pd.DataFrame({\"x\": [1, 2, 3]})\n",
    "\n",
    "# Write to a delta table \n",
    "table = \"delta_lake\"\n",
    "os.makedirs(table, exist_ok=True)\n",
    "write_deltalake(table, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba332a2",
   "metadata": {},
   "source": [
    "Delta Lake stores the data in a Parquet file and maintains a transaction log that records the data operations, enabling time travel and versioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e05c6",
   "metadata": {},
   "source": [
    "```bash\n",
    "delta_lake:\n",
    "\n",
    " ├──  0-4719861e-1d3a-49f8-8870-225e4e46e3a0-0.parquet  \n",
    " └──  _delta_log/ \n",
    " │  └────  00000000000000000000.json  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ec12e8",
   "metadata": {},
   "source": [
    "To load the Delta table as a pandas DataFrame, simply use the `DeltaTable` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45469b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "\n",
    "dt = DeltaTable(table)\n",
    "dt.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6652b712",
   "metadata": {},
   "source": [
    "Let's see what happens when we append another pandas DataFrame to the Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861dfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({\"x\": [8, 9, 10]})\n",
    "\n",
    "write_deltalake(table, df2, mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb92f2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delta table\n",
    "dt = DeltaTable(table)\n",
    "dt.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad716ba2",
   "metadata": {},
   "source": [
    "Our Delta table now has two versions. Version 0 contains the initial data and Version 1 includes the data that was appended.\n",
    "\n",
    "![](../img/delta_lake.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56521597",
   "metadata": {},
   "source": [
    "To get the metadata of files that currently make up the current table such as creation time, size, and statistics, call the `get_add_actions` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d86f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.get_add_actions(flatten=True).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a2bba",
   "metadata": {},
   "source": [
    "To access prior versions, simply specify the version number when loading the Delta table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33e53d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Version 0 of the dataset\n",
    "dt0 = DeltaTable(table, version=0)\n",
    "dt0.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6285347",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta-rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1c192b",
   "metadata": {},
   "source": [
    "### Beyond Parquet: Reliable Data Storage with Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb0c6f0",
   "metadata": {},
   "source": [
    "Traditional data storage methods, such as plain Parquet files, are susceptible to partial failures during write operations. This can result in incomplete data files and a lack of clear recovery options in the event of a system crash.\n",
    "\n",
    "Delta Lake's write operation with ACID transactions helps solve this by:\n",
    "- Ensuring either all data is written successfully or none of it is\n",
    "- Maintaining a transaction log that tracks all changes\n",
    "- Providing time travel capabilities to recover from failures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7183496",
   "metadata": {},
   "source": [
    "Here's an example showing Delta Lake's reliable write operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25030817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import write_deltalake, DeltaTable\n",
    "import pandas as pd\n",
    "\n",
    "initial_data = pd.DataFrame({\n",
    "    \"id\": [1, 2],\n",
    "    \"value\": [\"a\", \"b\"]\n",
    "})\n",
    "\n",
    "write_deltalake(\"customers\", initial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da089da",
   "metadata": {},
   "source": [
    "If the append operation fails halfway, Delta Lake's transaction log ensures that the table remains in its last valid state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aae22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Simulate a large append that fails halfway\n",
    "    new_data = pd.DataFrame({\n",
    "        \"id\": range(3, 1003),  # 1000 new rows\n",
    "        \"value\": [\"error\"] * 1000\n",
    "    })\n",
    "    \n",
    "    # Simulate system crash during append\n",
    "    raise Exception(\"System crash during append!\")\n",
    "    write_deltalake(\"customers\", new_data, mode=\"append\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Write failed: {e}\")\n",
    "    \n",
    "    # Check table state - still contains only initial data\n",
    "    dt = DeltaTable(\"customers\")\n",
    "    print(\"\\nTable state after failed append:\")\n",
    "    print(dt.to_pandas())\n",
    "    \n",
    "    # Verify version history\n",
    "    print(f\"\\nCurrent version: {dt.version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b44938",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta-rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f80a2",
   "metadata": {},
   "source": [
    "### Optimize Query Speed with Data Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad937ed",
   "metadata": {},
   "source": [
    "Partitioning data allows queries to target specific segments rather than scanning the entire table, which speeds up data retrieval.\n",
    "\n",
    "The following code uses Delta Lake to select partitions from a pandas DataFrame. Partitioned data loading is approximately 24.5 times faster than loading the complete dataset and then querying a particular subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc10668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deltalake.writer import write_deltalake\n",
    "from deltalake import DeltaTable\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f11445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with hourly sales data for 2 million records\n",
    "np.random.seed(0)  # For reproducibility\n",
    "\n",
    "start_date = datetime(2023, 1, 1)\n",
    "end_date = datetime(2023, 8, 31)\n",
    "date_range = pd.date_range(start_date, end_date, freq='H')\n",
    "\n",
    "data = {\n",
    "    'datetime': date_range,\n",
    "    'value': np.random.randint(100, 1000, len(date_range))\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['day'] = df['datetime'].dt.day\n",
    "df['hour'] = df['datetime'].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea19ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"month\", \"day\", \"hour\", \"value\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43984d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a Delta table\n",
    "table_path = 'delta_lake'\n",
    "write_deltalake(table_path, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba3e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Load the data from the Delta table\n",
    "DeltaTable(table_path).to_pandas().query(\"month == 1 & day == 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to a Delta table\n",
    "table_path = \"delta_lake2\"\n",
    "write_deltalake(table_path, df, partition_by=[\"month\", \"day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bdc262",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Load the data from the Delta table\n",
    "DeltaTable(table_path).to_pandas([(\"month\", \"=\", \"1\"), (\"day\", \"=\", \"1\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621c8151",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta-rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9cb135",
   "metadata": {},
   "source": [
    "### Overwrite Partitions of a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e349b0e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deltalake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9286331",
   "metadata": {},
   "source": [
    "If you need to modify a specific subset of your pandas DataFrame, such as yesterday's data, it is not possible to overwrite only that partition. Instead, you have to load the entire DataFrame into memory as a workaround solution.\n",
    "\n",
    "Delta Lake makes it easy to overwrite partitions of a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c83a7a",
   "metadata": {},
   "source": [
    "First, write out a pandas DataFrame as a Delta table that is partitioned by the `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deltalake.writer import write_deltalake\n",
    "from deltalake import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2168804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = \"tmp/records\" \n",
    "df = pd.DataFrame(\n",
    "    {\"a\": [1, 2, 3], \"date\": [\"04-21\", \"04-22\", \"04-22\"]}\n",
    ")\n",
    "write_deltalake(\n",
    "    table_path,\n",
    "    df,\n",
    "    partition_by=[\"date\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a156c416",
   "metadata": {},
   "source": [
    "The Delta table's contents are partitioned by date, with each partition represented by a directory\n",
    " \n",
    "```bash\n",
    " └──  _delta_log/ \n",
    " │  └────  00000000000000000000.json  \n",
    " └──  date=04-21/ \n",
    " │  └────  0-a6813d0c-157b-4ca6-8b3c-8d5afd51947c-0.parquet  \n",
    " └──  date=04-22/ \n",
    " │  └────  0-a6813d0c-157b-4ca6-8b3c-8d5afd51947c-0.parquet  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb798e9",
   "metadata": {},
   "source": [
    "View the Delta table as a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196309e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable(table_path).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df57f2",
   "metadata": {},
   "source": [
    "Next, create another DataFrame with two other records on 04-22. Overwrite the 04-22 partition with the new DataFrame and leave other partitions untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4af8bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\"a\": [7, 8], \"date\": [\"04-22\", \"04-22\"]}\n",
    ")\n",
    "write_deltalake(\n",
    "    table_path,\n",
    "    df,\n",
    "    mode=\"overwrite\",\n",
    "    partition_filters=[(\"date\", \"=\", \"04-22\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable(table_path).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636daab",
   "metadata": {},
   "source": [
    "Here is the updated contents of the Delta table:\n",
    "\n",
    "```bash\n",
    " └──  _delta_log/ \n",
    " │  └────  00000000000000000000.json\n",
    " │  └────  00000000000000000001.json    \n",
    " └──  date=04-21/ \n",
    " │  └────  0-a6813d0c-157b-4ca6-8b3c-8d5afd51947c-0.parquet  \n",
    " └──  date=04-22/ \n",
    " │  ├────  0-a6813d0c-157b-4ca6-8b3c-8d5afd51947c-0.parquet  \n",
    " │  └────  1-b5c9640f-f386-4754-b28f-90e361ab4320-0.parquet \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57662ad",
   "metadata": {},
   "source": [
    "Since the data files are not physically removed from disk, you can time travel to the initial version of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cecfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DeltaTable(table_path, version=0).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779a9bf2",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta-rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762335c",
   "metadata": {},
   "source": [
    "### Efficient Data Appending in Parquet Files: Delta Lake vs. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff99ad6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deltalake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e38e130",
   "metadata": {},
   "source": [
    "Appending data to an existing Parquet file using pandas involves:\n",
    "- Loading the entire existing table into memory.\n",
    "- Merging the new data with the existing table.\n",
    "- Writing the merged data to the existing file.\n",
    "\n",
    "This process can be time-consuming and memory-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056396b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "df1 = pd.DataFrame([\n",
    "    (1, \"John\", 5000),\n",
    "    (2, \"Jane\", 6000),\n",
    "], columns=[\"employee_id\", \"employee_name\", \"salary\"])\n",
    "\n",
    "df2 = pd.DataFrame([\n",
    "    (3, \"Alex\", 8000),\n",
    "], columns=[\"employee_id\", \"employee_name\", \"salary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ad637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a parquet file\n",
    "df1.to_parquet(\"data.parquet\")\n",
    "\n",
    "# Read the data\n",
    "existing_data = pd.read_parquet(\"data.parquet\")\n",
    "\n",
    "# Concat two dataframes\n",
    "df3 = pd.concat([df1, df2])\n",
    "\n",
    "# Save to a file\n",
    "df3.to_parquet(\"data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88006e",
   "metadata": {},
   "source": [
    "Delta Lake offers a more efficient approach to handling this process. With Delta Lake, you can add, remove, or modify columns without the need to recreate the entire table.\n",
    "\n",
    "Delta Lake is also built on top of the Parquet file format so it retains the efficiency and columnar storage benefits of Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa43186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake.writer import write_deltalake\n",
    "\n",
    "table_path = \"employees\"\n",
    "\n",
    "# Write to Delta Lake\n",
    "write_deltalake(table_path, df1)\n",
    "\n",
    "# Append to Delta Lake\n",
    "write_deltalake(table_path, df2, mode=\"append\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9d59b4",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta-rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15efc918",
   "metadata": {},
   "source": [
    "### Enforce Data Quality with Delta Lake Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f751db",
   "metadata": {},
   "source": [
    "Delta Lake provides a convenient way to enforce data quality by adding constraints to a table, ensuring that only valid and consistent data can be added.\n",
    "\n",
    "In the provided code, attempting to add new data with a negative salary violates the constraint of a positive salary, and thus, the data is not added to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df1b6a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deltalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc086203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deltalake.writer import write_deltalake\n",
    "from deltalake import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e894bab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_path = \"delta_lake\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2cc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(\n",
    "    [\n",
    "        (1, \"John\", 5000),\n",
    "        (2, \"Jane\", 6000),\n",
    "    ],\n",
    "    columns=[\"employee_id\", \"employee_name\", \"salary\"],\n",
    ")\n",
    "\n",
    "write_deltalake(table_path, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7744de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81528396",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = DeltaTable(table_path)\n",
    "table.alter.add_constraint({\"salary_gt_0\": \"salary > 0\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36544f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(\n",
    "    [(3, \"Alex\", -200)],\n",
    "    columns=[\"employee_id\", \"employee_name\", \"salary\"],\n",
    ")\n",
    "\n",
    "write_deltalake(table, df2, mode=\"append\", engine=\"rust\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e721a1c",
   "metadata": {},
   "source": [
    "```bash\n",
    "DeltaProtocolError: Invariant violations: [\"Check or Invariant (salary > 0) violated by value in row: [3, Alex, -200]\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2449d254",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta-rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac6997",
   "metadata": {},
   "source": [
    "### Efficient Data Updates and Scanning with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ab98f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U \"deltalake==0.10.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4e987f",
   "metadata": {},
   "source": [
    "Every time new data is appended to an existing Delta table, a new Parquet file is generated. This allows data to be ingested incrementally without having to rewrite the entire dataset.\n",
    "\n",
    "As files accumulate, read operations may surge. The compact function merges small files into larger ones, enhancing scanning performance.\n",
    "\n",
    "Combining incremental processing with the compact function enables efficient data updates and scans as your dataset expands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42badfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from deltalake.writer import write_deltalake\n",
    "\n",
    "table_path = 'delta_lake'\n",
    "data_url = \"https://gist.githubusercontent.com/khuyentran1401/458905fc5c630d7a1f7a510a04e5e0f9/raw/5b2d760011c9255a68eb08b83b3b8759ffa25d5c/data.csv\"\n",
    "dfs = pd.read_csv(data_url, chunksize=100)\n",
    "for df in dfs:\n",
    "    write_deltalake(table_path, df, mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d994ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "\n",
    "dt = DeltaTable(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3db78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = dt.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1cfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.optimize.compact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18113055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df = dt.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe62bf",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta-rs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b05c90",
   "metadata": {},
   "source": [
    "### Simplify Table Merge Operations with Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82ce3c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de53f27",
   "metadata": {},
   "source": [
    "Merging two datasets and performing both insert and update operations can be a complex task.\n",
    "\n",
    "Delta Lake makes it easy to perform multiple data manipulation operations during a merge operation.\n",
    "\n",
    "The following code demonstrates merging two datasets using Delta Lake:\n",
    "- If a match is found, the `last_talk` column in `people_table` is updated with the corresponding value from `new_df`. \n",
    "- If the `last_talk` value in `people_table` is older than 30 days and the corresponding row is not present in the `new_df` table, the `status` column is updated to 'rejected'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9aa96e5",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "# Configure Spark to use Delta\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e4221d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a spark dataframe\n",
    "data = [\n",
    "    (0, \"A\", \"2023-04-15\", \"interviewing\"),\n",
    "    (1, \"B\", \"2023-05-01\", \"interviewing\"),\n",
    "    (2, \"C\", \"2023-03-01\", \"interviewing\"),\n",
    "\n",
    "]\n",
    "\n",
    "df = (\n",
    "    spark.createDataFrame(data)\n",
    "    .toDF(\"id\", \"company\", \"last_talk\", \"status\")\n",
    "    .repartition(1)\n",
    ")\n",
    "\n",
    "# Write to a delta table\n",
    "path = \"tmp/interviews\"\n",
    "df.write.format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0149d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Update the delta table\n",
    "people_table = DeltaTable.forPath(spark, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25a20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target table\n",
    "people_table.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ba6cc",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "new_data = [(0, \"A\", \"2023-05-07\")]\n",
    "new_df = (\n",
    "    spark.createDataFrame(new_data).toDF(\"id\", \"company\", \"last_talk\").repartition(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c9eebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source table\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f5830b",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "one_month_ago = \"current_date() - INTERVAL '30' DAY\"\n",
    "\n",
    "people_table.alias(\"target\").merge(\n",
    "    new_df.alias(\"source\"), \"target.id = source.id\"\n",
    ").whenMatchedUpdate(\n",
    "    set={\"target.last_talk\": \"source.last_talk\", \"target.status\": \"'interviewing'\"}\n",
    ").whenNotMatchedBySourceUpdate(\n",
    "    condition=f\"target.last_talk <= {one_month_ago}\",\n",
    "    set={\"target.status\": \"'rejected'\"},\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3574f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "people_table.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe9b97",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ce54cd",
   "metadata": {},
   "source": [
    "### From Complex SQL to Simple Merges: Delta Lake's Upsert Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79bdca",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c33207",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "# Configure Spark to use Delta\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4e134",
   "metadata": {},
   "source": [
    "Traditionally, implementing upsert (update or insert) logic requires separate UPDATE and INSERT statements or complex SQL. This approach can be error-prone and inefficient, especially for large datasets. \n",
    "\n",
    "Delta Lake's merge operation solves this problem by allowing you to specify different actions for matching and non-matching records in a single, declarative statement.\n",
    "\n",
    "Here's an example that demonstrates the power and simplicity of Delta Lake's merge operation:\n",
    "\n",
    "First, let's set up our initial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc34e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data for 'customers' DataFrame\n",
    "customers_data = [\n",
    "    (1, \"John Doe\", \"john@example.com\", \"2023-01-01 10:00:00\"),\n",
    "    (2, \"Jane Smith\", \"jane@example.com\", \"2023-01-02 11:00:00\"),\n",
    "    (3, \"Bob Johnson\", \"bob@example.com\", \"2023-01-03 12:00:00\"),\n",
    "]\n",
    "customers = spark.createDataFrame(\n",
    "    customers_data, [\"customer_id\", \"name\", \"email\", \"last_updated\"]\n",
    ")\n",
    "\n",
    "# Create sample data for 'updates' DataFrame\n",
    "updates_data = [\n",
    "    (2, \"Jane Doe\", \"jane.doe@example.com\"),  # Existing customer with updates\n",
    "    (3, \"Bob Johnson\", \"bob@example.com\"),  # Existing customer without changes\n",
    "    (4, \"Alice Brown\", \"alice@example.com\"),  # New customer\n",
    "]\n",
    "updates = spark.createDataFrame(updates_data, [\"customer_id\", \"name\", \"email\"])\n",
    "\n",
    "# Show the initial data\n",
    "print(\"Initial Customers:\")\n",
    "customers.show()\n",
    "print(\"Updates:\")\n",
    "updates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30605db",
   "metadata": {},
   "source": [
    "Next, we create a Delta table from our initial customer data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa3d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where you want to save the Delta table\n",
    "delta_table_path = \"customers_delta\"\n",
    "\n",
    "# Write the DataFrame as a Delta table\n",
    "customers.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "# Create a DeltaTable object\n",
    "customers_delta = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "print(\"Customers Delta Table created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172716ae",
   "metadata": {},
   "source": [
    "Now, here's the key part - the merge operation that handles both updates and inserts in a single statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20777ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'customers_delta' is your target table and 'updates' is your source of new data\n",
    "customers_delta.alias(\"target\").merge(\n",
    "    updates.alias(\"source\"),\n",
    "    \"target.customer_id = source.customer_id\"\n",
    ").whenMatchedUpdate(set={\n",
    "    \"name\": \"source.name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"last_updated\": \"current_timestamp()\"\n",
    "}).whenNotMatchedInsert(values={\n",
    "    \"customer_id\": \"source.customer_id\",\n",
    "    \"name\": \"source.name\",\n",
    "    \"email\": \"source.email\",\n",
    "    \"last_updated\": \"current_timestamp()\"\n",
    "}).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e3d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the updated data\n",
    "print(\"Updated Customers Delta Table:\")\n",
    "customers_delta.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc70013",
   "metadata": {},
   "source": [
    "### The Best Way to Append Mismatched Data to Parquet Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d5cbb",
   "metadata": {},
   "source": [
    "Appending mismatched data to a Parquet table involves reading the existing data, concatenating it with the new data, and overwriting the existing Parquet file. This approach can be expensive and may lead to schema inconsistencies.\n",
    "\n",
    "In the following code, the datatype of `col3` is supposed to be `int64` instead of `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed4604",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee2ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "\n",
    "filepath = 'test.parquet'\n",
    "\n",
    "# Write a dataframe to a parquet file\n",
    "df1 = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})\n",
    "df1.to_parquet(filepath)\n",
    "\n",
    "# Append a dataframe to a parquet file\n",
    "df2 = pd.DataFrame({'col1': [2], 'col2': [7], 'col3': [0]})\n",
    "concatenation = pd.concat([df1, df2]) # concatenate dataframes\n",
    "concatenation.to_parquet(filepath) # overwrite original file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94663e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df = pd.read_parquet(filepath)\n",
    "print(concat_df, \"\\n\")\n",
    "print(concat_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cd3c88",
   "metadata": {},
   "source": [
    "With Delta Lake, you can effortlessly append DataFrames with extra columns while ensuring the preservation of your data's schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09479b4",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "# Configure Spark to use Delta\n",
    "builder = (\n",
    "    pyspark.sql.SparkSession.builder.appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\n",
    "        \"spark.sql.catalog.spark_catalog\",\n",
    "        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n",
    "    )\n",
    ")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd23de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark Dataframe\n",
    "data = [(1, 3), (2, 4)]\n",
    "\n",
    "df1 = (\n",
    "    spark.createDataFrame(data)\n",
    "    .toDF(\"col1\", \"col2\")\n",
    "    .repartition(1)\n",
    ")\n",
    "\n",
    "# Write to a delta table\n",
    "path = \"tmp\"\n",
    "df1.write.format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904aacdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame\n",
    "new_data = [(2, 7, 0)]\n",
    "df2 = (\n",
    "    spark.createDataFrame(new_data).toDF(\"col1\", \"col2\", \"col3\").repartition(1)\n",
    ")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append to the existing Delta table\n",
    "df2.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3da7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Delta table\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "table = DeltaTable.forPath(spark, path)\n",
    "concat_df = table.toDF().pandas_api()\n",
    "\n",
    "print(concat_df, \"\\n\")\n",
    "print(concat_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6721f",
   "metadata": {},
   "source": [
    "[Link to Delta Lake](https://github.com/delta-io/delta)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   20,
   24,
   28,
   32,
   43,
   47,
   57,
   61,
   66,
   70,
   76,
   80,
   86,
   90,
   92,
   96,
   100,
   104,
   108,
   117,
   121,
   131,
   135,
   157,
   161,
   165,
   171,
   179,
   198,
   202,
   208,
   214,
   220,
   224,
   228,
   232,
   236,
   242,
   246,
   252,
   262,
   275,
   279,
   281,
   285,
   297,
   299,
   314,
   318,
   320,
   324,
   328,
   332,
   341,
   354,
   366,
   372,
   382,
   386,
   390,
   396,
   402,
   408,
   412,
   424,
   428,
   433,
   440,
   446,
   450,
   454,
   458,
   466,
   477,
   483,
   488,
   492,
   495,
   499,
   503,
   507,
   517,
   536,
   558,
   567,
   572,
   581,
   586,
   601,
   603,
   607,
   611,
   617,
   634,
   646,
   670,
   674,
   685,
   689,
   706,
   710,
   714,
   720,
   727,
   742,
   746,
   750,
   769,
   784,
   793,
   798,
   807
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}