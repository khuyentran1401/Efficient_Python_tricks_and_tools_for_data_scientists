{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b5b2f2",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4575a3",
   "metadata": {},
   "source": [
    "This section some tools to process and work with text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6b913",
   "metadata": {},
   "source": [
    "### TextBlob: Processing Text in One Line of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0702e509",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7a266",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a6381",
   "metadata": {},
   "source": [
    "TextBlob offers quick text analysis, such as sentiment detection, tokenization, noun phrase extraction, word frequency analysis, and spelling correction. Start by creating a `TextBlob` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Today is a beautiful day\"\n",
    "blob = TextBlob(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508f372",
   "metadata": {},
   "source": [
    "Tokenize words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a363875",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb47b589",
   "metadata": {},
   "source": [
    "Extract noun phrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d90912",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b119f",
   "metadata": {},
   "source": [
    "Analyze sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41612876",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40c954",
   "metadata": {},
   "source": [
    "Count words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2db492",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093ce660",
   "metadata": {},
   "source": [
    "Correct spelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82a713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Today is a beutiful day\"\n",
    "blob = TextBlob(text)\n",
    "blob.correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dad6b",
   "metadata": {},
   "source": [
    "[Link to TextBlob](https://github.com/sloria/TextBlob)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b6977",
   "metadata": {},
   "source": [
    "### Convert Names into a Generalized Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914bbb9",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73632c12",
   "metadata": {},
   "source": [
    "Names collected from different sources might have different formats. To convert names into the same format for further processing, use mlxtend's `generalize_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7193c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.text import generalize_names\n",
    "\n",
    "generalize_names(\"Tran, Khuyen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a55863",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalize_names(\"Khuyen Tran\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "generalize_names(\"Khuyen Tran\", firstname_output_letters=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f5cb6",
   "metadata": {},
   "source": [
    "[Link to mlxtend](https://rasbt.github.io/mlxtend/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2d26a",
   "metadata": {},
   "source": [
    "### sumy: Summarize Text in One Line of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d854471",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8548df",
   "metadata": {},
   "source": [
    "Sumy is an easy-to-use tool for text summarization, offering 7 different methods. To summarize the article \"How to Learn Data Science (Step-By-Step)\" from DataQuest:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83f3b7",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ sumy lex-rank --length=10 --url=https://www.dataquest.io/blog/learn-data-science/ \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86363c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sumy lex-rank --length=10 --url=https://www.dataquest.io/blog/learn-data-science/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3540e4",
   "metadata": {},
   "source": [
    "[Link to Sumy](https://github.com/miso-belica/sumy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bdac7d",
   "metadata": {},
   "source": [
    "### Spacy_streamlit: Create a Web App to Visualize Your Text in 3 Lines of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3179a29e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install spacy-streamlit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d3518a",
   "metadata": {},
   "source": [
    "To quickly create an app to visualize the structure of a text, use spacy_streamlit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ef141",
   "metadata": {},
   "source": [
    "To understand how to use spacy_streamlit, we add the code below to a file called `streamlit_app.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50e8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile streamlit_app.py\n",
    "import spacy_streamlit \n",
    "\n",
    "models = ['en_core_web_sm']\n",
    "text = \"Today is a beautiful day\"\n",
    "spacy_streamlit.visualize(models, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7245f89",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ streamlit run streamlit_app.py\n",
    "```\n",
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74813f17",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c55a2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "!streamlit run streamlit_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e0d9eb",
   "metadata": {},
   "source": [
    "Click the URL and you should see something like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5835ad8e",
   "metadata": {},
   "source": [
    "![image](../img/streamlit_app.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814d762",
   "metadata": {},
   "source": [
    "[Link to spacy-streamlit](https://spacy.io/universe/project/spacy-streamlit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5738be",
   "metadata": {},
   "source": [
    "### textacy: Extract a Contiguous Sequence of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fdf782",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install spacy textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72058a52",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a47481d",
   "metadata": {},
   "source": [
    "To extract sequences of words (n-grams), use the `textacy` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766cb879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from textacy.extract import ngrams\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = nlp(\"Ice cream is a soft frozen food made with sweetened and flavored milk fat.\")\n",
    "\n",
    "# extract sequences of 3 words\n",
    "[n.text for n in ngrams(text, n=3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37c9d6",
   "metadata": {},
   "source": [
    "[Link to textacy](https://textacy.readthedocs.io/en/stable/quickstart.html#working-with-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5164d6cb",
   "metadata": {},
   "source": [
    "### Num2Words: Convert Number to Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3035fc8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f9c3a",
   "metadata": {},
   "source": [
    "The `num2words` library helps convert numerical values into words, making NLP tasks like matching numeric data to their textual equivalents easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa262d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "\n",
    "num2words(2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2words(2019, to='ordinal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2words(2019, to='ordinal_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b9d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2words(2019, to='year')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313c751",
   "metadata": {},
   "source": [
    "It also supports multiple languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22491a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2words(2019, lang='vi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f94fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num2words(2019, lang='es')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58451993",
   "metadata": {},
   "source": [
    "[Link to num2words](https://bit.ly/3VkWYJO)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8790e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Numerizer: Standardizing Numerical Data in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de17a58",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install numerizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826720a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Converting textual numbers to numeric values is challenging due to diverse language representations.\n",
    "\n",
    "Numerizer simplifies this process by turning various text formats into corresponding numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf67040",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from numerizer import numerize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f8e381",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "numerize('four hundred and sixty two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c717f365",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "numerize('four hundred sixty two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30363a95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "numerize('four sixty two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7ff42",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "numerize('four sixty-two')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d1fd71",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "[Link to numerizer](https://github.com/jaidevd/numerizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5af4a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Preprocess Text in One Line of Code with Texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e96f1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install texthero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d06934",
   "metadata": {},
   "source": [
    "Processing text in a DataFrame often involves writing lengthy code. Texthero simplifies this by enabling one-line preprocessing, including: \n",
    "- filling missing values\n",
    "- converting upper case to lower case\n",
    "- removing digits\n",
    "- removing punctuation\n",
    "- removing stopwords\n",
    "- removing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550eb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "\n",
    "text = [\n",
    "    \"Today is a    beautiful day\",\n",
    "    \"There are 3 ducks in this pond\",\n",
    "    \"This is. very cool.\",\n",
    "    np.nan,\n",
    "]\n",
    "df = pd.DataFrame({\"text\": text})\n",
    "\n",
    "df.text.pipe(hero.clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911902e6",
   "metadata": {},
   "source": [
    "You can also create a custom cleaning pipeline by chaining different processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3771d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.text\n",
    "    .pipe(hero.fillna)\n",
    "    .pipe(hero.remove_punctuation)\n",
    "    .pipe(hero.remove_stopwords)\n",
    "    .pipe(hero.remove_whitespace)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7942daf7",
   "metadata": {},
   "source": [
    "[Link to texthero](https://github.com/jbesomi/texthero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a06bb5",
   "metadata": {},
   "source": [
    "### texthero: Reduce Dimension and Visualize Text in One Line of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e238c39",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install 'gensim==3.6.0'\n",
    "!pip install 'texthero==1.1.0'\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4761ab",
   "metadata": {},
   "source": [
    "To visualize text data in 2D, typically, you need to clean, encode, and reduce the dimensions of your text, which can be tedious. Texthero simplifies this process into just two lines of code.\n",
    "\n",
    "Below is an example using descriptions from [CNN news articles](https://www.kaggle.com/datasets/hadasu92/cnn-articles-after-basic-cleaning). Each point represents an article, colored by its category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff23018",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import gdown \n",
    "\n",
    "gdown.download('https://drive.google.com/uc?id=1QPGCZ8mud5ptt8qJR79XQ6KoQnJuT-4D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e86c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"small_CNN.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688160e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pca\"] = df[\"Description\"].pipe(hero.clean).pipe(hero.tfidf).pipe(hero.pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "hero.scatterplot(df, col=\"pca\", color=\"Category\", title=\"CNN News\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c5289",
   "metadata": {},
   "source": [
    "![](../img/texthero_scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486071ec",
   "metadata": {},
   "source": [
    "[Link to texthero](https://github.com/jbesomi/texthero)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72fcdbf",
   "metadata": {},
   "source": [
    "### wordfreq: Estimate the Frequency of a Word in 36 Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac000457",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install wordfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e460bd3",
   "metadata": {},
   "source": [
    "If you need to check the frequency of a word in 36 different languages, `wordfreq` is an excellent tool. \n",
    "\n",
    "It even covers words that occur as infrequently as once per 10 million words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ee3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordfreq import word_frequency\n",
    "\n",
    "word_frequency(\"eat\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c03906",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency(\"the\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6abadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"There is a dog running in a park\"\n",
    "words = sentence.split(\" \")\n",
    "word_frequencies = [word_frequency(word, \"en\") for word in words]\n",
    "\n",
    "sns.barplot(words, word_frequencies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f2fbf",
   "metadata": {},
   "source": [
    "[Link to wordfreq](https://github.com/khuyentran1401/Python-data-science-code-snippet/blob/master/code_snippets/data_science_tools/wordfreq_example.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c870c2e",
   "metadata": {},
   "source": [
    "### newspaper3k: Extract Meaningful Information From an Articles in 2 Lines of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6bea3f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install newspaper3k nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e7321b",
   "metadata": {},
   "source": [
    "To quickly extract meaningful information from an article in a few lines of code, use `newspaper3k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://mathdatasimplified.com/2023/05/08/build-an-efficient-data-pipeline-is-dbt-the-key/\"\n",
    "article = Article(url)\n",
    "article.download()\n",
    "article.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf05ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591260e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article.publish_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "article.top_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b301c382",
   "metadata": {},
   "outputs": [],
   "source": [
    "article.nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc69ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7768e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "article.keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59aad5",
   "metadata": {},
   "source": [
    "[Link to newspaper3k](https://github.com/codelucas/newspaper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2378027d",
   "metadata": {},
   "source": [
    "### Questgen.ai: Question Generator in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099460e1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ramsrigouthamg/Questgen.ai\n",
    "!pip install git+https://github.com/boudinfl/pke.git\n",
    "\n",
    "!python -m nltk.downloader universal_tagset\n",
    "!python -m spacy download en "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1b001",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
    "!tar -xvf  s2v_reddit_2015_md.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53984ce6",
   "metadata": {},
   "source": [
    "Generating questions manually from a document can be time-consuming. `Questgen.ai` automates this task, allowing you to quickly generate Boolean or FAQ-style questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a92a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from Questgen import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa9fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"input_text\": \"\"\"The weather today was nice so I went for a walk. I stopped for a quick chat with my neighbor.\n",
    "    It turned out that my neighbor just got a dog named Pepper. It is a black Labrador Retriever.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbc07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qe = main.BoolQGen()\n",
    "output = qe.predict_boolq(payload)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = qg.predict_shortq(payload)\n",
    "pprint(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ffdbe",
   "metadata": {},
   "source": [
    "[Link to Questgen.ai](https://github.com/ramsrigouthamg/Questgen.ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080f77f",
   "metadata": {},
   "source": [
    "### Word Ninja: Slice Your Lumped-Together Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4120f541",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install wordninja "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13bcbe2",
   "metadata": {},
   "source": [
    "Want to split compound words? `Word Ninja` is surprisingly effective at doing just that. Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordninja \n",
    "\n",
    "wordninja.split(\"honeyinthejar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcd0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordninja.split(\"ihavetwoapples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cda095",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordninja.split(\"aratherblusterday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5905c1",
   "metadata": {},
   "source": [
    "[Link to Word Ninja](https://github.com/keredson/wordninja)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa980b7",
   "metadata": {},
   "source": [
    "### textstat: Calculate Statistics From Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ec33b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46dadc0",
   "metadata": {},
   "source": [
    "To analyze text statistics such as readability scores and reading time, use the `textstat` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c6a82e",
   "metadata": {},
   "source": [
    "To calculate the Automated Readability Index (ARI), which indicates the grade level required to understand a text, use `automated_readability_index`. For example, an ARI of 10.8 means the text is suitable for 10th to 11th graders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The working memory system is a form of conscious learning. But not all learning is conscious. Psychologists have long marveled at children’s ability to acquire perfect pronunciation in their first language or recognize faces.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d15ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "textstat.automated_readability_index(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19bdd0d",
   "metadata": {},
   "source": [
    "You can also measure the reading time of a text in seconds using `reading_time`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e77819",
   "metadata": {},
   "outputs": [],
   "source": [
    "textstat.reading_time(text, ms_per_char=14.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36b68c",
   "metadata": {},
   "source": [
    "[Link to textstat](https://github.com/shivam5992/textstat)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc581a",
   "metadata": {},
   "source": [
    "### RapidFuzz: Rapid String Matching in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1144d77",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a2fcd",
   "metadata": {},
   "source": [
    "To find similar strings efficiently, use the `RapidFuzz` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631448e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2175e8cd",
   "metadata": {},
   "source": [
    "You can calculate the similarity ratio between two strings using the `fuzz.ratio` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c7c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\"Let's meet at 10 am tomorrow\", \"Let's meet at 10 am tommorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c338540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.ratio(\"here you go\", \"you go here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0945a1d",
   "metadata": {},
   "source": [
    "For token sorting and matching, use `token_sort_ratio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b47c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz.token_sort_ratio(\"here you go\", \"you go here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6feab37",
   "metadata": {},
   "source": [
    "[Link to RapidFuzz](https://github.com/maxbachmann/RapidFuzz)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b700dd8",
   "metadata": {},
   "source": [
    "### Checklist: Create Data to Test Your NLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd0beb",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install checklist torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4cd003",
   "metadata": {},
   "source": [
    "It can be time-consuming to create data to test edge cases of your NLP model. To quickly create data to test your NLP models, use `Checklist`. \n",
    "\n",
    "In the code below, I use Checklist's `Editor` to create multiple examples of negation in one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2e87c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "\n",
    "editor = Editor()\n",
    "editor.template(\"{mask} is not {a:pos} option.\", pos=[\"good\", \"cool\"], nsamples=5).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5aa78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor.template(\"{mask} is not {a:neg} option.\", neg=[\"bad\", \"awful\"], nsamples=5).data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68571aeb",
   "metadata": {},
   "source": [
    "[Link to Checklist](https://github.com/marcotcr/checklist)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f991525e",
   "metadata": {},
   "source": [
    "### Top2Vec: Quick Topic Modeling in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0614018",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install top2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f231a88b",
   "metadata": {},
   "source": [
    "Manually identifying topics from large text collections and having to specify the number of topics beforehand results in subjective, inconsistent topic modeling and requires significant trial and error.\n",
    "\n",
    "Top2Vec automatically discovers topics by finding dense clusters of semantically similar documents and identifying the words that attract those documents together.\n",
    "\n",
    "In this example, we will use the Top2Vec library to detect topics in a dataset of fake news articles. \n",
    "\n",
    "Load the \"Fake-News\" dataset from OpenML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from top2vec import Top2Vec\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906faacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_openml(\"Fake-News\")\n",
    "text = news.data[\"text\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4f9da",
   "metadata": {},
   "source": [
    "Create a Top2Vec model with the text data, using the \"learn\" speed and 8 worker threads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc1466",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Top2Vec(documents=text, speed=\"learn\", workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73955cb",
   "metadata": {},
   "source": [
    "Get the number of topics detected by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0bbc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_num_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b92ff0",
   "metadata": {},
   "source": [
    "In this example, Top2Vec automatically determines the number and content of topics by analyzing document similarities in the embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee963a45",
   "metadata": {},
   "source": [
    "Get the topics in decreasing size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce0004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(72)\n",
    "\n",
    "print(\"\\nSecond Topic - Top 5 Words and Scores:\")\n",
    "for word, score in zip(topic_words[1][:5], word_scores[1][:5]):\n",
    "    print(f\"Word: {word:<20} Score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652cc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Second Topic - Top 5 Words: {topic_words[1][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8025c",
   "metadata": {},
   "source": [
    "Search for topics most similar to the keyword \"president\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1521e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"president\"], num_topics=2)\n",
    "\n",
    "first_five_words = [topic[:5] for topic in topic_words]\n",
    "\n",
    "print(\"Topics most similar to president:\")\n",
    "for topic_num, words in zip(topic_nums, first_five_words):\n",
    "    print(f\"Topic {topic_num}: {words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119bcf2",
   "metadata": {},
   "source": [
    "Generate word clouds for the topics most similar to \"president\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2da0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topic_nums:\n",
    "    model.generate_topic_wordcloud(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d18bf4",
   "metadata": {},
   "source": [
    "[Link to Top2Vec](https://github.com/ddangelov/Top2Vec)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402e41d",
   "metadata": {},
   "source": [
    "### Expanding English Contractions in Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da912e4",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09903a8a",
   "metadata": {},
   "source": [
    "Contraction can cause issues when processing text. To expand contractions using Python, use the library contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fc22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "sent = \"I'm not sure, but I'd like to do it\"\n",
    "\n",
    "contractions.fix(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9b331",
   "metadata": {},
   "source": [
    "### inflect: Generate Plurals, Singulars, and Indefinite Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aaca16",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install inflect "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3421335",
   "metadata": {},
   "source": [
    "To generate plurals, singulars, or indefinite articles from given words, use inflect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e31c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inflect\n",
    "\n",
    "p = inflect.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc00581",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plural_noun('he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfea44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.plural_verb('sees')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ab30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.gender(\"feminine\")\n",
    "p.singular_noun(\"they\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dee11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if p.compare_verbs('sees', 'see'):\n",
    "    print(\"same word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738e51a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the correct \"a\" or \"an\" for a given word\n",
    "fruit1 = 'apple'\n",
    "fruit2 = 'banana'\n",
    "print(f\"I got you {p.a(fruit1)} \"\n",
    "      f\"and {p.a(fruit2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3bd5d",
   "metadata": {},
   "source": [
    "[Link to inflect](https://github.com/jaraco/inflect)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feaa74",
   "metadata": {},
   "source": [
    "### ekphrasis: Text Processing Tool For Social Media Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1c5380",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install ekphrasis==0.5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0174d",
   "metadata": {},
   "source": [
    "It is challenging to process text from social media such as Twitter or Facebook. ekphrasis allows you to incorporate social tokenizers, word segmentation, spell correction, and more into a pipeline to process those texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
    "from ekphrasis.dicts.emoticons import emoticons\n",
    "\n",
    "text_processor = TextPreProcessor(\n",
    "    # terms that will be normalized\n",
    "    normalize=['url', 'user'],\n",
    "    # terms that will be annotated\n",
    "    annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
    "        'emphasis', 'censored'},\n",
    "    \n",
    "    # corpus for word segmentation \n",
    "    segmenter=\"twitter\", \n",
    "    \n",
    "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
    "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
    "    spell_correct_elong=False,  # spell correction for elongated words\n",
    "\n",
    "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
    "    \n",
    "    # Replace emojis with words\n",
    "    dicts=[emoticons]\n",
    ")\n",
    "\n",
    "sent = \"@coolyazzy94 I'm learning to retweeeet!! Least it sucks LESS than Facebook haha :P #learn-twitter https://t.co/7RdyMCVPKx\"\n",
    "\n",
    "print(\" \".join(text_processor.pre_process_doc(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee90af5",
   "metadata": {},
   "source": [
    "[Link to ekphrasis](https://github.com/cbaziotis/ekphrasis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283381ed",
   "metadata": {},
   "source": [
    "### Chroma: The Lightning-Fast Solution to Text Embeddings and Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e8133d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22891129",
   "metadata": {},
   "source": [
    "Semantic search uses embedding to understand the meaning of search queries instead of relying solely on keyword matches to locate documents.\n",
    "\n",
    "Embedding is like a translator converting words into numbers so that computers can understand. Chroma makes it easy to create embeddings from documents and find similar results with a few lines of code.\n",
    "\n",
    "In the code below, the documents with IDs 1 and 2 closely match the given query text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df439f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.get_or_create_collection(\"test\")\n",
    "\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"A man is eating food.\",\n",
    "        \"A man is eating yellow noodles.\",\n",
    "        \"The girl is carrying a baby.\",\n",
    "        \"A man is riding a horse.\",\n",
    "    ],\n",
    "    ids=[\"1\", \"2\", \"3\", \"4\"],\n",
    ")\n",
    "\n",
    "query_result = collection.query(\n",
    "    query_texts=[\"A man is eating pasta.\" ],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be78b52",
   "metadata": {},
   "source": [
    "[Link to Chroma](https://github.com/chroma-core/chroma)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef04f190",
   "metadata": {},
   "source": [
    "### Galatic: Clean and Analyze Massive Text Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49bcf54",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install galactic-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43b9b13",
   "metadata": {},
   "source": [
    "To clean, gain insights, and create embeddings from massive unstructured text datasets, use Galatic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869f250d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from galactic import GalacticDataset\n",
    "\n",
    "\n",
    "filter_func = lambda x: len(x[\"content\"]) < 1024\n",
    "dataset = GalacticDataset.from_hugging_face_stream(\n",
    "    \"tiiuae/falcon-refinedweb\",\n",
    "    split=\"train\",\n",
    "    filters=[filter_func],\n",
    "    dedup_fields=[\"content\"],\n",
    "    max_samples=5000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c437b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect the language of the text\n",
    "from collections import Counter\n",
    "dataset.detect_language(field=\"content\")\n",
    "Counter(dataset[\"__language\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a5b864",
   "metadata": {},
   "source": [
    "```python\n",
    "Counter({'en': 4975,\n",
    "         'es': 7,\n",
    "         'fr': 7,\n",
    "         'de': 3,\n",
    "         'da': 2,\n",
    "         'ru': 1,\n",
    "         'nl': 1,\n",
    "         'pt': 1,\n",
    "         'sh': 1,\n",
    "         'eo': 1,\n",
    "         'ceb': 1})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get personal information from the text\n",
    "dataset.detect_pii(\n",
    "    fields=[\"content\"]\n",
    ")\n",
    "print(\"Email:\", sum(dataset[\"__pii__email\"]))\n",
    "print(\"Phone:\", sum(dataset[\"__pii__phone\"]))\n",
    "print(\"Username/Password:\", sum(dataset[\"__pii__credential\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out all examples that have \"blogspot\" in the URL.\n",
    "dataset = dataset.filter_string(\n",
    "    fields=[\"url\"],\n",
    "    values=[\"blogspot\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2346fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "dataset.get_embeddings(input_field=\"content\", backend=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621607a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the embeddings with kmeans\n",
    "dataset.cluster(n_clusters=5, overwrite=True)\n",
    "dataset.get_cluster_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a26366",
   "metadata": {},
   "source": [
    "[Link to Galatic](https://github.com/taylorai/galactic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4623f6",
   "metadata": {},
   "source": [
    "### Efficient Keyword Extraction and Replacement with FlashText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640a7ec",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install flashtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf47b796",
   "metadata": {},
   "source": [
    "If you want to perform fast keyword extraction and replacement in text, use FlashText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404cc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "\n",
    "keyword_processor = KeywordProcessor()\n",
    "\n",
    "# Adding keywords with replacements\n",
    "keyword_processor.add_keyword(keyword=\"Python\")\n",
    "keyword_processor.add_keyword(keyword=\"DS\", clean_name=\"data science\")\n",
    "\n",
    "# Replacing keywords in text\n",
    "new_sentence = keyword_processor.replace_keywords(\"PYTHON is essential for DS.\")\n",
    "new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d22fffd",
   "metadata": {},
   "source": [
    "[Link to FlashText](https://bit.ly/4bQ1eqt)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f46b075",
   "metadata": {},
   "source": [
    "### BERTopic: Harnessing BERT for Interpretable Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34e050",
   "metadata": {
    "editable": true,
    "id": "SNa-KtKDRnus",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f1b4df",
   "metadata": {},
   "source": [
    "Managing and understanding large collections of text documents results in complex workflows with multiple preprocessing steps and difficult-to-interpret results. This causes data scientists to spend significant time trying to make sense of their document clusters and explaining them to stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f686256f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "With BERTopic, you can leverage state-of-the-art language models to create more meaningful and interpretable topics. You get automatic topic labeling, visualization capabilities, and the flexibility to customize the modeling process according to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b28e8b",
   "metadata": {
    "editable": true,
    "id": "Y3VGFZ1USMTu",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "For this example, we use the popular 20 Newsgroups dataset which contains roughly 18000 newsgroups posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f72316",
   "metadata": {
    "editable": true,
    "id": "JJij3WP6SEQD",
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7533b7d2",
   "metadata": {
    "id": "SBcNmZJzSTY8"
   },
   "source": [
    "In this example, we will go through the main components of BERTopic and the steps necessary to create a strong topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9d88bc",
   "metadata": {
    "id": "QI6vwelqnTL-"
   },
   "source": [
    "We start by instantiating BERTopic. We set language to `english` since our documents are in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd57134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453,
     "referenced_widgets": [
      "11f92fdcd9734ee883a98bf7d64441b2",
      "ab97eac14145428c8977aa81fa223865",
      "b16e51d7467547aa96f17b076d1fffe3",
      "7afada084577495dac21948c8723de75",
      "90a601e96edf46e488cd9f66356475cc",
      "c9bb9d766e5c4fa5bbf496e571547aff",
      "37692e481ffe49d7b6ddaa84844deb01",
      "a6f200cf31dc497b86ada1752118e59f",
      "fcadde72381f42c6beff807fe5517933",
      "6144fe4535d4467f9ab7618afd3a0de2",
      "5ba767c6a7384e10b89f546a648137cb",
      "050f3bd3b7604af4bc04faf9c2d24aca",
      "a1d0cae4886a46f4a7df1cf609de2941",
      "6e1271d7eb7a4d55b0c57f554e2eee6c",
      "08c0cbefee4c4233959252be914e9651",
      "eed5268b499849879beb87df1577820e",
      "64abaa2714654f6498111dd4060d6c85",
      "e34ef193f7d54751b149e5f5cf42cded",
      "5171da807c304800b63dda419fe505f6",
      "f1120d69f45d4940b803b74d38aa4267",
      "9db81cfed22549edb65871adfada1f38",
      "8dc04f61740a470ebc2ddaf9ca985b2c",
      "8bab1f41e55345f1b32c9ed549b7cacb",
      "1f444df7dc41409ba737cc34fd3759b1",
      "6b6c11fd3f6f43118f833cac813e7a95",
      "89748a71005e40a6a430d139957dba74",
      "d9e56e12fb8e4468b12be19aeb439058",
      "201fe878ad1b4487ae09c5feb5c68c2e",
      "2c65c96c214e4c8f8bf8e0b2ddc8f15c",
      "5cd8823600334387b6f820b9c13ff875",
      "236722fa23624b3e9d1fd2db3597c41a",
      "b1399b9c3c8c41fb9ab561fff04adf88",
      "0dedaca218f940269b1fa2c77d197e69",
      "4993487dda804769be12ed80f5a0ba1f",
      "db84a621349b478aa2dd0f0c51613ee4",
      "a084c426f4f1406caa4b61ea5f7d0597",
      "7a2219db3e224357bbfe2c67d85e7f1b",
      "9195b74202c9493e884bad686a49e687",
      "ef0f6e08164441dcbd2ffd70c6a7837d",
      "69fe53a8a50d4bf58f29b12652650c95",
      "72a920198eb74ba794804f7416585f8f",
      "dc8d955025f64ba08854e834540b55c8",
      "a96c566423124ad1af6cbbf426c23714",
      "1af379c21da04f5eb76c30a28c793d2a",
      "1ac1ec5ccc9e49b7b1f4d5d0f75a28cc",
      "2694fe1ceeeb43f5bc7d48753d6cfa3c",
      "1a34ebfc412f4852bfc75f1403ca9711",
      "88dee3b64c644e8c987d215590666367",
      "38f35a2054b64376b4520e5468b5a5c9",
      "0d109983418a44e494026cc2e1d01a4d",
      "493787120d774090b375fb718d281b8d",
      "302635d5f94147a4bd805341691d225c",
      "cd2a773537a34f62b122b271f2eba9a3",
      "49baeecf41ac49148c1582fc011b38f6",
      "119d56c86dfc40ecb082c5a4877b4896",
      "058890a9257b495c882114efcd8c6987",
      "3c4095a5470041dcb87b02ff7d501b8e",
      "0b051498baf947319c474179eefa239d",
      "4cf7524d16b24dc2aa49f378682869ea",
      "5242f947dd2643cf86d3c595af13c8fb",
      "5f7616ac5c694906a31c65c19a57333a",
      "4fcfb5edfe8e4205b7496d1f3d01fa07",
      "22b846963a10408591f8519170114cd9",
      "9c169e0cf33a44ce883d640a08a59c48",
      "4eb5d68b7f16457b923ff0c1645686a2",
      "ebd1ee7483b046ba9b538eea82db69be",
      "0768adbd8c554e0a923d55fa62140f70",
      "d548b44b4f5d4281acf61e37f18bb7d6",
      "c1dd125cf61d4438a26ee57a1efcb1de",
      "7d8c4dfecc474c1aa757ef7ded4cdd55",
      "74e7ab4585324e4f9e18f6a0ad4f54f5",
      "604456945f8a45139f0a29e82acbfb25",
      "c6dbfcf25a8a4a1381879f84c5d6f8a4",
      "38b5984b43124e02a883d27fb0fd94f5",
      "7bb5a260a7854b51b66957ebf68bea5f",
      "ad3d2452bdd64279be86bbafb65f5fe7",
      "b3250c2f180d418abc1f92594a1915e5",
      "95e9a59d2fd84febb8c0fbda398e295f",
      "e6a4c53239884fb9b1813d905d05592f",
      "73ae6c75741145339bad4f6e9e169b49",
      "0cb87870f9c543dcb792e32fa95ee324",
      "3b98f41d250545dea21203bbca27c11e",
      "658573d1b72948fd9c2026b57e29b09f",
      "04e4da585a2a4f4883afe0438ffe2c9c",
      "8f8188bddcb8442ba8b4a4ef7ab16718",
      "2a6560a4894d4e589f6d16598a103c70",
      "a263765c073947dfa86b9aef86cba81f",
      "b2369edc51504835b7022c99ba5ef46d",
      "9451969faa6c4dfca3e91f976aed6398",
      "8a479cae31b64ec59348111daae3a303",
      "5d1d1822fcd44d3fbcf7fa46b730f7c9",
      "9bfdd3910e8f4fd88065594ef25b29de",
      "aef4cd0d462846438eb98abe01d74cb6",
      "c0344970c2e74ddc913c62cbc148d472",
      "6e3bed9d7a994e82b17d10f35b797832",
      "de1505fd50e64305859aa71c9dfcdb78",
      "9550a4100a204e1596e7659db1bd8127",
      "38c0f36324054c29a8c1ad3fb592c8f0",
      "73514077146d4b069495b184009f6b87",
      "6b945640a2884e9c9e2bb5064e953862",
      "8cdf6d0427ba49f8af99883d7f15185d",
      "bd585fe8c3914b01b6da8a5e97dee8ce",
      "a965671bb5964c3cb759d4908fc7d05d",
      "62f3e02df14d44f28de9e99973046760",
      "595c0c6fb73d4d598d201a8ebd2cec36",
      "4ca4c7c771954283b6918527f0c18009",
      "e754dd72b44540b8a9476f7a64fb752f",
      "285cb4d8cb934472bce29b9dfe6e5d67",
      "39390b9f5f14430ea65cf66cada721d6",
      "d1974f3fc978409eb0cd5380a6cdf6e5",
      "d1a1a7a2d3724979ac8c80479a0478c2",
      "8d0c27b3c1784fa886266e95c795023f",
      "e1c7544dda374d1a94c1d9a2da0bf7f7",
      "75411ae0f844419fbc987ae25c10c8ef",
      "afb465ed9c8f453391e33a06f19bf0c2",
      "5d2b16e655774462aad43487425d6e37",
      "105ec09c7139491eb0212678f4418884",
      "dc43bcd09db94cf09bf979a45766a35c",
      "46c4854e59294187ab98ca4beea9d227",
      "f546d47a4ce44e50b1caaaee7059e800",
      "0cfd2a010ba140db8bc8d05e2d195a2c",
      "774ec98d68994b97abe86ff3ed3b76f3",
      "95e813a2d777402b8e3e463a80fc18d2",
      "720aeed12981455898fe676801ad31d0",
      "5ae311098bdc4820905b27cdc24e5d91",
      "b0179833d14247e48a49e8c7852586db",
      "c84640a03d8d4881ba230233976eaf18",
      "af63d7a0ddde4e7b897ce1cba5292529",
      "d957bd0a7ddc43758f302bbf1cb01947",
      "1350f8de7a7e46d2bac62044405ab30d",
      "49429184f2244ce6b6a9e0d9b3aa1040",
      "219a32964b984c90a33acf72981a499c"
     ]
    },
    "editable": true,
    "id": "TfhfzqkoSJ1I",
    "outputId": "d69cb33b-91b7-4423-899c-98471f411ed3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(language=\"english\", verbose=True)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d8495",
   "metadata": {
    "id": "A5O3KpHTnVpz"
   },
   "source": [
    "After fitting our model, we can start by looking at the results. Typically, we look at the most frequent topics first as they best represent the collection of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06490846",
   "metadata": {
    "id": "ScBUgXn06IK6"
   },
   "outputs": [],
   "source": [
    "freq = topic_model.get_topic_info()\n",
    "freq.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445def36",
   "metadata": {
    "id": "-BtOgifV7Q-H"
   },
   "source": [
    "-1 refers to all outliers and should typically be ignored. Next, let's take a look at a frequent topic that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f6b9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IVpvT4bA6KiN",
    "outputId": "9cf99b89-30bb-45fe-b98b-063f8f3624d9"
   },
   "outputs": [],
   "source": [
    "topic_model.get_topic(0)  # Select the most frequent topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15115e",
   "metadata": {},
   "source": [
    "Access the predicted topics for the first 10 documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9104687",
   "metadata": {
    "id": "uCMHaWVMpbo3"
   },
   "outputs": [],
   "source": [
    "topic_model.topics_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edef8ce",
   "metadata": {
    "id": "M8c8LenB8Zyl"
   },
   "source": [
    "Visualize topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068039b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "editable": true,
    "id": "S9qDqEHddgKq",
    "outputId": "3fddd5f1-194e-4708-a7dc-f0c5602c140a",
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bf9256",
   "metadata": {
    "id": "4spXl2_C6flq"
   },
   "source": [
    "We can visualize the selected terms for a few topics by creating bar charts out of the c-TF-IDF scores for each topic representation. Insights can be gained from the relative c-TF-IDF scores between and within topics. Moreover, you can easily compare topic representations to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232ae500",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "editable": true,
    "id": "zpm9LsKW6mi5",
    "outputId": "1197affc-dde2-44c1-9ba7-c9fb36a1143c",
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "fig = topic_model.visualize_barchart(top_n_topics=8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c4170",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "[Link to BertTopic](https://bit.ly/4fjwU9T)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363c131",
   "metadata": {},
   "source": [
    "### BertTopic: Enhance Topic Models with Expert-Defined Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c6cf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U bertopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341f0766",
   "metadata": {},
   "source": [
    "Data scientists and analysts often need to guide their topic modeling process with domain knowledge or specific themes they want to extract, but traditional topic modeling approaches don't allow for this kind of control over the generated topics.\n",
    "\n",
    "BERTopic is a topic modeling library that leverages BERT embeddings and c-TF-IDF to create easily interpretable topics. \n",
    "\n",
    "Seed words are predefined sets of words that represent themes or topics you expect or want to find in your documents. BERTopic allows you to guide the topic modeling process using these seed words. By providing seed words, you can:\n",
    "\n",
    "- Direct the model towards specific themes of interest\n",
    "- Incorporate domain expertise into the topic discovery process\n",
    "- Ensure certain important themes are captured\n",
    "\n",
    "Here's how to implement guided topic modeling with seed words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b97ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load example data\n",
    "docs = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24273ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define seed topics\n",
    "seed_topic_list = [[\"drug\", \"cancer\", \"drugs\", \"doctor\"],\n",
    "                   [\"windows\", \"drive\", \"dos\", \"file\"],\n",
    "                   [\"space\", \"launch\", \"orbit\", \"lunar\"]]\n",
    "\n",
    "# Create and train the model with seed topics\n",
    "topic_model = BERTopic(seed_topic_list=seed_topic_list)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at three different topics in detail\n",
    "print(\"\\nFirst topic (Sports):\")\n",
    "print(topic_model.get_topic(0))\n",
    "\n",
    "print(\"\\nSecond topic (Cryptography):\")\n",
    "print(topic_model.get_topic(1))\n",
    "\n",
    "print(\"\\nFifth topic (Space Exploration):\")\n",
    "print(topic_model.get_topic(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baed6920",
   "metadata": {},
   "source": [
    "The results show how seed words influence topic discovery:\n",
    "\n",
    "- **Seed Word Integration**: In Topic 5, space-related seed words ('space', 'launch', 'orbit', 'lunar') have high weights. The model expands on these words to include related terms like 'shuttle', 'mission', and 'station'.\n",
    "- **Natural Topic Discovery**: The model discovers prominent topics like sports (Topic 0) and cryptography (Topic 1), despite being seeded with medical and computer themes. This shows that seed words guide the model without constraining it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da2139",
   "metadata": {},
   "source": [
    "[Link to BertTopic](https://github.com/MaartenGr/BERTopic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bdd3a4",
   "metadata": {},
   "source": [
    "### BertViz: Visualize NLP Model Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a49d267",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57788be2",
   "metadata": {},
   "source": [
    "Understanding how attention mechanisms work in transformer models is challenging due to the complex interactions between multiple attention heads across different layers. \n",
    "\n",
    "BertViz allows you to interactively visualize and explore attention patterns through multiple views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils\n",
    "\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "# Find popular HuggingFace models here: https://huggingface.co/models\n",
    "model_name = \"microsoft/xtremedistil-l12-h384-uncased\"  \n",
    "input_text = \"The cat sat on the mat\"\n",
    "\n",
    "# Configure model to return attention values\n",
    "model = AutoModel.from_pretrained(model_name, output_attentions=True)  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Run model\n",
    "outputs = model(inputs)\n",
    "\n",
    "# Retrieve attention from model outputs\n",
    "attention = outputs[-1]\n",
    "\n",
    "# Convert input ids to token strings\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156e3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import model_view, head_view\n",
    "\n",
    "# Display model view\n",
    "model_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c147988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display head view\n",
    "head_view(attention, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d076d",
   "metadata": {},
   "source": [
    "[Link to BertViz](https://github.com/jessevig/bertviz)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c23b446",
   "metadata": {},
   "source": [
    "### Beyond Keywords: Building a Semantic Recipe Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad69e1",
   "metadata": {},
   "source": [
    "Semantic search enables content discovery based on meaning rather than just keywords. This approach uses vector embeddings - numerical representations of text that capture semantic essence. \n",
    "\n",
    "By converting text to vector embeddings, we can quantify semantic similarity between different pieces of content in a high-dimensional vector space. This allows for comparison and search based on underlying meaning, surpassing simple keyword matching.\n",
    "\n",
    "Here's a Python implementation of semantic search for recipe recommendations using sentence-transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ca35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Prepare our data\n",
    "recipes = [\n",
    "    \"Banana and Date Sweetened Oatmeal Cookies\",\n",
    "    \"No-Bake Berry Chia Seed Pudding\",\n",
    "    \"Deep-Fried Oreo Sundae with Caramel Sauce\",\n",
    "    \"Loaded Bacon Cheeseburger Pizza\",\n",
    "]\n",
    "\n",
    "# Step 2: Load a pre-trained model for creating embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 3: Create embeddings for our recipe descriptions\n",
    "recipe_embeddings = model.encode(recipes)\n",
    "\n",
    "# Step 4: Function to find similar recipes \n",
    "def find_similar_recipes(query, top_k=2):\n",
    "    # Create embedding for the query\n",
    "    query_embedding = model.encode([query])\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarities = cosine_similarity(query_embedding, recipe_embeddings)[0]\n",
    "    \n",
    "    # Get top k similar recipes \n",
    "    top_indices = similarities.argsort()[-top_k:][::-1]\n",
    "    \n",
    "    return [(recipes[i], similarities[i]) for i in top_indices]\n",
    "\n",
    "# Step 5: Test our semantic search\n",
    "query = \"healthy dessert without sugar\"\n",
    "results = find_similar_recipes(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(\"Most similar recipes:\")\n",
    "for recipe, score in results:\n",
    "    print(f\"- {recipe} (Similarity: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff4a5c9",
   "metadata": {},
   "source": [
    "### SkillNER: Automating Skill Extraction in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd0a77",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install skillNer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddb650c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76080315",
   "metadata": {},
   "source": [
    "Extracting skills from job postings, resumes, or other unstructured text can be time-consuming if done manually. SkillNER automates this process, making it faster and more efficient.\n",
    "\n",
    "This tool can be useful for:\n",
    "- Recruiters to automate skill extraction for faster candidate screening.\n",
    "- Data scientists to extract structured data from unstructured job-related text.\n",
    "\n",
    "Here's a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0073312",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from skillNer.general_params import SKILL_DB\n",
    "from skillNer.skill_extractor_class import SkillExtractor\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Initialize the SkillExtractor\n",
    "skill_extractor = SkillExtractor(nlp, SKILL_DB, PhraseMatcher)\n",
    "\n",
    "# Sample job description\n",
    "job_description = \"\"\"\n",
    "You are a data scientist with strong expertise in Python. You have solid experience in \n",
    "data analysis and visualization, and can manage end-to-end data science projects. \n",
    "You quickly adapt to new tools and technologies, and are fluent in both English and SQL.\n",
    "\"\"\"\n",
    "\n",
    "# Extract skills from the job description\n",
    "annotations = skill_extractor.annotate(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6dd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2da9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_extractor.describe(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0c297",
   "metadata": {},
   "source": [
    "### nlpaug: Enhancing NLP Model Performance with Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead7d4c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install nlpaug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df6072",
   "metadata": {},
   "source": [
    "Limited training data for natural language processing tasks often results in overfitting and poor generalization of models.\n",
    "\n",
    "The nlpaug library offers diverse NLP data augmentation techniques, helping researchers expand datasets and create robust models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6702e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fe5c2c",
   "metadata": {},
   "source": [
    "Substitute character by keyboard distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "aug = nac.KeyboardAug()\n",
    "augmented_text = aug.augment(text)\n",
    "\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8c91b",
   "metadata": {},
   "source": [
    "Insert character randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2efc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = nac.RandomCharAug(action=\"insert\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda0787",
   "metadata": {},
   "source": [
    "Substitute word by spelling mistake words dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a761a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SpellingAug()\n",
    "augmented_texts = aug.augment(text, n=3)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Texts:\")\n",
    "print(augmented_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8a5696",
   "metadata": {},
   "source": [
    "Insert word by contextual word embeddings (BERT, DistilBERT, RoBERTA or XLNet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31009a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.ContextualWordEmbsAug(\n",
    "    model_path='bert-base-uncased', action=\"substitute\")\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c0580",
   "metadata": {},
   "source": [
    "Substitute word by WordNet's synonym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487abf82",
   "metadata": {},
   "source": [
    "Split word to two tokens randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29922c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.SplitAug()\n",
    "augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2545050",
   "metadata": {},
   "source": [
    "[Link to nlpaug](https://github.com/makcedward/nlpaug)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfc0d46",
   "metadata": {},
   "source": [
    "### GLiNER: The Lightweight Alternative to LLMs for Custom NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357fecd0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install gliner spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4f2a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f7b2c",
   "metadata": {},
   "source": [
    "Traditional NER models are limited to predefined entity types. For example, spaCy's default English model is trained to recognize only the follwoing entity types:\n",
    "\n",
    "- PERSON (e.g., \"John Smith\")\n",
    "- ORG (e.g., \"Microsoft\")\n",
    "- GPE (e.g., \"New York\")\n",
    "- DATE (e.g., \"June 15th\")\n",
    "- MONEY (e.g., \"$500\")\n",
    "\n",
    "\n",
    "Example of a traditional approach with spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ea6fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a pre-trained model with fixed entity types\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"\n",
    "Maria Rodriguez loves making sushi and pizza in her spare time. She practices yoga \n",
    "and rock climbing on weekends in Boulder, Colorado. Her friend Tom enjoys baking \n",
    "fresh croissants and often brings them when they go hiking together in the Rocky Mountains.\n",
    "\"\"\"\n",
    "\n",
    "# Can only detect pre-defined entity types\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} => {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5f3eb",
   "metadata": {},
   "source": [
    "Identifying and extracting other types of entities from text requires either training separate models for each entity type or using large language models, which results in high computational costs and resource requirements.\n",
    "\n",
    "GLiNER allows you to extract any custom entity types from text using a lightweight model. You can specify the entity types you want to extract at inference time without retraining the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868ac1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "# Initialize the model\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_medium-v2.1\")\n",
    "\n",
    "text = \"\"\"\n",
    "Maria Rodriguez loves making sushi and pizza in her spare time. She practices yoga \n",
    "and rock climbing on weekends in Boulder, Colorado. Her friend Tom enjoys baking \n",
    "fresh croissants and often brings them when they go hiking together in the Rocky Mountains.\n",
    "\"\"\"\n",
    "\n",
    "# Define custom entity types\n",
    "labels = [\"Person\", \"Food\", \"Hobby\", \"Location\"]\n",
    "\n",
    "# Extract entities\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "# Print the extracted entities\n",
    "for entity in entities:\n",
    "    print(f\"{entity['text']} => {entity['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284dc084",
   "metadata": {},
   "source": [
    "This example shows how GLiNER can identify multiple custom entity types (persons, foods, hobbies, and locations) in a single pass without needing separate models or extensive training for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26202702",
   "metadata": {},
   "source": [
    "[Link to GLiNER](https://github.com/urchade/GLiNER)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   20,
   24,
   30,
   34,
   38,
   43,
   47,
   49,
   53,
   55,
   59,
   61,
   65,
   67,
   71,
   75,
   79,
   83,
   87,
   91,
   97,
   101,
   103,
   107,
   111,
   115,
   119,
   125,
   127,
   131,
   135,
   139,
   143,
   147,
   154,
   162,
   168,
   172,
   176,
   180,
   184,
   188,
   194,
   198,
   202,
   212,
   216,
   220,
   224,
   228,
   234,
   238,
   242,
   244,
   248,
   252,
   254,
   258,
   262,
   272,
   278,
   287,
   296,
   305,
   314,
   323,
   327,
   331,
   335,
   345,
   359,
   363,
   371,
   375,
   379,
   385,
   391,
   396,
   404,
   408,
   412,
   417,
   421,
   425,
   429,
   433,
   439,
   447,
   451,
   458,
   462,
   466,
   470,
   474,
   481,
   488,
   492,
   496,
   500,
   504,
   508,
   510,
   515,
   519,
   529,
   534,
   538,
   545,
   552,
   558,
   561,
   565,
   569,
   573,
   577,
   583,
   587,
   589,
   593,
   597,
   601,
   605,
   609,
   613,
   617,
   619,
   623,
   625,
   629,
   633,
   637,
   641,
   643,
   647,
   651,
   653,
   657,
   659,
   663,
   667,
   671,
   677,
   685,
   687,
   691,
   695,
   697,
   707,
   712,
   715,
   719,
   721,
   725,
   727,
   731,
   735,
   743,
   745,
   749,
   757,
   761,
   764,
   768,
   772,
   776,
   780,
   786,
   790,
   794,
   798,
   804,
   808,
   812,
   817,
   822,
   828,
   832,
   836,
   840,
   844,
   872,
   876,
   880,
   884,
   892,
   915,
   919,
   923,
   927,
   931,
   945,
   950,
   966,
   976,
   984,
   989,
   993,
   997,
   1001,
   1005,
   1009,
   1021,
   1025,
   1029,
   1038,
   1042,
   1046,
   1050,
   1061,
   1067,
   1071,
   1134,
   1138,
   1145,
   1149,
   1157,
   1161,
   1167,
   1171,
   1185,
   1189,
   1204,
   1208,
   1212,
   1214,
   1228,
   1236,
   1247,
   1257,
   1264,
   1268,
   1272,
   1280,
   1286,
   1312,
   1319,
   1322,
   1326,
   1330,
   1338,
   1377,
   1381,
   1387,
   1391,
   1401,
   1429,
   1433,
   1435,
   1439,
   1443,
   1449,
   1453,
   1457,
   1467,
   1471,
   1478,
   1482,
   1489,
   1493,
   1501,
   1505,
   1512,
   1516,
   1523,
   1527,
   1531,
   1541,
   1549,
   1562,
   1578,
   1584,
   1605,
   1609
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}