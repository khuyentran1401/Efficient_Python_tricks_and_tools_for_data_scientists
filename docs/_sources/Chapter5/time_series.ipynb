{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4477284a",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7d351",
   "metadata": {},
   "source": [
    "This section shows some tools to work with datetime and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c57cb9",
   "metadata": {},
   "source": [
    "![](../img/datetime.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469371a2",
   "metadata": {},
   "source": [
    "### datefinder: Automatically Find Dates and Time in a Python String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a25d47",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install datefinder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d234e",
   "metadata": {},
   "source": [
    "To automatically find date and time with different formats in a Python string, use datefinder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datefinder import find_dates\n",
    "\n",
    "text = \"\"\"\"We have one meeting on May 17th, 2021 at 9:00am \n",
    "and another meeting on 5/18/2021 at 10:00. \n",
    "I hope you can attend one of the meetings.\"\"\"\n",
    "\n",
    "matches = find_dates(text)\n",
    "\n",
    "for match in matches:\n",
    "    print(\"Date and time:\", match)\n",
    "    print(\"Only day:\", match.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e83b1",
   "metadata": {},
   "source": [
    "[Link to datefinder](https://github.com/akoumjian/datefinder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a8962",
   "metadata": {},
   "source": [
    "### Fastai's add_datepart: Add Relevant DateTime Features in One Line of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793815ce",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5229ba",
   "metadata": {},
   "source": [
    "Time series analysis often benefits from date-based features like year, month, and day of week. Fastai's `add_datepart` method generates these features in one line of code, simplifying data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abeded7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fastai.tabular.core import add_datepart\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"date\": [\n",
    "            datetime(2020, 2, 5),\n",
    "            datetime(2020, 2, 6),\n",
    "            datetime(2020, 2, 7),\n",
    "            datetime(2020, 2, 8),\n",
    "        ],\n",
    "        \"val\": [1, 2, 3, 4],\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda4b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_datepart(df, \"date\")\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7773453",
   "metadata": {},
   "source": [
    "[Link to Fastai's methods to work with tabular data](https://docs.fast.ai/tabular.core.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca656b1",
   "metadata": {},
   "source": [
    "### Maya: Convert the string to datetime automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c34070",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install maya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527d79a3",
   "metadata": {},
   "source": [
    "Date string conversion to datetime can be tedious using `strptime()`. The `maya` library simplifies this process by automatically parsing date strings without needing to specify the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0670f352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import maya\n",
    "\n",
    "# Automatically parse datetime string\n",
    "string = \"2016-12-16 18:23:45.423992+00:00\"\n",
    "maya.parse(string).datetime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b520323",
   "metadata": {},
   "source": [
    "Better yet, if you want to convert the string to a different time zone (for example, CST), you can parse that into maya’s datetime function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64228d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "maya.parse(string).datetime(to_timezone=\"US/Central\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e0fd7",
   "metadata": {},
   "source": [
    "[Link to maya](https://github.com/timofurrer/maya)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef150aee",
   "metadata": {},
   "source": [
    "### Pendulum: Python Datetimes Made Easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e482aa",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8364ba",
   "metadata": {},
   "source": [
    "While native datetime instances are sufficient for simple cases, they can become difficult to work with when dealing with more complex scenarios. \n",
    "\n",
    "Conversely, Pendulum offers a more intuitive and user-friendly API, making it a convenient drop-in replacement for the standard datetime class.\n",
    "\n",
    "The examples below demonstrate the syntax differences between using the standard datetime library and Pendulum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a7d79e",
   "metadata": {},
   "source": [
    "Datetime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1747200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Creating a datetime\n",
    "now = datetime.now()\n",
    "print(f\"Current time: {now}\")\n",
    "\n",
    "# Date arithmetic\n",
    "future = now + timedelta(days=7)\n",
    "print(f\"7 days from now: {future}\")\n",
    "\n",
    "# Timezone handling\n",
    "utc_now = datetime.now(pytz.UTC)\n",
    "tokyo_tz = pytz.timezone('Asia/Tokyo')\n",
    "tokyo_time = utc_now.astimezone(tokyo_tz)\n",
    "print(f\"Time in Tokyo: {tokyo_time}\")\n",
    "\n",
    "# Parsing (requires exact format)\n",
    "parsed = datetime.strptime(\"2023-05-15 14:30:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Time difference (not human-readable)\n",
    "diff = parsed - now \n",
    "print(f\"Difference: {diff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb46f575",
   "metadata": {},
   "source": [
    "Pendulum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b00a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pendulum\n",
    "\n",
    "# Creating a datetime\n",
    "now = pendulum.now()\n",
    "print(f\"Current time: {now}\")\n",
    "\n",
    "# Date arithmetic (more intuitive than datetime)\n",
    "future = now.add(days=7)\n",
    "print(f\"7 days from now: {future}\")\n",
    "\n",
    "# Timezone handling\n",
    "tokyo_time = now.in_timezone(\"Asia/Tokyo\")\n",
    "print(f\"Time in Tokyo: {tokyo_time}\")\n",
    "\n",
    "# Parsing without specifying format\n",
    "parsed = pendulum.parse(\"2023-05-15 14:30:00\")\n",
    "print(f\"Parsed date: {parsed}\")\n",
    "\n",
    "# Human-readable differences\n",
    "diff = parsed - now\n",
    "print(f\"Difference: {diff.in_words()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b416d0",
   "metadata": {},
   "source": [
    "[Link to Pendulum](https://bit.ly/49aJ4gS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0db89a",
   "metadata": {},
   "source": [
    "### traces: A Python Library for Unevenly-Spaced Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f55f0c2",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2da0e",
   "metadata": {},
   "source": [
    "To analyze unevenly-spaced time series, you can use the `traces` library. It allows you to infer missing values in your time series based on the data you already have."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91efc194",
   "metadata": {},
   "source": [
    "Here’s an example where we log working hours for several dates but miss a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7171f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log working hours for each date\n",
    "import traces \n",
    "from datetime import datetime \n",
    "\n",
    "working_hours = traces.TimeSeries()\n",
    "working_hours[datetime(2021, 9, 10)] = 10\n",
    "working_hours[datetime(2021, 9, 12)] = 5\n",
    "working_hours[datetime(2021, 9, 13)] = 6\n",
    "working_hours[datetime(2021, 9, 16)] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3999716f",
   "metadata": {},
   "source": [
    "To retrieve the hours for dates where no data was logged, `traces` fills in the gaps using neighboring values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6e208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value on 2021/09/11\n",
    "working_hours[datetime(2021, 9, 11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value on 2021/09/14\n",
    "working_hours[datetime(2021, 9, 14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c71fa7",
   "metadata": {},
   "source": [
    "You can also calculate the distribution of working hours over a range of dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1e94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = working_hours.distribution(\n",
    "    start=datetime(2021, 9, 10),\n",
    "    end=datetime(2021, 9, 16)\n",
    ")\n",
    "distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e37781e",
   "metadata": {},
   "source": [
    "From this, we can infer that 50% of the time, 6 hours were logged per day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f515826",
   "metadata": {},
   "source": [
    "To get the median and mean working hours across the range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446b6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5294848a",
   "metadata": {},
   "source": [
    "[Link to traces](https://github.com/datascopeanalytics/traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3e8972",
   "metadata": {},
   "source": [
    "### Extract holiday from date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af06e1e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60a9b98",
   "metadata": {},
   "source": [
    "To identify holidays from a date, use the `holidays` package. This package offers a dictionary of holidays for various countries. \n",
    "\n",
    "The following code checks if \"2020-07-04\" is a US holiday and retrieves its name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6182d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import holidays\n",
    "\n",
    "us_holidays = holidays.UnitedStates()\n",
    "\n",
    "\"2014-07-04\" in us_holidays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b82e3",
   "metadata": {},
   "source": [
    "The package handles various date formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71cafc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays.get(\"2014-7-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3001e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_holidays.get(\"2014/7/4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed22e91",
   "metadata": {},
   "source": [
    "[Link to holidays](https://pypi.org/project/holidays/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb6940",
   "metadata": {},
   "source": [
    "### Workalendar: Handle Working-Day Computation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d5191",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install workalendar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd4700",
   "metadata": {},
   "source": [
    "To manage calendars, holidays, and working-day computations, use the `workalendar` package. It supports nearly 100 countries worldwide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf4bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date \n",
    "from workalendar.usa import UnitedStates\n",
    "from workalendar.asia import Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bcdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all holidays in the US\n",
    "\n",
    "US_cal = UnitedStates()\n",
    "US_cal.holidays(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_cal.is_working_day(date(2024, 9, 15)) # Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd5e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "US_cal.is_working_day(date(2024, 9, 2)) # Labor Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d9afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate working days between 2024/1/19 and 2024/5/15\n",
    "US_cal.get_working_days_delta(date(2024, 1, 19), date(2024, 5, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d83035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get holidays in Japan\n",
    "JA_cal = Japan()\n",
    "JA_cal.holidays(2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b59215",
   "metadata": {},
   "source": [
    "[Link to Workalendar](https://github.com/workalendar/workalendar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57566d73",
   "metadata": {},
   "source": [
    "### Pmdarima: Harness R's auto.arima Power with a scikit-learn-Like Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1413f475",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pmdarima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f245e04",
   "metadata": {},
   "source": [
    "To achieve functionality similar to R's `auto.arima` within a scikit-learn-like interface, use Pmdarima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca14beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load/split your data\n",
    "y = pm.datasets.load_wineind()\n",
    "train, test = train_test_split(y, train_size=150)\n",
    "\n",
    "# Fit your model\n",
    "model = pm.auto_arima(train, seasonal=True, m=12)\n",
    "\n",
    "# Make your forecasts\n",
    "forecasts = model.predict(test.shape[0])  # predict N steps into the future\n",
    "\n",
    "# Visualize the forecasts (blue=train, green=forecasts)\n",
    "x = np.arange(y.shape[0])\n",
    "plt.plot(x[:150], train, c=\"blue\")\n",
    "plt.plot(x[150:], forecasts, c=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1569ecad",
   "metadata": {},
   "source": [
    "Fitting a more complex pipeline on the sunspots dataset, serializing it, and then loading it from disk to make predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ab449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "from pmdarima.model_selection import train_test_split\n",
    "from pmdarima.pipeline import Pipeline\n",
    "from pmdarima.preprocessing import BoxCoxEndogTransformer\n",
    "import pickle\n",
    "\n",
    "# Load/split your data\n",
    "y = pm.datasets.load_sunspots()\n",
    "train, test = train_test_split(y, train_size=2700)\n",
    "\n",
    "# Define and fit your pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('boxcox', BoxCoxEndogTransformer(lmbda2=1e-6)),\n",
    "    ('arima', pm.AutoARIMA(seasonal=True, m=12,\n",
    "                           suppress_warnings=True,\n",
    "                           trace=True))\n",
    "])\n",
    "\n",
    "pipeline.fit(train)\n",
    "\n",
    "# Serialize your model just like you would in scikit:\n",
    "with open('model.pkl', 'wb') as pkl:\n",
    "    pickle.dump(pipeline, pkl)\n",
    "    \n",
    "# Load it and make predictions seamlessly:\n",
    "with open('model.pkl', 'rb') as pkl:\n",
    "    mod = pickle.load(pkl)\n",
    "    print(mod.predict(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99779179",
   "metadata": {},
   "source": [
    "[Link to Pmdarima](https://github.com/alkaline-ml/pmdarima)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf98134",
   "metadata": {},
   "source": [
    "### aeon: The Ultimate Library for Time-Series Forecasting and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a477e2",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install aeon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d4d819",
   "metadata": {},
   "source": [
    "aeon is a library for time-series data that is compatible with scikit-learn and offers a variety of advanced algorithms for learning tasks like forecasting and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b749c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from aeon.forecasting.trend import TrendForecaster\n",
    "\n",
    "y = pd.Series([20.0, 40.0, 60.0, 80.0, 100.0])\n",
    "forecaster = TrendForecaster()\n",
    "\n",
    "# fit the forecaster\n",
    "forecaster.fit(y)  \n",
    "\n",
    "# forecast the next 3 values\n",
    "forecaster.predict(fh=[1, 2, 3])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c37fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aeon.classification.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "# 3 samples and 6 time steps \n",
    "X =  np.array([[1, 2, 3, 4, 5, 5], [1, 2, 3, 4, 4, 2], [8, 7, 6, 5, 4, 4]])\n",
    "\n",
    "# class labels for each sample\n",
    "y = np.array([\"low\", \"low\", \"high\"])\n",
    "\n",
    "# Define the classifier\n",
    "clf = KNeighborsTimeSeriesClassifier(distance=\"dtw\")\n",
    "\n",
    "# fit the classifier on train data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Test data\n",
    "X_test = np.array([[2, 2, 2, 2, 2, 2], [6, 6, 6, 6, 6, 6]])\n",
    "\n",
    "# Make class predictions on new data\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d332191",
   "metadata": {},
   "source": [
    "[Link to aeon](https://github.com/aeon-toolkit/aeon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67149c",
   "metadata": {},
   "source": [
    "### Ruptures: Detecting Change Points in Non-Stationary Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffde572",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install ruptures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67dcaf",
   "metadata": {},
   "source": [
    "Use ruptures to detect change points from non-stationary signals such as trend, seasonality, and variance.\n",
    "\n",
    "With change points, you can detect anomalies or deviations from the expected behavior and gain insights into when these transitions occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4f3771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import ruptures as rpt\n",
    "\n",
    "# generate signal\n",
    "n_samples, n_features, sigma = 1000, 3, 3\n",
    "num_breakpoints = 3\n",
    "signal, true_breakpoints = rpt.pw_constant(\n",
    "    n_samples, n_features, num_breakpoints, noise_std=sigma\n",
    ")\n",
    "\n",
    "# detection\n",
    "algo = rpt.Pelt(model=\"rbf\").fit(signal)\n",
    "predicted_breakpoints = algo.predict(pen=10)\n",
    "\n",
    "# display\n",
    "rpt.display(signal, predicted_breakpoints)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c2494",
   "metadata": {},
   "source": [
    "[Link to ruptures](https://github.com/deepcharles/ruptures)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f786b09",
   "metadata": {},
   "source": [
    "### GluonTS: Probabilistic Time Series Modeling in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77bed1",
   "metadata": {},
   "source": [
    "Probabilistic models offer a range of possible future outcomes, rather than a single fixed prediction, allowing for the assessment of risk associated with adverse events. \n",
    "\n",
    "GluonTS streamlines the process of using probabilistic models for time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80e292",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import split\n",
    "from gluonts.torch import DeepAREstimator\n",
    "\n",
    "# Load data from a CSV file into a PandasDataset\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/AileenNielsen/\"\n",
    "    \"TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\",\n",
    "    index_col=0,\n",
    "    parse_dates=True,\n",
    ")\n",
    "dataset = PandasDataset(df, target=\"#Passengers\")\n",
    "\n",
    "# Split the data for training and testing\n",
    "training_data, test_gen = split(dataset, offset=-36)\n",
    "test_data = test_gen.generate_instances(prediction_length=12, windows=3)\n",
    "\n",
    "# Train the model and make predictions\n",
    "model = DeepAREstimator(\n",
    "    prediction_length=12, freq=\"M\", trainer_kwargs={\"max_epochs\": 5}\n",
    ").train(training_data)\n",
    "\n",
    "forecasts = list(model.predict(test_data.input))\n",
    "\n",
    "# Plot predictions\n",
    "plt.plot(df[\"1954\":], color=\"black\")\n",
    "for forecast in forecasts:\n",
    "  forecast.plot()\n",
    "plt.legend([\"True values\"], loc=\"upper left\", fontsize=\"xx-large\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ce06ce",
   "metadata": {},
   "source": [
    "![](../img/gluonts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ebb16f",
   "metadata": {},
   "source": [
    "[Link to GluonTS](https://github.com/awslabs/gluonts)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8fe663",
   "metadata": {},
   "source": [
    "### tfcausalimpact: Understand Causal Relationships in Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4baf0e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install tfcausalimpact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c838171a",
   "metadata": {},
   "source": [
    "You're running a marketing campaign and see a user increase. But how do you know if it's due to the campaign or just a coincidence?\n",
    "\n",
    "That is when tfcausalimpact comes in handy. It forecasts future data trends using a Bayesian structural model and compares them to actual data to give you meaningful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9142d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from causalimpact import CausalImpact\n",
    "\n",
    "\n",
    "data = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/WillianFuks/tfcausalimpact/master/tests/fixtures/arma_data.csv\"\n",
    ")[[\"y\", \"X\"]]\n",
    "data.iloc[70:, 0] += 5\n",
    "\n",
    "\n",
    "ci = CausalImpact(data, pre_period=[0, 69], post_period=[70, 99])\n",
    "print(ci.summary())\n",
    "print(ci.summary(output=\"report\"))\n",
    "ci.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3d3fd",
   "metadata": {},
   "source": [
    "Posterior Inference {Causal Impact}\n",
    "                          Average            Cumulative\n",
    "Actual                    125.23             3756.86\n",
    "Prediction (s.d.)         120.33 (0.3)       3609.94 (9.11)\n",
    "95% CI                    [119.75, 120.94]   [3592.64, 3628.34]\n",
    "\n",
    "Absolute effect (s.d.)    4.9 (0.3)          146.93 (9.11)\n",
    "95% CI                    [4.28, 5.47]       [128.52, 164.23]\n",
    "\n",
    "Relative effect (s.d.)    4.07% (0.25%)      4.07% (0.25%)\n",
    "95% CI                    [3.56%, 4.55%]     [3.56%, 4.55%]\n",
    "\n",
    "Posterior tail-area probability p: 0.0\n",
    "Posterior prob. of a causal effect: 100.0%\n",
    "\n",
    "For more details run the command: `print(impact.summary('report'))`\n",
    "\n",
    "Analysis report {CausalImpact}\n",
    "\n",
    "\n",
    "During the post-intervention period, the response variable had\n",
    "an average value of approx. 125.23. By contrast, in the absence of an\n",
    "intervention, we would have expected an average response of 120.33.\n",
    "The 95% interval of this counterfactual prediction is [119.75, 120.94].\n",
    "Subtracting this prediction from the observed response yields\n",
    "an estimate of the causal effect the intervention had on the\n",
    "response variable. This effect is 4.9 with a 95% interval of\n",
    "[4.28, 5.47]. For a discussion of the significance of this effect,\n",
    "see below.\n",
    "\n",
    "\n",
    "Summing up the individual data points during the post-intervention\n",
    "period (which can only sometimes be meaningfully interpreted), the\n",
    "response variable had an overall value of 3756.86.\n",
    "By contrast, had the intervention not taken place, we would have expected\n",
    "a sum of 3609.94. The 95% interval of this prediction is [3592.64, 3628.34].\n",
    "\n",
    "\n",
    "The above results are given in terms of absolute numbers. In relative\n",
    "terms, the response variable showed an increase of +4.07%. The 95%\n",
    "interval of this percentage is [3.56%, 4.55%].\n",
    "\n",
    "\n",
    "This means that the positive effect observed during the intervention\n",
    "period is statistically significant and unlikely to be due to random\n",
    "fluctuations. It should be noted, however, that the question of whether\n",
    "this increase also bears substantive significance can only be answered\n",
    "by comparing the absolute effect (4.9) to the original goal\n",
    "of the underlying intervention.\n",
    "\n",
    "\n",
    "The probability of obtaining this effect by chance is very small\n",
    "(Bayesian one-sided tail-area probability p = 0.0).\n",
    "This means the causal effect can be considered statistically\n",
    "significant.\n",
    "\n",
    "![](../img/causal_impact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c75dd67",
   "metadata": {},
   "source": [
    "[Link to tfcausalimpact](https://github.com/WillianFuks/tfcausalimpact)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58685e2",
   "metadata": {},
   "source": [
    "### QuantStats: Simplify Stock Performance Analysis in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0183da",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install quantstats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b952ac9",
   "metadata": {},
   "source": [
    "To visualize and analyze the performance of specific stocks using just a few lines of Python, try QuantStats.\n",
    "\n",
    "The code below shows how to use QuantStats to visualize stock performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b34809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "qs.extend_pandas()\n",
    "\n",
    "# fetch the daily returns for a stock\n",
    "stock = qs.utils.download_returns('SPY')\n",
    "\n",
    "# visualize stock performance\n",
    "qs.plots.snapshot(stock, title='SPY Performance', show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51671ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs.reports.html(stock, \"SPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38917bb",
   "metadata": {},
   "source": [
    "Running the code above will generate a report that looks similar to this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bff987",
   "metadata": {},
   "source": [
    "![](../img/quanstats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b79ae3",
   "metadata": {},
   "source": [
    "[Link to QuantStats](https://github.com/ranaroussi/quantstats)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef058d",
   "metadata": {},
   "source": [
    "### kneed: Knee-Point Detection in Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c80fe",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install \"kneed[plot]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8884ef90",
   "metadata": {},
   "source": [
    "Knee-point detection in time series identifies the point of maximum curvature. The knee point can identify anomalies or outliers in the time series. If a data point is far away from the knee point, it may indicate an anomaly or unexpected behavior.\n",
    "\n",
    "The kneed library makes it easy to implement knee-point detection in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aae6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import DataGenerator, KneeLocator\n",
    "\n",
    "x, y = DataGenerator.figure2()\n",
    "\n",
    "kneedle = KneeLocator(x, y, S=1.0, curve=\"concave\", direction=\"increasing\")\n",
    "kneedle.plot_knee_normalized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec06417a",
   "metadata": {},
   "source": [
    "[Link to kneed](https://github.com/arvkevi/kneed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883e4a38",
   "metadata": {},
   "source": [
    "### NeuralForecast: Streamline Neural Forecasting with Familiar Sklearn Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b5cc48",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pip install neuralforecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154dcbc2",
   "metadata": {},
   "source": [
    "Neural forecasting methods can enhance the accuracy of forecasting, but they are often difficult to use and computationally expensive.\n",
    "\n",
    "NeuralForecast provides a simple way to use proven accurate and efficient models, using familiar sklearn syntax. The models available in NeuralForecast range from classic networks like RNN to the latest transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d570edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast import NeuralForecast\n",
    "from neuralforecast.models import NBEATS\n",
    "from neuralforecast.utils import AirPassengersDF\n",
    "\n",
    "nf = NeuralForecast(models=[NBEATS(input_size=24, h=12, max_steps=100)], freq=\"M\")\n",
    "\n",
    "nf.fit(df=AirPassengersDF)\n",
    "nf.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52bac01",
   "metadata": {},
   "source": [
    "[Link to NeuralForecast](https://github.com/Nixtla/neuralforecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f6faef",
   "metadata": {},
   "source": [
    "### Scaling Time-Series Forecasting with StatsForecast and Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3aaf8c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install statsforecast pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a639ddfb",
   "metadata": {},
   "source": [
    "Traditional time series libraries are typically built to run in-memory on single machines, which poses challenges when handling extremely large datasets.\n",
    "\n",
    "StatsForecast, however, provides seamless compatibility with Spark, allowing users to perform scalable and efficient time-series forecasting on large datasets directly within Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6ce7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.config(\n",
    "    \"spark.executorEnv.NIXTLA_ID_AS_COL\", \"1\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fc7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsforecast.core import StatsForecast\n",
    "from statsforecast.models import AutoETS\n",
    "from statsforecast.utils import generate_series\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "n_series = 4\n",
    "horizon = 7\n",
    "\n",
    "series = generate_series(n_series)\n",
    "\n",
    "# Convert to Spark\n",
    "spark_df = spark.createDataFrame(series)\n",
    "spark_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd45e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = StatsForecast(models=[AutoETS(season_length=7)], freq=\"D\")\n",
    "\n",
    "# Returns a Spark DataFrame\n",
    "sf.forecast(df=spark_df, h=horizon, level=[90]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656999dd",
   "metadata": {},
   "source": [
    "[Link to StatsForecast.](https://github.com/Nixtla/statsforecast/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c1e20",
   "metadata": {},
   "source": [
    "### Beyond Point Estimates: Leverage Prediction Intervals for Robust Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e20444",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mlforecast utilsforecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4f48fd",
   "metadata": {},
   "source": [
    "Generating a forecast typically produces a single point estimate, which does not reflect the uncertainty associated with the prediction.\n",
    "\n",
    "To quantify this uncertainty, we need prediction intervals - a range of values the forecast can take with a given probability. MLForecast allows you to train sklearn models to generate both point forecasts and prediction intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baa003f",
   "metadata": {},
   "source": [
    "To demonstrate this, let's consider the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db77f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utilsforecast.plotting import plot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffebc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"https://auto-arima-results.s3.amazonaws.com/M4-Hourly.csv\")\n",
    "test = pd.read_csv(\"https://auto-arima-results.s3.amazonaws.com/M4-Hourly-test.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7eb00",
   "metadata": {},
   "source": [
    "We’ll only use the first series of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1adb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_series = 1\n",
    "uids = train[\"unique_id\"].unique()[:n_series]  \n",
    "train = train.query(\"unique_id in @uids\")\n",
    "test = test.query(\"unique_id in @uids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c8a106",
   "metadata": {},
   "source": [
    "Plot these series using the `plot_series` function from the utilsforecast library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9682111",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_series(\n",
    "    df=train,\n",
    "    forecasts_df=test.rename(columns={\"y\": \"y_test\"}),\n",
    "    models=[\"y_test\"],\n",
    "    palette=\"tab10\",\n",
    ")\n",
    "\n",
    "fig.set_size_inches(8, 3)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645db1af",
   "metadata": {},
   "source": [
    "Train multiple models that follow the sklearn syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06d406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from mlforecast.utils import PredictionIntervals\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee273970",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf = MLForecast(\n",
    "    models=[\n",
    "        LinearRegression(),\n",
    "        KNeighborsRegressor(),\n",
    "    ],\n",
    "    freq=1,\n",
    "    target_transforms=[Differences([1])],\n",
    "    lags=[24 * (i + 1) for i in range(7)],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927cabe3",
   "metadata": {},
   "source": [
    "Apply the feature engineering and train the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b82322",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlf.fit(\n",
    "    df=train,\n",
    "    prediction_intervals=PredictionIntervals(n_windows=10, h=48),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7989530b",
   "metadata": {},
   "source": [
    "Generate forecasts with prediction intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2d2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of floats with the confidence levels of the prediction intervals\n",
    "levels = [50, 80, 95]\n",
    "\n",
    "# Predict the next 48 hours\n",
    "horizon = 48\n",
    "\n",
    "# Generate forecasts with prediction intervals\n",
    "forecasts = mlf.predict(h=horizon, level=levels)\n",
    "forecasts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3740981",
   "metadata": {},
   "source": [
    "Merge the test data with forecasts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ef985",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_with_forecasts = test.merge(forecasts, how=\"left\", on=[\"unique_id\", \"ds\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c53c6",
   "metadata": {},
   "source": [
    "Plot the point and the prediction intervals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [50, 80, 95]\n",
    "fig = plot_series(\n",
    "    train,\n",
    "    test_with_forecasts,\n",
    "    plot_random=False,\n",
    "    models=[\"KNeighborsRegressor\"],\n",
    "    level=levels,\n",
    "    max_insample_length=48,\n",
    "    palette='tab10',\n",
    ")\n",
    "fig.set_size_inches(8, 4)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae89108",
   "metadata": {},
   "source": [
    "[Link to MLForecast](https://github.com/Nixtla/mlforecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60748c39",
   "metadata": {},
   "source": [
    "### Sliding Window Approach to Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5e442",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mlforecast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3698d7",
   "metadata": {},
   "source": [
    "Time series cross-validation evaluates a model's predictive performance by training on past data and testing on subsequent time periods using a sliding window approach. \n",
    "\n",
    "MLForecast offers an efficient and easy-to-use implementation of this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407febad",
   "metadata": {},
   "source": [
    "To see how to implement time series cross-validation with MLForecast, let's start reading a subset of the M4 Competition hourly dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bcdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utilsforecast.plotting import plot_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cdddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df = pd.read_csv(\"https://datasets-nixtla.s3.amazonaws.com/m4-hourly.csv\").query(\n",
    "    \"unique_id == 'H1'\"\n",
    ")\n",
    "print(Y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab531e",
   "metadata": {},
   "source": [
    "Plot the time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_series(Y_df, plot_random=False, max_insample_length=24 * 14)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca518f59",
   "metadata": {},
   "source": [
    "Instantiate a new MLForecast object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb169324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "mlf = MLForecast(\n",
    "    models=[LinearRegression()],\n",
    "    freq=1,\n",
    "    target_transforms=[Differences([24])],\n",
    "    lags=range(1, 25),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fca1cea",
   "metadata": {},
   "source": [
    "Once the MLForecast object has been instantiated, we can use the `cross_validation` method.\n",
    "\n",
    "For this particular example, we’ll use 3 windows of 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d4101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 3 windows of 24 hours\n",
    "cross_validation_df = mlf.cross_validation(\n",
    "    df=Y_df,\n",
    "    h=24,\n",
    "    n_windows=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e34cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cross_validation_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711975d2",
   "metadata": {},
   "source": [
    "We’ll now plot the forecast for each cutoff period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f1bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_cv(df, df_cv, last_n=24 * 14):\n",
    "    cutoffs = df_cv[\"cutoff\"].unique()\n",
    "    fig, ax = plt.subplots(\n",
    "        nrows=len(cutoffs), ncols=1, figsize=(14, 6), gridspec_kw=dict(hspace=0.8)\n",
    "    )\n",
    "    for cutoff, axi in zip(cutoffs, ax.flat):\n",
    "        df.tail(last_n).set_index(\"ds\").plot(ax=axi, y=\"y\")\n",
    "        df_cv.query(\"cutoff == @cutoff\").set_index(\"ds\").plot(\n",
    "            ax=axi,\n",
    "            y=\"LinearRegression\",\n",
    "            title=f\"{cutoff=}\",\n",
    "        )\n",
    "\n",
    "\n",
    "plot_cv(Y_df, cross_validation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c3759",
   "metadata": {},
   "source": [
    "Notice that in each cutoff period, we generated a forecast for the next 24 hours using only the data y before said period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244dd386",
   "metadata": {},
   "source": [
    "[Link to MLForecast](https://github.com/Nixtla/mlforecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f138cbae",
   "metadata": {},
   "source": [
    "### Hierarchical Forecasting in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09726726",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install hierarchicalforecast\n",
    "!pip install -U statsforecast numba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a446e5",
   "metadata": {},
   "source": [
    "In complex datasets, forecasts at detailed levels (e.g., regions, products) should align with higher-level forecasts (e.g., countries, categories). Inconsistent forecasts can lead to poor decisions.\n",
    "\n",
    "Hierarchical forecasting ensures forecasts are consistent across all levels to reconcile and match forecasts from lower to higher levels.\n",
    "\n",
    "HierarchicalForecast, an open-source library from Nixtla, offers tools and methods specifically designed for creating and reconciling hierarchical forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f4ae73",
   "metadata": {},
   "source": [
    "For illustrative purposes, consider a sales dataset with the following columns:\n",
    "\n",
    "*   **Country**: The country where the sales occurred.\n",
    "*   **Region**: The region within the country.\n",
    "*   **State**: The state within the region.\n",
    "*   **Purpose**: The purpose of the sale (e.g., Business, Leisure).\n",
    "*   **ds**: The date of the sale.\n",
    "*   **y**: The sales amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ea095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da38eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "Y_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/tourism.csv')\n",
    "Y_df = Y_df.rename({'Trips': 'y', 'Quarter': 'ds'}, axis=1)\n",
    "Y_df.insert(0, 'Country', 'Australia')\n",
    "Y_df = Y_df[['Country', 'State', 'Region', 'Purpose', 'ds', 'y']]\n",
    "Y_df['ds'] = Y_df['ds'].str.replace(r'(\\d+) (Q\\d)', r'\\1-\\2', regex=True)\n",
    "Y_df['ds'] = pd.to_datetime(Y_df['ds'])\n",
    "Y_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b939b68",
   "metadata": {},
   "source": [
    "The dataset can be grouped in the following non-strictly hierarchical structure:\n",
    "\n",
    "*   **Country**\n",
    "*   **Country, State**\n",
    "*   **Country, Purpose**\n",
    "*   **Country, State, Region**\n",
    "*   **Country, State, Purpose**\n",
    "*   **Country, State, Region, Purpose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = [\n",
    "    ['Country'],\n",
    "    ['Country', 'State'], \n",
    "    ['Country', 'Purpose'], \n",
    "    ['Country', 'State', 'Region'], \n",
    "    ['Country', 'State', 'Purpose'], \n",
    "    ['Country', 'State', 'Region', 'Purpose']\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157df77f",
   "metadata": {},
   "source": [
    "Using the aggregate function from HierarchicalForecast we can get the full set of time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c86f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalforecast.utils import aggregate\n",
    "\n",
    "Y_df, S_df, tags = aggregate(Y_df, spec)\n",
    "Y_df = Y_df.reset_index()\n",
    "Y_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aefb017",
   "metadata": {},
   "source": [
    "Get all the distinct 'Country/Purpose' combinations present in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags['Country/Purpose']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a311d",
   "metadata": {},
   "source": [
    "We use the final two years (8 quarters) as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6857a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_df = Y_df.groupby('unique_id').tail(8)\n",
    "Y_train_df = Y_df.drop(Y_test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d39edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_df = Y_test_df.set_index('unique_id')\n",
    "Y_train_df = Y_train_df.set_index('unique_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a958a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_df.groupby('unique_id').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923f1be",
   "metadata": {},
   "source": [
    "The following cell generates **base forecasts** for each time series in `Y_df` using the `ETS` model. The forecasts and fitted values are stored in `Y_hat_df` and `Y_fitted_df`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbcc174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from statsforecast.models import ETS\n",
    "from statsforecast.core import StatsForecast\n",
    "\n",
    "fcst = StatsForecast(df=Y_train_df, \n",
    "                     models=[ETS(season_length=4, model='ZZA')], \n",
    "                     freq='QS', n_jobs=-1)\n",
    "Y_hat_df = fcst.forecast(h=8, fitted=True)\n",
    "Y_fitted_df = fcst.forecast_fitted_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333bdb68",
   "metadata": {},
   "source": [
    "Since `Y_hat_df` contains forecasts that are not coherent—meaning forecasts at detailed levels (e.g., by State, Region, Purpose) may not align with those at higher levels (e.g., by Country, State, Purpose)—we will use the `HierarchicalReconciliation` class with the `BottomUp` approach to ensure coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e937a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hierarchicalforecast.methods import BottomUp\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "\n",
    "reconcilers = [BottomUp()]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "Y_rec_df = hrec.reconcile(Y_hat_df=Y_hat_df, Y_df=Y_fitted_df, S=S_df, tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8f46f0",
   "metadata": {},
   "source": [
    "The dataframe `Y_rec_df` contains the reconciled forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b21a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_rec_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff7842",
   "metadata": {},
   "source": [
    "[Link to Hierarchical Forecast](https://github.com/Nixtla/hierarchicalforecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa49e6",
   "metadata": {},
   "source": [
    "### Generative Pre-trained Forecasting with TimeGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5831cf0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install nixtla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60633614",
   "metadata": {},
   "source": [
    "TimeGPT is a powerful generative pre-trained forecasting model that can generate accurate forecasts for new time series without the need for training. TimeGPT can be used across a variety of tasks including demand forecasting, anomaly detection, financial forecasting, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94884d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nixtla import NixtlaClient\n",
    "\n",
    "nixtla_client = NixtlaClient(api_key=\"my_api_key_provided_by_nixtla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b507c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "time_column = \"Month\"\n",
    "value_column = \"Sales\"\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/monthly-car-sales.csv\",\n",
    "    parse_dates=[time_column],\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc77b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client.plot(df, time_col=time_column, target_col=value_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timegpt_fcst_df = nixtla_client.forecast(\n",
    "    df=df, h=12, freq=\"MS\", time_col=time_column, target_col=value_column\n",
    ")\n",
    "timegpt_fcst_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97945166",
   "metadata": {},
   "outputs": [],
   "source": [
    "nixtla_client.plot(df, timegpt_fcst_df, time_col=time_column, target_col=value_column, max_insample_length=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a5937",
   "metadata": {},
   "source": [
    "[Link to TimeGPT](https://github.com/Nixtla/nixtla)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d9d42a",
   "metadata": {},
   "source": [
    "### Automate Time Series Feature Engineering with tsfresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4525ad2",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install tsfresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901a177",
   "metadata": {},
   "source": [
    "Data scientists spend much of their time cleaning data or building features. While the former is unavoidable, the latter can be automated. \n",
    "\n",
    "tsfresh uses a robust feature selection algorithm to automatically extract relevant time series features, freeing up data scientists' time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc35df0",
   "metadata": {},
   "source": [
    "To demonstrate this, start with loading an example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.examples.robot_execution_failures import (\n",
    "    download_robot_execution_failures,\n",
    "    load_robot_execution_failures,\n",
    ")\n",
    "\n",
    "download_robot_execution_failures()\n",
    "timeseries, y = load_robot_execution_failures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef9a5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd0657",
   "metadata": {},
   "source": [
    "Extract features and select only relevant features for each time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a76f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh import extract_relevant_features\n",
    "\n",
    "# extract relevant features\n",
    "features_filtered = extract_relevant_features(\n",
    "    timeseries, y, column_id=\"id\", column_sort=\"time\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95379679",
   "metadata": {},
   "source": [
    "You can now use the features in `features_filtered` to train your classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9834d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform model training with the extracted features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266f91b",
   "metadata": {},
   "source": [
    "[Link to tsfresh](https://github.com/blue-yonder/tsfresh)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7593d",
   "metadata": {},
   "source": [
    "### tsmoothie: Fast and Flexible Tool for Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b84bce5",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pip install --upgrade tsmoothie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dade8bf",
   "metadata": {},
   "source": [
    "Exponential smoothing is useful for capturing the underlying pattern in the data, especially for data with a strong trend or seasonal component.\n",
    "\n",
    "tsmoothie is designed to be fast and efficient and provides a wide range of smoothing techniques.\n",
    "\n",
    "To see how tsmoothie works, let's generate a single random walk time series of length 200 using the `sim_randomwalk()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602502ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tsmoothie.utils_func import sim_randomwalk\n",
    "from tsmoothie.smoother import LowessSmoother\n",
    "\n",
    "# generate a random walk of length 200\n",
    "np.random.seed(123)\n",
    "data = sim_randomwalk(n_series=1, timesteps=200, process_noise=10, measure_noise=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9602f2b2",
   "metadata": {},
   "source": [
    "Next, create a `LowessSmoother` object with a `smooth_fraction` of `0.1` (i.e., 10% of the data points are used for local regression) and 1 iteration. We then apply the smoothing operation to the data using the `smooth()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850eadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operate smoothing\n",
    "smoother = LowessSmoother(smooth_fraction=0.1, iterations=1)\n",
    "smoother.smooth(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bf960d",
   "metadata": {},
   "source": [
    "After smoothing the data, we use the `get_intervals()` method of the `LowessSmoother` object to calculate the lower and upper bounds of the prediction interval for the smoothed time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a79d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate intervals\n",
    "low, up = smoother.get_intervals(\"prediction_interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffbbfd",
   "metadata": {},
   "source": [
    "Finally, we plot the smoothed time series (as a blue line), and the prediction interval (as a shaded region) using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada39b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the smoothed time series with intervals\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(smoother.smooth_data[0], linewidth=3, color=\"blue\")\n",
    "plt.plot(smoother.data[0], \".k\")\n",
    "plt.title(f\"timeseries\")\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "plt.fill_between(range(len(smoother.data[0])), low[0], up[0], alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841161dc",
   "metadata": {},
   "source": [
    "This graph effectively highlights the trend and seasonal components present in the time series data through the use of a smoothed representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496d454",
   "metadata": {},
   "source": [
    "[Link to tsmoothie](https://github.com/cerlymarco/tsmoothie)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195e8ce8",
   "metadata": {},
   "source": [
    "### Backtesting: Assess Trading Strategy Performance Effortlessly in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fa93b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U backtesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cde32",
   "metadata": {},
   "source": [
    "Evaluating trading strategies' effectiveness is crucial for financial decision-making, but it's challenging due to the complexities of historical data analysis and strategy testing. \n",
    "\n",
    "Backtesting allows users to simulate trades based on historical data and visualize the outcomes through interactive plots in three lines of code.\n",
    "\n",
    "To see how Backtesting works, let's create our first strategy to backtest on these Google data, a simple moving average (MA) cross-over strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting.test import GOOG\n",
    "\n",
    "print(GOOG.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95535443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def SMA(values, n):\n",
    "    \"\"\"\n",
    "    Return simple moving average of `values`, at\n",
    "    each step taking into account `n` previous values.\n",
    "    \"\"\"\n",
    "    return pd.Series(values).rolling(n).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1fcfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting import Strategy\n",
    "from backtesting.lib import crossover\n",
    "\n",
    "\n",
    "class SmaCross(Strategy):\n",
    "    # Define the two MA lags as *class variables*\n",
    "    # for later optimization\n",
    "    n1 = 10\n",
    "    n2 = 20\n",
    "\n",
    "    def init(self):\n",
    "        # Precompute the two moving averages\n",
    "        self.sma1 = self.I(SMA, self.data.Close, self.n1)\n",
    "        self.sma2 = self.I(SMA, self.data.Close, self.n2)\n",
    "\n",
    "    def next(self):\n",
    "        # If sma1 crosses above sma2, close any existing\n",
    "        # short trades, and buy the asset\n",
    "        if crossover(self.sma1, self.sma2):\n",
    "            self.position.close()\n",
    "            self.buy()\n",
    "\n",
    "        # Else, if sma1 crosses below sma2, close any existing\n",
    "        # long trades, and sell the asset\n",
    "        elif crossover(self.sma2, self.sma1):\n",
    "            self.position.close()\n",
    "            self.sell()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f15d3",
   "metadata": {},
   "source": [
    "To assess the performance of our investment strategy, we will instantiate a `Backtest` object, using Google stock data as our asset of interest and incorporating the `SmaCross` strategy class. We'll start with an initial cash balance of 10,000 units and set the broker's commission to a realistic rate of 0.2%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backtesting import Backtest\n",
    "\n",
    "bt = Backtest(GOOG, SmaCross, cash=10_000, commission=.002)\n",
    "stats = bt.run()\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e41b0",
   "metadata": {},
   "source": [
    "Plot the outcomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a89377",
   "metadata": {},
   "outputs": [],
   "source": [
    "bt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e431cefb",
   "metadata": {},
   "source": [
    "![](../img/backtesting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65fd38",
   "metadata": {},
   "source": [
    "[Link to Backtesting](https://github.com/kernc/backtesting.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda7b19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Chronos: Unleashing Pre-trained Language Models for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a5a0b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/amazon-science/chronos-forecasting.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5745881",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Probabilistic time series forecasting helps data scientists and analysts predict future values with uncertainty estimates. To leverage pre-trained language models for accurate time series predictions, use Chronos.\n",
    "\n",
    "The power of Chronos lies in its ability to generate accurate forecasts right out of the box, eliminating the need for extensive model training or fine-tuning in many cases.\n",
    "\n",
    "Here's a quick example of using Chronos for forecasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e7ec1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/AileenNielsen/TimeSeriesAnalysisWithPython/master/data/AirPassengers.csv\"\n",
    ")\n",
    "\n",
    "# forecast shape: [num_series, num_samples, prediction_length]\n",
    "forecast = pipeline.predict(\n",
    "    context=torch.tensor(df[\"#Passengers\"]), prediction_length=12, num_samples=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b98d23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "This code loads the air passenger data, uses the pretrained Chronos model to generate 20 possible forecasts for the next 12 months, and then calculates the median forecast along with a 90% prediction interval.\n",
    "\n",
    "The Chronos model internally transforms the numerical time series into tokens, processes them through a language model architecture, and then converts the output back into numerical forecasts. This approach allows it to capture complex patterns and dependencies in the data, potentially outperforming traditional forecasting methods, especially in zero-shot scenarios where the model hasn't been specifically trained on the target time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3eef0b",
   "metadata": {},
   "source": [
    "[Link to Chronos](https://github.com/amazon-science/chronos-forecasting)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   20,
   24,
   28,
   32,
   36,
   48,
   53,
   57,
   61,
   65,
   85,
   88,
   92,
   96,
   100,
   104,
   110,
   114,
   116,
   120,
   124,
   128,
   136,
   140,
   164,
   168,
   190,
   194,
   198,
   202,
   206,
   210,
   220,
   224,
   229,
   232,
   236,
   242,
   246,
   250,
   254,
   256,
   260,
   264,
   268,
   274,
   281,
   285,
   289,
   291,
   295,
   299,
   303,
   307,
   313,
   320,
   324,
   328,
   333,
   337,
   341,
   345,
   349,
   353,
   374,
   378,
   407,
   411,
   415,
   419,
   423,
   437,
   459,
   463,
   467,
   471,
   477,
   495,
   499,
   503,
   509,
   547,
   551,
   555,
   559,
   563,
   569,
   584,
   644,
   648,
   652,
   656,
   662,
   674,
   676,
   680,
   684,
   688,
   692,
   696,
   702,
   709,
   713,
   717,
   721,
   727,
   736,
   740,
   744,
   748,
   754,
   762,
   778,
   783,
   787,
   791,
   795,
   801,
   805,
   810,
   814,
   818,
   823,
   827,
   837,
   841,
   849,
   859,
   863,
   868,
   872,
   882,
   886,
   888,
   892,
   905,
   909,
   913,
   917,
   923,
   927,
   932,
   937,
   941,
   944,
   948,
   960,
   966,
   975,
   977,
   981,
   1000,
   1004,
   1008,
   1012,
   1018,
   1026,
   1037,
   1042,
   1051,
   1063,
   1072,
   1076,
   1082,
   1086,
   1088,
   1092,
   1097,
   1102,
   1104,
   1108,
   1118,
   1122,
   1129,
   1134,
   1136,
   1140,
   1144,
   1148,
   1152,
   1158,
   1171,
   1175,
   1182,
   1184,
   1188,
   1192,
   1196,
   1202,
   1206,
   1216,
   1218,
   1222,
   1229,
   1233,
   1235,
   1239,
   1243,
   1247,
   1255,
   1264,
   1268,
   1272,
   1276,
   1279,
   1283,
   1293,
   1297,
   1301,
   1305,
   1309,
   1317,
   1323,
   1335,
   1363,
   1367,
   1373,
   1377,
   1379,
   1383,
   1387,
   1391,
   1401,
   1409,
   1433,
   1439
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}