{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7820a007",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cbe2bd5",
   "metadata": {},
   "source": [
    "![](../img/test.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d27151c",
   "metadata": {},
   "source": [
    "### Efficiently Resume Work After Breaks with Failing Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da73f044",
   "metadata": {},
   "source": [
    "Do you forget what feature to implement when taking a break from work? \n",
    "\n",
    "To keep your train of thought, write a unit test that describes the desired behavior of the feature and makes it fail intentionally.\n",
    "\n",
    "This will give you a clear idea of what to work on when returning to the project, allowing you to get back on track faster."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb60f354",
   "metadata": {},
   "source": [
    "```python\n",
    "def calculate_average(nums: list):\n",
    "    return sum(nums)/len(nums) \n",
    "    # TODO: code to handle an empty list\n",
    "\n",
    "def test_calculate_average_two_nums():\n",
    "    # Will work\n",
    "    nums = [2, 3]\n",
    "    assert calculate_average(nums) == 2.5\n",
    "\n",
    "def test_calculate_average_empty_list():\n",
    "    # Will fail intentionally\n",
    "    nums = []\n",
    "    return calculate_average(nums) == 0 \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1084abc7",
   "metadata": {},
   "source": [
    "### Choose a Descriptive Name Over a Short One When Naming Your Function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13177a59",
   "metadata": {},
   "source": [
    "Using a short and unclear name for a testing function may lead to confusion and misunderstandings. To make your tests more readable, use a descriptive name instead, even if it results in a longer name. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a34b1732",
   "metadata": {},
   "source": [
    "Instead of this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffa99266",
   "metadata": {},
   "source": [
    "```python\n",
    "def contain_word(word: str, text: str):\n",
    "    return word in text\n",
    "\n",
    "\n",
    "def test_contain_word_1():\n",
    "    assert contain_word(word=\"duck\", text=\"This is a duck\")\n",
    "\n",
    "\n",
    "def test_contain_word_2():\n",
    "    assert contain_word(word=\"duck\", text=\"This is my coworker, Mr. Duck\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ee780d5",
   "metadata": {},
   "source": [
    "Write this:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45446ca6",
   "metadata": {},
   "source": [
    "```python\n",
    "def contain_word(word: str, text: str):\n",
    "    return word in text\n",
    "\n",
    "\n",
    "def test_contain_word_exact():\n",
    "    assert contain_word(word=\"duck\", text=\"This is a duck\")\n",
    "\n",
    "\n",
    "def test_contain_word_different_case():\n",
    "    assert contain_word(word=\"duck\", text=\"This is my coworker, Mr. Duck\")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c57c914",
   "metadata": {},
   "source": [
    "### pytest benchmark: A Pytest Fixture to Benchmark Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c189b0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aaf852a8",
   "metadata": {},
   "source": [
    "If you want to benchmark your code while testing with pytest, try pytest-benchmark. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "815482f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:42:17.296160Z",
     "start_time": "2021-09-11T18:42:17.288429Z"
    }
   },
   "source": [
    "To use pytest-benchmark works, add `benchmark` to the test function that you want to benchmark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407e3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_benchmark_example.py\n",
    "def list_comprehension(len_list=5):\n",
    "    return [i for i in range(len_list)]\n",
    "\n",
    "\n",
    "def test_concat(benchmark):\n",
    "    res = benchmark(list_comprehension)\n",
    "    assert res == [0, 1, 2, 3, 4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e4fa910",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_benchmark_example.py\n",
    "```\n",
    "Now you should see the statistics of the time it takes to execute the test functions on your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8ae8527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T16:36:26.021951Z",
     "start_time": "2021-09-12T16:36:22.946901Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_benchmark_example.py \u001b[32m.\u001b[0m\u001b[32m                                            [100%]\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33m----------------------------------------------------- benchmark: 1 tests ----------------------------------------------------\u001b[0m\n",
      "Name (time in ns)          Min         Max      Mean    StdDev    Median     IQR   Outliers  OPS (Mops/s)  Rounds  Iterations\n",
      "\u001b[33m-----------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_concat         \u001b[1m  286.4501\u001b[0m\u001b[1m  4,745.5498\u001b[0m\u001b[1m  309.3872\u001b[0m\u001b[1m  106.6583\u001b[0m\u001b[1m  297.5001\u001b[0m\u001b[1m  5.3500\u001b[0m  2686;5843\u001b[1m        3.2322\u001b[0m  162101          20\n",
      "\u001b[33m-----------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 2.47s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"!pytest pytest_benchmark_example.py \";\n",
       "                var nbb_formatted_code = \"!pytest pytest_benchmark_example.py\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pytest pytest_benchmark_example.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14d71ab4",
   "metadata": {},
   "source": [
    "[Link to pytest-benchmark](https://github.com/ionelmc/pytest-benchmark)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec5bd092",
   "metadata": {},
   "source": [
    "### pytest.mark.parametrize: Test Your Functions with Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2d9ec",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21e88ff1",
   "metadata": {},
   "source": [
    "If you want to test your function with different examples, use `pytest.mark.parametrize` decorator.\n",
    "\n",
    "To use `pytest.mark.parametrize`, add `@pytest.mark.parametrize` to the test function that you want to experiment with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ad6ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:43:09.283883Z",
     "start_time": "2021-09-11T18:43:09.273973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest_parametrize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest_parametrize.py\n",
    "import pytest\n",
    "\n",
    "def text_contain_word(word: str, text: str):\n",
    "    '''Find whether the text contains a particular word'''\n",
    "    \n",
    "    return word in text\n",
    "\n",
    "test = [\n",
    "    ('There is a duck in this text',True),\n",
    "    ('There is nothing here', False)\n",
    "    ]\n",
    "\n",
    "@pytest.mark.parametrize('sample, expected', test)\n",
    "def test_text_contain_word(sample, expected):\n",
    "\n",
    "    word = 'duck'\n",
    "\n",
    "    assert text_contain_word(word, sample) == expected"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4263702d",
   "metadata": {},
   "source": [
    "In the code above, I expect the first sentence to contain the word “duck” and expect the second sentence not to contain that word. Let's see if my expectations are correct by running:\n",
    "```bash\n",
    "$ pytest pytest_parametrize.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "031ac02c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:43:10.924823Z",
     "start_time": "2021-09-11T18:43:10.494719Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.6, pytest-7.2.1, pluggy-1.0.0 -- /Users/khuyen/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/khuyen/book/book/Chapter5\n",
      "plugins: anyio-3.6.2\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "pytest_parametrize.py::test_text_contain_word[There is a duck in this text-True] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "pytest_parametrize.py::test_text_contain_word[There is nothing here-False] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_parametrize.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "488b4f51",
   "metadata": {},
   "source": [
    "Sweet! 2 tests passed when running pytest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4a725d73",
   "metadata": {},
   "source": [
    "[Link to my article about pytest](https://towardsdatascience.com/pytest-for-data-scientists-2990319e55e6?sk=2d3a81903b154db0c7ca832b9f29fee8).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77b236f2",
   "metadata": {},
   "source": [
    "### pytest parametrize twice: Test All Possible Combinations of Two Sets of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef317c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0ccdbf9",
   "metadata": {},
   "source": [
    "If you want to test the combinations of two sets of parameters, writing all possible combinations can be time-consuming and is difficult to read. \n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "def perc_difference(n1, n2):\n",
    "    return (n2 - n1)/n1 * 100\n",
    "\n",
    "# Test the combinations of operations and inputs\n",
    "@pytest.mark.parametrize(\"operation, n1, n2\", [(average, 1, 2), (average, 2, 3), (perc_difference, 1, 2), (perc_difference, 2, 3)])\n",
    "def test_is_float(operation, n1, n2):\n",
    "    assert isinstance(operation(n1, n2), float)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c28b30c",
   "metadata": {},
   "source": [
    "You can save your time by using `pytest.mark.parametrize` twice instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6fca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_combination.py\n",
    "import pytest\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "def perc_difference(n1, n2):\n",
    "    return (n2 - n1)/n1 * 100\n",
    "\n",
    "# Test the combinations of operations and inputs\n",
    "@pytest.mark.parametrize(\"operation\", [average, perc_difference])\n",
    "@pytest.mark.parametrize(\"n1, n2\", [(1, 2), (2, 3)])\n",
    "def test_is_float(operation, n1, n2):\n",
    "    assert isinstance(operation(n1, n2), float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c881522",
   "metadata": {},
   "source": [
    "On your terminal, run:\n",
    "```bash\n",
    "$ pytest -v pytest_combination.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c94aef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T15:06:56.991305Z",
     "start_time": "2022-02-23T15:06:56.158431Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1 -- /home/khuyen/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/khuyen/book/book/Chapter5/.hypothesis/examples')\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, typeguard-2.13.3\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "pytest_combination.py::test_is_float[1-2-average] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 25%]\u001b[0m\n",
      "pytest_combination.py::test_is_float[1-2-perc_difference] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 50%]\u001b[0m\n",
      "pytest_combination.py::test_is_float[2-3-average] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 75%]\u001b[0m\n",
      "pytest_combination.py::test_is_float[2-3-perc_difference] \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_combination.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2041b7e5",
   "metadata": {},
   "source": [
    "From the output above, we can see that all possible combinations of the given operations and inputs are tested."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0378ff50",
   "metadata": {},
   "source": [
    "### Assign IDs to Test Cases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "059a737d",
   "metadata": {},
   "source": [
    "When using pytest parametrize, it can be difficult to understand the role of each test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdb0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_without_ids.py\n",
    "from pytest import mark\n",
    "\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "@mark.parametrize(\n",
    "    \"n1, n2\",\n",
    "    [(-1, -2), (2, 3), (0, 0)],\n",
    ")\n",
    "def test_is_float(n1, n2):\n",
    "    assert isinstance(average(n1, n2), float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ba8fa45",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest -v pytest_without_ids.py \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1502853a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T13:34:28.749494Z",
     "start_time": "2022-03-16T13:34:27.844208Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1 -- /home/khuyen/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/khuyen/book/book/Chapter5/.hypothesis/examples')\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, cases-3.6.10, typeguard-2.13.3\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "pytest_without_ids.py::test_is_float[-1--2] \u001b[32mPASSED\u001b[0m\u001b[32m                       [ 33%]\u001b[0m\n",
      "pytest_without_ids.py::test_is_float[2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 66%]\u001b[0m\n",
      "pytest_without_ids.py::test_is_float[0-0] \u001b[32mPASSED\u001b[0m\u001b[32m                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.26s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_without_ids.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "454d748a",
   "metadata": {},
   "source": [
    "You can add `ids` to pytest parametrize to assign a name to each test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e79dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_ids.py\n",
    "from pytest import mark\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "@mark.parametrize(\n",
    "    \"n1, n2\",\n",
    "    [(-1, -2), (2, 3), (0, 0)],\n",
    "    ids=[\"neg and neg\", \"pos and pos\", \"zero and zero\"],\n",
    ")\n",
    "def test_is_float(n1, n2):\n",
    "    assert isinstance(average(n1, n2), float)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7872c3a6",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest -v pytest_ids.py \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2c76e2",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1 -- /home/khuyen/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/khuyen/book/book/Chapter5/.hypothesis/examples')\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, cases-3.6.10, typeguard-2.13.3\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "pytest_ids.py::test_is_float[neg and neg] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 33%]\u001b[0m\n",
      "pytest_ids.py::test_is_float[pos and pos] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 66%]\u001b[0m\n",
      "pytest_ids.py::test_is_float[zero and zero] \u001b[32mPASSED\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_ids.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7392f742",
   "metadata": {},
   "source": [
    "We can see that instead of `[-1--2]`, the first test case is shown as `neg and neg`. This makes it easier for others to understand the roles of your test cases.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72543d9f",
   "metadata": {},
   "source": [
    "If you want to specify the test IDs together with the actual data, instead of listing them separately, use `pytest.param`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3807afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_param.py\n",
    "import pytest\n",
    "\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "\n",
    "examples = [\n",
    "    pytest.param(-1, -2, id=\"neg-neg\"),\n",
    "    pytest.param(2, 3, id=\"pos-pos\"),\n",
    "    pytest.param(0, 0, id=\"0-0\"),\n",
    "]\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"n1, n2\", examples)\n",
    "def test_is_float(n1, n2):\n",
    "    assert isinstance(average(n1, n2), float)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99380417",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest -v pytest_param.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcdab4a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-16T13:54:09.767614Z",
     "start_time": "2022-09-16T13:54:09.119019Z"
    },
    "tags": [
     "hide-inpy"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.9, pytest-7.1.2, pluggy-1.0.0 -- /Users/khuyen/book/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /Users/khuyen/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\r\n",
      "plugins: anyio-3.6.1, pyfakefs-4.6.3, picked-0.4.6\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r\n",
      "collected 3 items                                                              \u001b[0m\r\n",
      "\r\n",
      "pytest_param.py::test_is_float[neg-neg] \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 33%]\u001b[0m\r\n",
      "pytest_param.py::test_is_float[pos-pos] \u001b[32mPASSED\u001b[0m\u001b[32m                           [ 66%]\u001b[0m\r\n",
      "pytest_param.py::test_is_float[0-0] \u001b[32mPASSED\u001b[0m\u001b[32m                               [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_param.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cac57677",
   "metadata": {},
   "source": [
    "### Pytest Fixtures: Use The Same Data for Different Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c745a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest textblob "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5256a1b",
   "metadata": {},
   "source": [
    "If you want to use the same data to test different functions, use pytest fixtures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ce9cb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:45:26.995394Z",
     "start_time": "2021-09-11T18:45:26.472717Z"
    }
   },
   "source": [
    "To use pytest fixtures,  add the decorator `@pytest.fixture` to the function that creates the data you want to reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8472909",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_fixture.py\n",
    "import pytest \n",
    "from textblob import TextBlob\n",
    "\n",
    "def extract_sentiment(text: str):\n",
    "    \"\"\"Extract sentimetn using textblob. Polarity is within range [-1, 1]\"\"\"\n",
    "    \n",
    "    text = TextBlob(text)\n",
    "    return text.sentiment.polarity\n",
    "\n",
    "@pytest.fixture \n",
    "def example_data():\n",
    "    return 'Today I found a duck and I am happy'\n",
    "\n",
    "def test_extract_sentiment(example_data):\n",
    "    sentiment = extract_sentiment(example_data)\n",
    "    assert sentiment > 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29564978",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_fixture.py\n",
    "```\n",
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79658c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:45:46.386950Z",
     "start_time": "2021-09-11T18:45:45.299302Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: benchmark-3.4.1, anyio-3.3.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_fixture.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.53s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_fixture.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "636cfcc6",
   "metadata": {},
   "source": [
    "### Execute a Fixture Only Once per Session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83bcc41a",
   "metadata": {},
   "source": [
    "By default, every time you use a pytest fixture in a test, a fixture will be executed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5ebe803",
   "metadata": {},
   "source": [
    "```python\n",
    "# example.py\n",
    "import pytest \n",
    "\n",
    "@pytest.fixture\n",
    "def my_data():\n",
    "    print(\"Reading data...\")\n",
    "    return 1\n",
    "\n",
    "def test_division(my_data):\n",
    "    print(\"Test division...\")\n",
    "    assert my_data / 2 == 0.5\n",
    "\n",
    "def test_modulus(my_data):\n",
    "    print(\"Test modulus...\")\n",
    "    assert my_data % 2 == 1\n",
    "```\n",
    "From the output, we can see that the fixture `my_data` is executed twice.\n",
    "\n",
    "```bash\n",
    "$ pytest example.py -s\n",
    "Reading data...\n",
    "Test division...\n",
    "Reading data...\n",
    "Test modulus...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f7993ce",
   "metadata": {},
   "source": [
    "If a fixture is expensive to execute, you can make the fixture be executed only once per session using `scope=session`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53898a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_scope.py\n",
    "import pytest \n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def my_data():\n",
    "    print(\"Reading data...\")\n",
    "    return 1\n",
    "\n",
    "def test_division(my_data):\n",
    "    print(\"Test division...\")\n",
    "    assert my_data / 2 == 0.5\n",
    "\n",
    "def test_modulus(my_data):\n",
    "    print(\"Test modulus...\")\n",
    "    assert my_data % 2 == 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2dbf247e",
   "metadata": {},
   "source": [
    "From the output, we can see that the fixture `my_data` is executed only once.\n",
    "```bash\n",
    "$ pytest pytest_scope.py -s\n",
    "Reading data...\n",
    "Test division...\n",
    "Test modulus...\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ff3fcd1",
   "metadata": {},
   "source": [
    "### Pytest skipif: Skip a Test When a Condition is Not Met"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1338ccba",
   "metadata": {},
   "source": [
    "If you want to skip a test when a condition is not met, use pytest `skipif`. For example, in the code below, I use `skipif` to skip a test if the python version is less than 3.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd648a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pytest_skip.py\n",
    "import sys\n",
    "import pytest \n",
    "\n",
    "def add_two(num: int):\n",
    "    return num + 2 \n",
    "\n",
    "@pytest.mark.skipif(sys.version_info < (3, 9), reason=\"Eequires Python 3.9 or higher\")\n",
    "def test_add_two(): \n",
    "    assert add_two(3) == 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ecb40560",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_skip.py -v \n",
    "```\n",
    "\n",
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3df6c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T14:38:54.377174Z",
     "start_time": "2022-05-05T14:38:53.709196Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.10, pytest-7.1.2, pluggy-1.0.0 -- /Users/khuyen/book/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /Users/khuyen/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r\n",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "pytest_skip.py::test_add_two \u001b[33mSKIPPED\u001b[0m (Eequires Python 3.9 or higher)\u001b[33m     [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[33m============================== \u001b[33m\u001b[1m1 skipped\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ==============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_skip.py -v "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8576a9a4",
   "metadata": {},
   "source": [
    "### Pytest xfail: Mark a Test as Expected to Fail"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6d1f6b8",
   "metadata": {},
   "source": [
    "If you expect a test to fail, use pytest `xfail` marker. This will prevent pytest from marking a test as failed when there is an exception.\n",
    "\n",
    "To be more specific about what exception you expect to see, use the `raises` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52c638a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>col2</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      col1\n",
       "col2      \n",
       "a      1.5\n",
       "b      3.5\n",
       "c      3.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"col1\": [1, 2, 3, 4, 3],\n",
    "        \"col2\": [\"a\", \"a\", \"b\", \"b\", \"c\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "df.groupby([\"col2\"]).agg({\"col1\": \"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e22d4235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pytest_mark_xfail.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest_mark_xfail.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mean(df, group_column, value_column):\n",
    "    if df[group_column].isna().any():\n",
    "        raise ValueError(\"Group column contains NaN values\")\n",
    "    return df.groupby(group_column)[value_column].mean()\n",
    "\n",
    "\n",
    "@pytest.mark.xfail(raises=ValueError)\n",
    "def test_cget_mean():\n",
    "    df = pd.DataFrame({\"group\": [\"a\", np.nan, \"b\", \"b\"], \"value\": [1, 2, 3, 0]})\n",
    "    get_mean(df, \"group\", \"value\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "560df152",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "\n",
    "```bash\n",
    "$ pytest pytest_mark_xfail.py\n",
    "```\n",
    "\n",
    "We can see that no test failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99bdf5d8",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, hypothesis-6.88.4\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_mark_xfail.py \u001b[33mx\u001b[0m\u001b[33m                                                   [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== \u001b[33m\u001b[1m1 xfailed\u001b[0m\u001b[33m in 0.31s\u001b[0m\u001b[33m ==============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest pytest_mark_xfail.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b941c6a-029c-4866-9dcf-83b62d01a74a",
   "metadata": {},
   "source": [
    "### Stop Flaky Float Tests with pytest.approx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa6c4e-1c54-4b38-ae44-6f8a022823e9",
   "metadata": {},
   "source": [
    "When working with floating-point numbers in tests, comparing exact equality often fails due to inherent precision limitations, which results in flaky or unreliable tests.\n",
    "\n",
    "You can use `pytest.approx()` to compare floating-point numbers with a reasonable tolerance, making your tests more robust and readable.\n",
    "\n",
    "Here's a code example to show its usefulness:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78121c95-ae35-49c2-9ec8-86afc6138a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_approx.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_approx.py\n",
    "import pytest\n",
    "\n",
    "\n",
    "# Without pytest.approx - test fails\n",
    "def test_calculation_without_approx():\n",
    "    result = 0.1 + 0.2\n",
    "    assert result == 0.3  # This fails!\n",
    "\n",
    "\n",
    "# With pytest.approx - test passes\n",
    "def test_calculation_with_approx():\n",
    "    result = 0.1 + 0.2\n",
    "    assert result == pytest.approx(0.3)  # This passes!\n",
    "\n",
    "\n",
    "# Works with sequences too\n",
    "def test_list_of_floats():\n",
    "    results = [0.1 + 0.2, 0.2 + 0.4]\n",
    "    expected = [0.3, 0.6]\n",
    "    assert results == pytest.approx(expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5330236f-a49e-4622-a13f-d87874b3083d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.6, pytest-8.2.0, pluggy-1.5.0\n",
      "Fugue tests will be initialized with options:\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: anyio-4.0.0, dash-2.17.1, fugue-0.9.1, Faker-19.13.0, returns-0.22.0\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "test_approx.py \u001b[31mF\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                                       [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m_______________________ test_calculation_without_approx ________________________\u001b[0m\n",
      "\n",
      "    \u001b[0m\u001b[94mdef\u001b[39;49;00m \u001b[92mtest_calculation_without_approx\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
      "        result = \u001b[94m0.1\u001b[39;49;00m + \u001b[94m0.2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      ">       \u001b[94massert\u001b[39;49;00m result == \u001b[94m0.3\u001b[39;49;00m  \u001b[90m# This fails!\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 0.30000000000000004 == 0.3\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_approx.py\u001b[0m:7: AssertionError\n",
      "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
      "\u001b[31mFAILED\u001b[0m test_approx.py::\u001b[1mtest_calculation_without_approx\u001b[0m - assert 0.30000000000000004 == 0.3\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_approx.py -sv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dbf81a-37d3-4190-920c-8c4d04428d25",
   "metadata": {},
   "source": [
    "In these examples:\n",
    "- The first test fails because 0.1 + 0.2 in floating-point arithmetic doesn't exactly equal 0.3.\n",
    "- The second test passes because `pytest.approx()` allows for a small tolerance in the comparison (by default, relative tolerance of 1e-6).\n",
    "- The third example shows how it works with lists of numbers, making it convenient for comparing multiple results at once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ff6d9",
   "metadata": {},
   "source": [
    "### Test for Specific Exceptions in Unit Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c087d03",
   "metadata": {},
   "source": [
    "To test for a specific exception in unit testing, use `pytest.raises`.\n",
    "\n",
    "For example, you can use it to test if a ValueError is thrown when there are NaN values in the group column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24775262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pytest_to_fail.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest_to_fail.py\n",
    "import pytest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_mean(df, group_column, value_column):\n",
    "    if df[group_column].isna().any():\n",
    "        raise ValueError(\"Group column contains NaN values\")\n",
    "    return df.groupby(group_column)[value_column].mean()\n",
    "\n",
    "\n",
    "def test_get_mean():\n",
    "    with pytest.raises(ValueError):\n",
    "        df = pd.DataFrame({\"group\": [\"a\", np.nan, \"b\", \"b\"], \"value\": [1, 2, 3, 0]})\n",
    "        get_mean(df, \"group\", \"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de71c1",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest pytest_to_fail.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2898168f",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, hypothesis-6.88.4\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_to_fail.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.28s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest pytest_to_fail.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3657f5a4",
   "metadata": {},
   "source": [
    "### Organize and Control Test Execution using pytest.mark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffdbfef",
   "metadata": {},
   "source": [
    "pytest.mark lets you label test functions for conditional or selective execution based on specific needs. \n",
    "\n",
    "For instance, you can mark slow tests or tests involving integration with external services to run them separately or exclude them from regular test runs. This helps you organize and execute your tests more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f48a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pytest_mark.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pytest_mark.py\n",
    "import pytest\n",
    "import time\n",
    "\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_long_running_function():\n",
    "    # Test that takes a long time to complete\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "@pytest.mark.db\n",
    "def test_database_interaction():\n",
    "    # Test that requires a database connection\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_function_1():\n",
    "    pass\n",
    "\n",
    "\n",
    "def test_function_2():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155aa47c",
   "metadata": {},
   "source": [
    "Run only slow tests:\n",
    "\n",
    "```bash\n",
    "$ pytest pytest_mark.py -m slow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "244b7a98",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 4 items / 3 deselected / 1 selected                                  \u001b[0m\n",
      "\n",
      "pytest_mark.py \u001b[32m.\u001b[0m\u001b[33m                                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m================= \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m3 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 5.02s\u001b[0m\u001b[33m ==================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest pytest_mark.py -m slow --disable-pytest-warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71a7e21",
   "metadata": {},
   "source": [
    "Skip slow tests:\n",
    "\n",
    "```bash\n",
    "$ pytest pytest_mark.py -m \"not slow\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5d03906",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 4 items / 1 deselected / 3 selected                                  \u001b[0m\n",
      "\n",
      "pytest_mark.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m================= \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m1 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ==================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest pytest_mark.py -m \"not slow\" --disable-pytest-warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d60730",
   "metadata": {},
   "source": [
    "Skip database tests:\n",
    "\n",
    "```bash\n",
    "$ pytest pytest_mark.py -m \"not db\" \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea78f1b",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 4 items / 1 deselected / 3 selected                                  \u001b[0m\n",
      "\n",
      "pytest_mark.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m================= \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m1 deselected\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 5.02s\u001b[0m\u001b[33m ==================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest pytest_mark.py -m \"not db\" --disable-pytest-warnings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75e01c50",
   "metadata": {},
   "source": [
    "### Verify Logging Error with pytest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3ca94129",
   "metadata": {},
   "source": [
    "To ensure that your application logs an error under a specific condition, use the built-in fixture called `caplog` in pytest. \n",
    "\n",
    "This fixture allows you to capture log messages generated during the execution of your test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e07feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_logging.py\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)\n",
    "\n",
    "def divide(num1: float, num2: float) -> float:\n",
    "    if num2 == 0:\n",
    "        logger.error(f\"Can't divide {num1} by 0\")\n",
    "    else:\n",
    "        logger.info(f\"Divide {num1} by {num2}\")\n",
    "        return num1 / num2\n",
    "\n",
    "def test_divide_by_0(caplog):\n",
    "    divide(1, 0)\n",
    "    assert \"Can't divide 1 by 0\" in caplog.text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a1d1429",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_logging.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fae6cc17",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.6, pytest-7.2.1, pluggy-1.0.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: anyio-3.6.2\n",
      "collected 1 item                                                               \u001b[0m\u001b[1m\n",
      "\n",
      "test_logging.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.66s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_logging.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3bb08f7",
   "metadata": {},
   "source": [
    "### Pytest repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8f66f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-repeat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa2ad279",
   "metadata": {},
   "source": [
    "It is a good practice to test your functions to make sure they work as expected, but sometimes you need to test 100 times until you found the rare cases when the test fails. That is when pytest-repeat comes in handy.\n",
    "\n",
    "To use pytest-repeat, add the decorator `@pytest.mark.repeat(N)` to the test function you want to repeat `N` times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a081ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writfile pytest_repeat_example.py\n",
    "import pytest \n",
    "import random \n",
    "\n",
    "def generate_numbers():\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "@pytest.mark.repeat(100)\n",
    "def test_generate_numbers():\n",
    "    assert generate_numbers() > 1 and generate_numbers() < 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "518854b9",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_repeat_example.py\n",
    "```\n",
    "We can see that 100 experiments are executed and passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7291ca8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:51:02.831644Z",
     "start_time": "2021-09-11T18:51:02.331407Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0\n",
      "collected 100 items                                                            \u001b[0m\n",
      "\n",
      "pytest_repeat_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 47%]\n",
      "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m100 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_repeat_example.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb66973",
   "metadata": {},
   "source": [
    "[Link to pytest-repeat](https://github.com/pytest-dev/pytest-repeat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df0eb505",
   "metadata": {},
   "source": [
    "### pytest-sugar: Show the Failures and Errors Instantly With a Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c47f15",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-sugar "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "933a0f98",
   "metadata": {},
   "source": [
    "It can be frustrating to wait for a lot of tests to run before knowing the status of the tests. If you want to see the failures and errors instantly with a progress bar, use pytest-sugar. \n",
    "\n",
    "pytest-sugar is a plugin for pytest. To see how pytest-sugar works, assume we have several test files in the `pytest_sugar_example` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5b6964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_benchmark_example.py  test_parametrize.py\n",
      "test_fixture.py            test_repeat_example.py\n"
     ]
    }
   ],
   "source": [
    "%ls pytest_sugar_example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d38ba696",
   "metadata": {},
   "source": [
    "The code below shows how the outputs will look like when running pytest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51987496",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest pytest_sugar_example\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8669e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T13:59:12.920920Z",
     "start_time": "2021-11-29T13:59:09.573556Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.8.10, pytest 6.2.5, pytest-sugar 0.9.4)\u001b[0m\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, sugar-0.9.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_benchmark_example.py\u001b[0m \u001b[32m✓\u001b[0m                  \u001b[32m1% \u001b[0m\u001b[40m\u001b[32m▏\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_fixture.py\u001b[0m \u001b[32m✓\u001b[0m                            \u001b[32m2% \u001b[0m\u001b[40m\u001b[32m▎\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_parametrize.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m                       \u001b[32m4% \u001b[0m\u001b[40m\u001b[32m▍\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_repeat_example.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m23% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍\u001b[0m\u001b[40m\u001b[32m       \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m42% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▎\u001b[0m\u001b[40m\u001b[32m     \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m62% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▎\u001b[0m\u001b[40m\u001b[32m   \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m81% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▏ \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
      "\n",
      "\u001b[33m---------------------------------------------------- benchmark: 1 tests ---------------------------------------------------\u001b[0m\n",
      "Name (time in ns)          Min         Max      Mean   StdDev    Median     IQR  Outliers  OPS (Mops/s)  Rounds  Iterations\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_concat         \u001b[1m  302.8003\u001b[0m\u001b[1m  3,012.5000\u001b[0m\u001b[1m  328.2844\u001b[0m\u001b[1m  97.9087\u001b[0m\u001b[1m  321.5999\u001b[0m\u001b[1m  8.2495\u001b[0m  866;2220\u001b[1m        3.0461\u001b[0m   90868          20\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\n",
      "Results (2.63s):\n",
      "\u001b[32m     104 passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_sugar_example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "768bb279",
   "metadata": {},
   "source": [
    "[Link to pytest-sugar](https://github.com/Teemu/pytest-sugar)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6382f46",
   "metadata": {},
   "source": [
    "### pytest-steps: Share Data Between Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ca87dbb",
   "metadata": {},
   "source": [
    "Have you ever wanted to use the result of one test for another test? That is when pytest_steps comes in handy. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4810c8b5",
   "metadata": {},
   "source": [
    "![](../img/pytest_steps.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45a25187",
   "metadata": {},
   "source": [
    "In the code below, I use the result of `sum_test` as the input of `average_2_nums`. The argument `steps_data` allows me to share the data between 2 tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eafdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_steps.py\n",
    "from pytest_steps import test_steps\n",
    "\n",
    "\n",
    "def sum(n1, n2):\n",
    "    return n1 + n2\n",
    "\n",
    "\n",
    "def average_2_nums(sum):\n",
    "    return sum / 2\n",
    "\n",
    "\n",
    "def sum_test(steps_data):\n",
    "    res = sum(1, 3)\n",
    "    assert res == 4\n",
    "    steps_data.res = res\n",
    "\n",
    "\n",
    "def perc_difference_test(steps_data):\n",
    "    avg = average_2_nums(steps_data.res)\n",
    "    assert avg == 2\n",
    "\n",
    "\n",
    "@test_steps(sum_test, perc_difference_test)\n",
    "def test_calc_suite(test_step, steps_data):\n",
    "    if test_step == 'sum_test':\n",
    "        sum_test(steps_data)\n",
    "    elif test_step == 'perc_difference_test':\n",
    "        perc_difference_test(steps_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8218cfd3",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_steps.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b123df70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T14:02:08.595254Z",
     "start_time": "2022-04-25T14:02:07.505819Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.10, pytest-7.1.2, pluggy-0.13.1\r\n",
      "rootdir: /Users/khuyen/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\r\n",
      "plugins: anyio-3.5.0, steps-1.8.0, typeguard-2.12.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r\n",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "test_steps.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest test_steps.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbc7aa74",
   "metadata": {},
   "source": [
    "[Link to pytest_steps](https://smarie.github.io/python-pytest-steps/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c69f8f70",
   "metadata": {},
   "source": [
    "### pytest-picked: Run the Tests Related to the Unstaged Files in Git"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a896d47c",
   "metadata": {},
   "source": [
    "It can be time-consuming to run all tests in your project. Wouldn't it be nice if you can run only the tests related to the unstaged files in Git? That is when pytest-picked comes in handy.\n",
    "\n",
    "In the code below, only tests in the file `test_picked.py` are executed because it is an unstaged file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_picked.py\n",
    "def plus_one(num: int):\n",
    "    return num + 1\n",
    "\n",
    "\n",
    "def test_plus_one():\n",
    "    assert plus_one(2) == 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9ffcd195",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ git status\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31f73a62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T14:15:23.578936Z",
     "start_time": "2022-09-07T14:15:23.210279Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\r\n",
      "Your branch is up to date with 'origin/master'.\r\n",
      "\r\n",
      "Untracked files:\r\n",
      "  (use \"git add <file>...\" to include in what will be committed)\r\n",
      "\t\u001b[31mtest_picked.py\u001b[m\r\n",
      "\r\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\r\n"
     ]
    }
   ],
   "source": [
    "!git status"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fd32a32",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest --picked\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d510bf3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-07T14:15:35.485328Z",
     "start_time": "2022-09-07T14:15:34.923559Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Changed test files... 1. ['test_picked.py']\r\n",
      "Changed test folders... 0. []\r\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.9, pytest-7.1.2, pluggy-1.0.0\r\n",
      "rootdir: /Users/khuyen/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\r\n",
      "plugins: anyio-3.6.1, picked-0.4.6\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r\n",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "test_picked.py \u001b[32m.\u001b[0m\u001b[32m                                                         [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest --picked"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f4c86da",
   "metadata": {},
   "source": [
    "[Link to pytest-picked](https://github.com/anapaulagomes/pytest-picked)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9454dd3b",
   "metadata": {},
   "source": [
    "### Efficient Testing of Python Class with setUp Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd2ee585",
   "metadata": {},
   "source": [
    "When testing a Python class, it can be repetitive and time-consuming to create multiple instances to test a large number of instance methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dd541a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing get_dog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_dog.py \n",
    "class Dog:\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "    def walk(self):\n",
    "        return f\"{self.name} is walking\"\n",
    "\n",
    "    def bark(self):\n",
    "        return f\"{self.name} is barking\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a81d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_get_dog.py\n",
    "import unittest\n",
    "from get_dog import Dog\n",
    "\n",
    "class TestDog(unittest.TestCase):\n",
    "    def test_walk(self):\n",
    "        dog = Dog(\"Max\", 3) \n",
    "        dog.walk() == \"Max is walking\"\n",
    "\n",
    "    def test_bark(self):\n",
    "        dog = Dog(\"Max\", 3) \n",
    "        dog.bark() == \"Max is barking\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1c7f560",
   "metadata": {},
   "source": [
    "A better approach is to use the `setUp` method to instantiate a class object before running each test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "283cfcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_get_dog.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_get_dog.py\n",
    "import unittest\n",
    "from get_dog import Dog\n",
    "\n",
    "class TestDog(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.dog = Dog(\"Max\", 3)\n",
    "\n",
    "    def test_walk(self):\n",
    "        self.dog.walk() == \"Max is walking\"\n",
    "\n",
    "    def test_bark(self):\n",
    "        self.dog.bark() == \"Max is barking\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "915e9960",
   "metadata": {},
   "source": [
    "### FreezeGun: Freezing Time for Reliable Python Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069bd32",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install freezegun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979b680-a898-402b-b92d-412cfbb3572c",
   "metadata": {},
   "source": [
    "Testing time-dependent functions can be challenging and unreliable as the results may vary based on when the test is executed. This results in flaky tests that pass or fail inconsistently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85ef39bd",
   "metadata": {},
   "source": [
    "With FreezeGun, you can freeze time at a particular point, ensuring your tests always run with the same date context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62213cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_freezegun.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_freezegun.py\n",
    "from freezegun import freeze_time\n",
    "import datetime \n",
    "\n",
    "def get_day_of_week():\n",
    "    return datetime.datetime.now().weekday()\n",
    "\n",
    "@freeze_time(\"2024-10-13\")\n",
    "def test_get_day_of_week():\n",
    "    assert get_day_of_week() == 6\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67d161da",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_freezegun.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "376870d5",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.6, pytest-8.2.0, pluggy-1.5.0\n",
      "Fugue tests will be initialized with options:\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: anyio-4.0.0, dash-2.17.1, fugue-0.9.1, Faker-19.13.0, returns-0.22.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_freezegun.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_freezegun.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9274e666-b811-4997-9764-117ec5257f90",
   "metadata": {},
   "source": [
    "This code uses `get_day_of_week()` to return the current weekday (0-6). The `@freeze_time(\"2024-10-13\")` decorator sets a fixed date (Sunday, October 13, 2024).\n",
    "\n",
    "The test calls `get_day_of_week()` and checks if it returns 6 (Sunday). This test will consistently pass because FreezeGun ensures `datetime.datetime.now()` always returns the specified frozen date."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c5f4d11",
   "metadata": {},
   "source": [
    "[Link to FreezeGun](https://github.com/spulec/freezegun)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b810c274",
   "metadata": {},
   "source": [
    "### Simulate External Services in Testing with Mock Objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be2fecae",
   "metadata": {},
   "source": [
    "Testing code that relies on external services, like a database, can be difficult since the behaviors of these services can change. \n",
    "\n",
    "A mock object can control the behavior of a real object in a testing environment by simulating responses from external services.\n",
    "\n",
    "Here are two common use cases with examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa134f51",
   "metadata": {},
   "source": [
    "1. **Mocking Time-Dependent Functions**\n",
    "\n",
    "When testing functions that depend on the current time or date, you can mock the time to ensure consistent results.\n",
    "\n",
    "Example: Testing a function that returns data for the last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25d20b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "def get_data_for_last_week():\n",
    "    end_date = datetime.now().date()\n",
    "    start_date = end_date - timedelta(days=7)\n",
    "    return {\n",
    "        \"start_date\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"end_date\": end_date.strftime(\"%Y-%m-%d\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66a0f82",
   "metadata": {},
   "source": [
    "Now, let's create a test for this function using mock:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7754b84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_main.py\n",
    "from datetime import datetime\n",
    "from unittest.mock import patch\n",
    "from main import get_data_for_last_week\n",
    "\n",
    "\n",
    "@patch(\"main.datetime\")\n",
    "def test_get_data_for_last_week(mock_datetime):\n",
    "    # Set a fixed date for the test\n",
    "    mock_datetime.now.return_value = datetime(2024, 8, 5)\n",
    "\n",
    "    # Call the function\n",
    "    result = get_data_for_last_week()\n",
    "\n",
    "    # Assert the results\n",
    "    assert result[\"start_date\"] == \"2024-07-29\"\n",
    "    assert result[\"end_date\"] == \"2024-08-05\"\n",
    "\n",
    "    # Verify that datetime.now() was called\n",
    "    mock_datetime.now.assert_called_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124b826",
   "metadata": {},
   "source": [
    "This test mocks the `datetime.now()` method to return a fixed date, allowing for predictable and consistent test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a6c35293",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0 -- /Users/khuyentran/.pyenv/versions/3.11.2/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase(PosixPath('/Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5/.hypothesis/examples'))\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_main.py::test_get_data_for_last_week \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.09s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest -sv test_main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d68ef-2f73-43ab-98cf-8268bab793a0",
   "metadata": {},
   "source": [
    "2. **Mocking API calls**\n",
    "\n",
    "When testing code that makes external API calls, mocking helps avoid actual network requests during testing.\n",
    "\n",
    "Example: Testing a function that makes an API call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0a3fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    \"\"\"Make an API call to Postgres\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:5432\")\n",
    "        return response.json()\n",
    "    except ConnectionError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f5b1755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_main.py\n",
    "from unittest.mock import patch\n",
    "from requests.exceptions import ConnectionError\n",
    "from main import get_data\n",
    "\n",
    "\n",
    "@patch(\"main.requests.get\")\n",
    "def test_get_data_fails(mock_get):\n",
    "    \"\"\"Test the get_data function when the API call fails\"\"\"\n",
    "    # Define what happens when the function is called\n",
    "    mock_get.side_effect = ConnectionError\n",
    "    assert get_data() is None\n",
    "\n",
    "\n",
    "@patch(\"main.requests.get\")\n",
    "def test_get_data_succeeds(mock_get):\n",
    "    \"\"\"Test the get_data function when the API call succeeds\"\"\"\n",
    "    # Define the return value of the function\n",
    "    mock_get.return_value.json.return_value = {\"data\": \"test\"}\n",
    "    assert get_data() == {\"data\": \"test\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0701b9",
   "metadata": {},
   "source": [
    "These tests mock the `requests.get()` function to simulate both successful and failed API calls, allowing us to test our function's behavior in different scenarios without making actual network requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3af021c9",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_main.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                          [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.12s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest test_main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ffc227-fd00-40ef-aaa2-74e8997aa2cd",
   "metadata": {},
   "source": [
    "By using mocks in these ways, we can create more reliable and controlled unit tests for our data projects, ensuring that our code behaves correctly under various conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e516385",
   "metadata": {},
   "source": [
    "### pytest-mock vs unittest.mock: Simplifying Mocking in Python Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6041e6a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-mock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f5aa4",
   "metadata": {},
   "source": [
    "Traditional mocking with unittest.mock often requires repetitive setup and teardown code, which can make test code harder to read and maintain. \n",
    "\n",
    "pytest-mock addresses this issue by leveraging pytest's fixture system, simplifying the mocking process and reducing boilerplate code.\n",
    "\n",
    "Consider the following example that demonstrates the difference between unittest.mock and pytest-mock.\n",
    "\n",
    "Using unittest.mock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed814822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_rm_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_rm_file.py\n",
    "from unittest.mock import patch\n",
    "import os\n",
    "\n",
    "\n",
    "def rm_file(filename):\n",
    "    os.remove(filename)\n",
    "\n",
    "\n",
    "def test_with_unittest_mock():\n",
    "    with patch(\"os.remove\") as mock_remove:\n",
    "        rm_file(\"file\")\n",
    "        mock_remove.assert_called_once_with(\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9d8b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, mock-3.14.0, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_rm_file.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest test_rm_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0607964b",
   "metadata": {},
   "source": [
    "Using pytest-mock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e772780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_rm_file.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_rm_file.py\n",
    "import os\n",
    "\n",
    "\n",
    "def rm_file(filename):\n",
    "    os.remove(filename)\n",
    "\n",
    "\n",
    "def test_unix_fs(mocker):\n",
    "    mocker.patch(\"os.remove\")\n",
    "    rm_file(\"file\")\n",
    "    os.remove.assert_called_once_with(\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74a6fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, mock-3.14.0, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_rm_file.py \u001b[32m.\u001b[0m\u001b[32m                                                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest test_rm_file.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f873409",
   "metadata": {},
   "source": [
    "Key differences:\n",
    "\n",
    "1. Setup: pytest-mock uses the `mocker` fixture, automatically provided by pytest, eliminating the need to import patching utilities.\n",
    "\n",
    "2. Patching: With pytest-mock, you simply call `mocker.patch('os.remove')`, whereas unittest.mock requires a context manager or decorator.\n",
    "\n",
    "3. Cleanup: pytest-mock automatically undoes mocking after the test, while unittest.mock relies on the context manager for cleanup.\n",
    "\n",
    "4. Accessing mocks: pytest-mock allows direct access to the patched function (e.g., `os.remove.assert_called_once_with()`), while unittest.mock requires accessing the mock through a variable (e.g., `mock_remove.assert_called_once_with()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca3e0b7",
   "metadata": {},
   "source": [
    "[Link to pytest-mock](https://bit.ly/4dBDAOE)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8861f52b",
   "metadata": {},
   "source": [
    "### tmp_path: Create a Temporary Directory for Testing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a05e6821",
   "metadata": {},
   "source": [
    "Use the `tmp_path` fixture in pytest to create a temporary directory for testing the function that interacts with files. This will prevent any changes to the actual filesystem or production files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc7360ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_tmp_path.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_tmp_path.py\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def save_result(folder: str, file_name: str, text: str):\n",
    "    # Create new file inside the folder\n",
    "    file = Path(folder) / file_name\n",
    "    file.touch()\n",
    "\n",
    "    # Write result to the new file\n",
    "    file.write_text(text)\n",
    "\n",
    "def test_save_result(tmp_path):\n",
    "    # Create a temporary folder\n",
    "    folder = tmp_path / \"new\"\n",
    "    folder.mkdir() \n",
    "\n",
    "    file_name = \"my_file.txt\"\n",
    "    text = \"Accuracy: 0.9\"\n",
    "\n",
    "    save_result(folder=folder, file_name=file_name, text=text)\n",
    "    res = Path(f\"{folder}/{file_name}\").read_text()\n",
    "    assert res == text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc8e7e4a",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_tmp_path.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e36109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-14T14:33:36.383371Z",
     "start_time": "2022-09-14T14:33:34.825466Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.11.2, pytest-7.4.3, pluggy-1.3.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dvc-3.28.0, hydra-core-1.3.2, typeguard-4.1.5, anyio-4.2.0, hypothesis-6.88.4\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_tmp_path.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pytest test_tmp_path.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebc33f68",
   "metadata": {},
   "source": [
    "### Pandera: a Python Library to Validate Your Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3906a7",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U pandera"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eafdc0b4",
   "metadata": {},
   "source": [
    "Poor data quality can lead to incorrect conclusions and bad model performance. Thus, it is important to check data for consistency and reliability before using it. \n",
    "\n",
    "pandera makes it easy to perform data validation on dataframe-like objects. If the dataframe does not pass validation checks, pandera provides useful error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45a2573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:52:57.721905Z",
     "start_time": "2021-09-11T18:52:57.661626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>store</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>Aldi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name    store  price\n",
       "0   apple     Aldi      2\n",
       "1  banana  Walmart      1\n",
       "2   apple  Walmart      4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fruits = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"apple\", \"banana\", \"apple\"],\n",
    "        \"store\": [\"Aldi\", \"Walmart\", \"Walmart\"],\n",
    "        \"price\": [2, 1, 4],\n",
    "    }\n",
    ")\n",
    "\n",
    "fruits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a453b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_fruits = [\"apple\", \"banana\", \"orange\"]\n",
    "nearby_stores = [\"Aldi\", \"Walmart\"]\n",
    "\n",
    "import pandera as pa\n",
    "from pandera import Column, Check\n",
    "\n",
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"name\": Column(str, Check.isin(available_fruits)),\n",
    "        \"store\": Column(str, Check.isin(nearby_stores)),\n",
    "        \"price\": Column(int, Check.less_than(4)),\n",
    "    }\n",
    ")\n",
    "schema.validate(fruits) # validation fails"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e94fb1e7",
   "metadata": {},
   "source": [
    "Output:\n",
    "```bash\n",
    "SchemaError:  failed element-wise validator 0:\n",
    "\n",
    "failure cases:\n",
    "   index  failure_case\n",
    "0      2             4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1dd4d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>store</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apple</td>\n",
       "      <td>Aldi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>banana</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apple</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>orange</td>\n",
       "      <td>Aldi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name    store  price\n",
       "0   apple     Aldi      2\n",
       "1  banana  Walmart      1\n",
       "2   apple  Walmart      3\n",
       "3  orange     Aldi      4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"name\": Column(str, Check.isin(available_fruits)),\n",
    "        \"store\": Column(str, Check.isin(nearby_stores)),\n",
    "        \"price\": Column(int, Check.less_than(5)),\n",
    "    }\n",
    ")\n",
    "schema.validate(fruits) # validation succeeds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f416a669",
   "metadata": {},
   "source": [
    "With pandera’s decorator `check_input`, you can validate input data before calling a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69badd3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:53:11.865113Z",
     "start_time": "2021-09-11T18:53:11.828443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandera import check_input\n",
    "\n",
    "\n",
    "fruits = pd.DataFrame(\n",
    "    {\n",
    "        \"name\": [\"apple\", \"banana\", \"apple\"],\n",
    "        \"store\": [\"Aldi\", \"Walmart\", \"Walmart\"],\n",
    "        \"price\": [2, 1, 5],\n",
    "    }\n",
    ")\n",
    "\n",
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"name\": Column(str, Check.isin(available_fruits)),\n",
    "        \"store\": Column(str, Check.isin(nearby_stores)),\n",
    "        \"price\": Column(int, Check.less_than(6)),\n",
    "    }\n",
    ")\n",
    "\n",
    "@check_input(schema)\n",
    "def get_total_price(fruits: pd.DataFrame):\n",
    "    return fruits.price.sum()\n",
    "\n",
    "get_total_price(fruits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56d1e7ae",
   "metadata": {},
   "source": [
    "[Link to Pandera](https://pandera.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d49a6",
   "metadata": {},
   "source": [
    "### Efficiently Generate Falsified Examples for Unit Tests with Pandera and Hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c379819e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install hypothesis pandera pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d1742",
   "metadata": {},
   "source": [
    "Generating readable edge cases for unit tests can often be a challenging task. However, with the combined power of Pandera and Hypothesis, you can efficiently detect falsified examples and write cleaner tests.\n",
    "\n",
    "Pandera allows you to define constraints for inputs and outputs, while Hypothesis automatically identifies edge cases that match the specified schema. Hypothesis further simplifies complex examples until it finds a smaller example that still reproduces the issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0ca0f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_processing_fn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_processing_fn.py\n",
    "import hypothesis\n",
    "import pandera as pa\n",
    "\n",
    "# Specify the schema of the df used for testing\n",
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"val1\": pa.Column(int, pa.Check.in_range(-2, 3)),\n",
    "        \"val2\": pa.Column(int, pa.Check.in_range(-2, 3)),\n",
    "    }\n",
    ")\n",
    "\n",
    "out_schema = schema.add_columns(\n",
    "    {\n",
    "        \"val3\": pa.Column(float, pa.Check.in_range(-2, 3)),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "@pa.check_output(out_schema)\n",
    "def processing_fn(df):\n",
    "    processed = df.assign(val3=df.val1/df.val2)\n",
    "    return processed\n",
    "\n",
    "\n",
    "@hypothesis.given(schema.strategy(size=5)) # Generate 5 examples\n",
    "def test_processing_fn(dataframe):\n",
    "    processing_fn(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd8517",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pytest test_processing_fn.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d924f83c",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_processing_fn.py \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eec17f",
   "metadata": {},
   "source": [
    "```bash\n",
    "test_processing_fn.py F                                                  [100%]\n",
    "\n",
    "=================================== FAILURES ===================================\n",
    "______________________________ test_processing_fn ______________________________\n",
    "pandera.errors.SchemaError: error in check_output decorator of function 'processing_fn': non-nullable series 'val3' contains null values:\n",
    "0   NaN\n",
    "1   NaN\n",
    "2   NaN\n",
    "3   NaN\n",
    "4   NaN\n",
    "Name: val3, dtype: float64\n",
    "Falsifying example: test_processing_fn(\n",
    "    dataframe=\n",
    "           val1  val2\n",
    "        0     0     0\n",
    "        1     0     0\n",
    "        2     0     0\n",
    "        3     0     0\n",
    "        4     0     0\n",
    ")\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3b11c7d",
   "metadata": {},
   "source": [
    "### DeepDiff Find Deep Differences of Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63860a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:17:50.028351Z",
     "start_time": "2021-11-22T13:17:44.520188Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deepdiff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdd83ce9",
   "metadata": {},
   "source": [
    "When testing the outputs of your functions, it can be frustrated to see your tests fail because of something you don't care too much about such as: \n",
    "\n",
    "- order of items in a list\n",
    "\n",
    "- different ways to specify the same thing such as abbreviation\n",
    "\n",
    "- exact value up to the last decimal point, etc\n",
    "\n",
    "\n",
    "Is there a way that you can exclude certain parts of the object from the comparison? That is when DeepDiff comes in handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f3301f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:18:00.366533Z",
     "start_time": "2021-11-22T13:18:00.121414Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepdiff import DeepDiff "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e9481da",
   "metadata": {},
   "source": [
    "DeepDiff can output a meaningful comparison like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1e7fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:24:27.583499Z",
     "start_time": "2021-11-22T13:24:27.572205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'values_changed': {\"root['banana'][0]\": {'new_value': 2, 'old_value': 3},\n",
       "  \"root['banana'][1]\": {'new_value': 3, 'old_value': 2}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price1 = {'apple': 2, 'orange': 3, 'banana': [3, 2]}\n",
    "price2 = {'apple': 2, 'orange': 3, 'banana': [2, 3]}\n",
    "\n",
    "DeepDiff(price1, price2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9132debf",
   "metadata": {},
   "source": [
    "With DeepDiff, you also have full control of which characteristics of the Python object DeepDiff should ignore. In the example below, since the order is ignored `[3, 2]` is equivalent to `[2, 3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d47c262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:24:31.055481Z",
     "start_time": "2021-11-22T13:24:31.045776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore orders \n",
    "\n",
    "DeepDiff(price1, price2, ignore_order=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3123f38",
   "metadata": {},
   "source": [
    "We can also exclude certain part of our object from the comparison. In the code below, we ignore `ml` and `machine learning` since `ml` is a abbreviation of `machine learning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93d493b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:27:04.545073Z",
     "start_time": "2021-11-22T13:27:04.514993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"experience1 = {\\\"machine learning\\\": 2, \\\"python\\\": 3}\\nexperience2 = {\\\"ml\\\": 2, \\\"python\\\": 3}\\n\\nDeepDiff(\\n    experience1,\\n    experience2,\\n    exclude_paths={\\\"root['ml']\\\", \\\"root['machine learning']\\\"},\\n)\";\n",
       "                var nbb_formatted_code = \"experience1 = {\\\"machine learning\\\": 2, \\\"python\\\": 3}\\nexperience2 = {\\\"ml\\\": 2, \\\"python\\\": 3}\\n\\nDeepDiff(\\n    experience1,\\n    experience2,\\n    exclude_paths={\\\"root['ml']\\\", \\\"root['machine learning']\\\"},\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experience1 = {\"machine learning\": 2, \"python\": 3}\n",
    "experience2 = {\"ml\": 2, \"python\": 3}\n",
    "\n",
    "DeepDiff(\n",
    "    experience1,\n",
    "    experience2,\n",
    "    exclude_paths={\"root['ml']\", \"root['machine learning']\"},\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "709e8ea7",
   "metadata": {},
   "source": [
    "Cmpare 2 numbers up to a specific decimal point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87f96e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:28:38.951195Z",
     "start_time": "2021-11-22T13:28:38.932287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"num1 = 0.258\\nnum2 = 0.259\\n\\nDeepDiff(num1, num2, significant_digits=2)\";\n",
       "                var nbb_formatted_code = \"num1 = 0.258\\nnum2 = 0.259\\n\\nDeepDiff(num1, num2, significant_digits=2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num1 = 0.258\n",
    "num2 = 0.259\n",
    "\n",
    "DeepDiff(num1, num2, significant_digits=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f8cfa53",
   "metadata": {},
   "source": [
    "[Link to DeepDiff](https://github.com/seperman/deepdiff)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fafb772f",
   "metadata": {},
   "source": [
    "### dirty-equals: Write Declarative Assertions in Your Unit Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c37f0e4",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install dirty-equals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c72643f1",
   "metadata": {},
   "source": [
    "If you want to write declarative assertions and avoid boilerplate code in your unit tests, try dirty_equals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe65e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dirty_equals import IsNow, IsPartialDict, IsList, IsStr, IsTrueLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02ebc2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "shopping = {\n",
    "    \"time\": datetime.today().now(),\n",
    "    \"quantity\": {\"apple\": 1, \"banana\": 2, \"orange\": 1},\n",
    "    \"locations\": [\"Walmart\", \"Aldi\"],\n",
    "    \"is_male\": 1 \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96487678",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert shopping == {\n",
    "    \"time\": IsNow(delta=timedelta(hours=1)),\n",
    "    \"quantity\": IsPartialDict(apple=1, orange=1),\n",
    "    \"locations\": IsList(\"Aldi\", \"Walmart\", check_order=False),\n",
    "    \"is_male\": IsTrueLike\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9b76b6a",
   "metadata": {},
   "source": [
    "[Link to dirty-equals](https://github.com/samuelcolvin/dirty-equals)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f59c4732",
   "metadata": {},
   "source": [
    "### hypothesis: Property-based Testing in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb458fe8",
   "metadata": {
    "tags": [
     "hide-code"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install hypothesis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a5711c6",
   "metadata": {},
   "source": [
    "If you want to test some properties or assumptions, it can be cumbersome to write a wide range of scenarios. To automatically run your tests against a wide range of scenarios and find edge cases in your code that you would otherwise have missed, use hypothesis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d864f27f",
   "metadata": {},
   "source": [
    "In the code below, I test if the addition of two floats is commutative. The test fails when either `x` or `y` is `NaN`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d724e527",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_hypothesis.py \n",
    "from hypothesis import given\n",
    "from hypothesis.strategies import floats\n",
    "\n",
    "\n",
    "\n",
    "@given(floats(), floats())\n",
    "def test_floats_are_commutative(x, y):\n",
    "    assert x + y == y + x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e041cdb0",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_hypothesis.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04224a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T14:05:11.335715Z",
     "start_time": "2021-12-15T14:05:10.283884Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.8.10, pytest 6.2.5, pytest-sugar 0.9.4)\u001b[0m\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, sugar-0.9.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      "\n",
      "――――――――――――――――――――――――― test_floats_are_commutative ――――――――――――――――――――――――――\n",
      "\n",
      "    \u001b[37m@given\u001b[39;49;00m(floats(), floats())\n",
      ">   \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_floats_are_commutative\u001b[39;49;00m(x, y):\n",
      "\n",
      "\u001b[1m\u001b[31mtest_hypothesis.py\u001b[0m:7: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = 0.0, y = nan\n",
      "\n",
      "    \u001b[37m@given\u001b[39;49;00m(floats(), floats())\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_floats_are_commutative\u001b[39;49;00m(x, y):\n",
      ">       \u001b[94massert\u001b[39;49;00m x + y == y + x\n",
      "\u001b[1m\u001b[31mE       assert (0.0 + nan) == (nan + 0.0)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_hypothesis.py\u001b[0m:8: AssertionError\n",
      "---------------------------------- Hypothesis ----------------------------------\n",
      "Falsifying example: test_floats_are_commutative(\n",
      "    x=0.0, y=nan,  # Saw 1 signaling NaN\n",
      ")\n",
      "\n",
      " \u001b[36m\u001b[0mtest_hypothesis.py\u001b[0m \u001b[31m⨯\u001b[0m                                            \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_hypothesis.py::test_floats_are_commutative - assert (0.0 + nan) =...\n",
      "\n",
      "Results (0.38s):\n",
      "\u001b[31m       1 failed\u001b[0m\n",
      "         - \u001b[36m\u001b[0mtest_hypothesis.py\u001b[0m:6 \u001b[31mtest_floats_are_commutative\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_hypothesis.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68b8929b",
   "metadata": {},
   "source": [
    "Now I can rewrite my code to make it more robust against these edge cases. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d56dd3da",
   "metadata": {},
   "source": [
    "[Link to hypothesis](https://hypothesis.readthedocs.io/en/latest/quickstart.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d030f9a2",
   "metadata": {},
   "source": [
    "### Deepchecks: Check Category Mismatch Between Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8fa6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:33:45.159767Z",
     "start_time": "2022-01-21T15:33:31.410396Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deepchecks "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12e9c3e8",
   "metadata": {},
   "source": [
    "Sometimes, it is important to know if your test set contains the same categories in the train set. If you want to check the category mismatch between the train and test set, use Deepchecks's `CategoryMismatchTrainTest`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23406d85",
   "metadata": {},
   "source": [
    "In the example below, the result shows that there are 2 new categories in the test set. They are 'd' and 'e'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e214c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:41:30.697748Z",
     "start_time": "2022-01-21T15:41:30.682598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"from deepchecks.checks.integrity.new_category import CategoryMismatchTrainTest\\nfrom deepchecks.base import Dataset\\nimport pandas as pd\";\n",
       "                var nbb_formatted_code = \"from deepchecks.checks.integrity.new_category import CategoryMismatchTrainTest\\nfrom deepchecks.base import Dataset\\nimport pandas as pd\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.checks.integrity.new_category import CategoryMismatchTrainTest\n",
    "from deepchecks.base import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d7f941c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:41:31.957360Z",
     "start_time": "2022-01-21T15:41:31.947954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"train = pd.DataFrame({'col1': ['a', 'b', 'c']})\\ntest = pd.DataFrame({'col1': ['c', 'd', 'e']})\\n\\ntrain_ds = Dataset(train, cat_features=['col1'])\\ntest_ds = Dataset(test, cat_features=['col1'])\";\n",
       "                var nbb_formatted_code = \"train = pd.DataFrame({\\\"col1\\\": [\\\"a\\\", \\\"b\\\", \\\"c\\\"]})\\ntest = pd.DataFrame({\\\"col1\\\": [\\\"c\\\", \\\"d\\\", \\\"e\\\"]})\\n\\ntrain_ds = Dataset(train, cat_features=[\\\"col1\\\"])\\ntest_ds = Dataset(test, cat_features=[\\\"col1\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.DataFrame({\"col1\": [\"a\", \"b\", \"c\"]})\n",
    "test = pd.DataFrame({\"col1\": [\"c\", \"d\", \"e\"]})\n",
    "\n",
    "train_ds = Dataset(train, cat_features=[\"col1\"])\n",
    "test_ds = Dataset(test, cat_features=[\"col1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "465d2e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:44:25.958451Z",
     "start_time": "2022-01-21T15:44:25.930984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Category Mismatch Train Test</h4><p>Find new categories in the test set.</p><h5>Additional Outputs</h5><style type=\"text/css\">\n",
       "#T_1a4e2_ table {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ thead {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ tbody {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ th {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ td {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1a4e2_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Number of new categories</th>\n",
       "      <th class=\"col_heading level0 col1\" >Percent of new categories in sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >New categories examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Column</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1a4e2_level0_row0\" class=\"row_heading level0 row0\" >col1</th>\n",
       "      <td id=\"T_1a4e2_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_1a4e2_row0_col1\" class=\"data row0 col1\" >66.67%</td>\n",
       "      <td id=\"T_1a4e2_row0_col2\" class=\"data row0 col2\" >['d', 'e']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"CategoryMismatchTrainTest().run(train_ds, test_ds)\";\n",
       "                var nbb_formatted_code = \"CategoryMismatchTrainTest().run(train_ds, test_ds)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CategoryMismatchTrainTest().run(train_ds, test_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22584fe0",
   "metadata": {},
   "source": [
    "[Link to Deepchecks](https://docs.deepchecks.com/en/stable/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "543d97cb",
   "metadata": {},
   "source": [
    "### leAB: AB Testing Analysis in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45056791",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install leab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f271989",
   "metadata": {},
   "source": [
    "AB testing is crucial for assessing the effectiveness of changes in a controlled environment. With the leAB library, you can compute the appropriate sample size before launching the test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62c61ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20177"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from leab import before\n",
    "\n",
    "# What is the number of sample needed per variation to detect a 1% result \n",
    "# difference in a population with a 15% conversion rate?\n",
    "ab_test = before.leSample(conversion_rate=15, min_detectable_effect=1)\n",
    "ab_test.get_size_per_variation()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e941b62e",
   "metadata": {},
   "source": [
    "After reaching the sample size, you can compare the successes between group A and group B. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b7fdc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   success\n",
       "0        1\n",
       "1        0\n",
       "2        1\n",
       "3        1\n",
       "4        0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from leab import after, leDataset\n",
    "\n",
    "# Import sample data for A and B\n",
    "data = leDataset.SampleLeSuccess()\n",
    "data.A.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "661cbf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No significant difference'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_test = after.leSuccess(data.A, data.B, confidence_level=0.95)\n",
    "\n",
    "# Get the conclusion on the test \n",
    "ab_test.get_verdict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e67559a0",
   "metadata": {},
   "source": [
    "[Link to leAB](https://github.com/tlentali/leab)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f1993ce",
   "metadata": {},
   "source": [
    "### pytest-postgresql: Incorporate Database Testing into Your pytest Test Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebb9ea5",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-postgresql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e13d0d9",
   "metadata": {},
   "source": [
    "If you want to incorporate database testing seamlessly within your pytest test suite, use pytest-postgresql.\n",
    "\n",
    "pytest-postgres provides fixtures that manage the setup and cleanup of test databases, ensuring repeatable tests. Additionally, each test runs in isolation, preventing any impact on the production database from testing changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737b7abf",
   "metadata": {},
   "source": [
    "To see how pytest-postgres works, let's create a test function that sets up a test table in a PostgreSQL database, inserts some test data, and then verifies that the query results match the expected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435bef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test_postgres.py \n",
    "def test_query_results(postgresql):\n",
    "    \"\"\"Check that the query results are as expected.\"\"\"\n",
    "    with postgresql.cursor() as cur:\n",
    "        cur.execute(\"CREATE TABLE test_table (id SERIAL PRIMARY KEY, name VARCHAR);\")\n",
    "        cur.execute(\"INSERT INTO test_table (name) VALUES ('John'), ('Jane'), ('Alice');\")\n",
    "\n",
    "        # Assert the results\n",
    "        cur.execute(\"SELECT * FROM test_table;\")\n",
    "        assert cur.fetchall() == [(1, 'John'), (2, 'Jane'), (3, 'Alice')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de96acd7",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_postgres.py \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d71b6c5d",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.6, pytest-7.2.1, pluggy-1.0.0\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: dash-2.10.2, postgresql-5.0.0, anyio-3.6.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_postgres.py \u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.20s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_postgres.py "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f44b1127",
   "metadata": {},
   "source": [
    "[Link to pytest-postgresql](https://github.com/ClearcodeHQ/pytest-postgresql)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "987ab29c",
   "metadata": {},
   "source": [
    "### Maintain the Accuracy of Docstring Examples with Doctest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e30d195d",
   "metadata": {},
   "source": [
    "Including examples in a docstring is helpful. However, examples can become obsolete as the function evolves.\n",
    "\n",
    "To ensure that the examples remain accurate, use doctest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1b3efb7",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile example.py \n",
    "def perc_difference(n1, n2):\n",
    "    \"\"\"Return the percentage difference between two numbers, n1 and n2.\n",
    "\n",
    "    Formula: ((n2 - n1) / n1) * 100\n",
    "\n",
    "    Examples:\n",
    "    >>> perc_difference(50, 60)\n",
    "    20.0\n",
    "    >>> perc_difference(100, 100)\n",
    "    0.0\n",
    "\n",
    "    :param n1: The first number (the original value).\n",
    "    :param n2: The second number (the new value).\n",
    "    :return: The percentage difference between n1 and n2 as a float.\n",
    "    \"\"\"\n",
    "    return ((n2 - n1) / n1) * 100\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5c1ad2f",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ python example.py -v\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11b5f256",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying:\n",
      "    perc_difference(50, 60)\n",
      "Expecting:\n",
      "    20.0\n",
      "ok\n",
      "Trying:\n",
      "    perc_difference(100, 100)\n",
      "Expecting:\n",
      "    0.0\n",
      "ok\n",
      "1 items had no tests:\n",
      "    __main__\n",
      "1 items passed all tests:\n",
      "   2 tests in __main__.perc_difference\n",
      "2 tests in 2 items.\n",
      "2 passed and 0 failed.\n",
      "Test passed.\n"
     ]
    }
   ],
   "source": [
    "!python example.py -v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "283f6cd6",
   "metadata": {},
   "source": [
    "### DeepEval: Unit Testing for Your LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3a23d9",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U deepeval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3d18c54",
   "metadata": {},
   "source": [
    "When deploying an LLM model to production, it's crucial that the model is accurate, relevant to the specific question, and free from biases.\n",
    "\n",
    "DeepEval simplifies unit testing of LLM outputs in Python using these criteria.\n",
    "\n",
    "In the following code, we use DeepEval to check if the LLM output is accurate and aligns with established facts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b79ba41f",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_chatbot.py\n",
    "import pytest\n",
    "from deepeval.metrics.factual_consistency import FactualConsistencyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.run_test import assert_test\n",
    "\n",
    "def test_case():\n",
    "    query = \"What if these shoes don't fit?\"\n",
    "    context = \"All customers are eligible for a 30 day full refund.\"\n",
    "\n",
    "    # Replace this with the actual output from your LLM application\n",
    "    actual_output = \"We offer a 30-day full refund.\"\n",
    "    factual_consistency_metric = FactualConsistencyMetric(minimum_score=0.7)\n",
    "    test_case = LLMTestCase(query=query, output=actual_output, context=context)\n",
    "    assert_test(test_case, [factual_consistency_metric])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e0fecee",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ deepeval test run test_chatbot.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74a617a5",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.9.6, pytest-7.2.1, pluggy-1.0.0 -- /Users/khuyentran/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/khuyentran/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\n",
      "plugins: deepeval-0.20.0, postgresql-5.0.0, dash-2.13.0, typeguard-4.1.2, anyio-3.6.2\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "\u001b[2KDownloading FactualConsistencyModel (may take up to 2 minutes if running for th…\n",
      "\u001b[1A\u001b[2K\u001b[32mPASSED\u001b[0mRunning teardown with pytest sessionfinish\u001b[33m...\u001b[0m\n",
      "\n",
      "\n",
      "============================= slowest 10 durations =============================\n",
      "7.00s call     test_chatbot.py::test_case\n",
      "\n",
      "(2 durations < 0.005s hidden.  Use -vv to show these durations.)\n",
      "\u001b[33m======================== \u001b[32m1 passed\u001b[0m, \u001b[33m\u001b[1m2 warnings\u001b[0m\u001b[33m in 7.02s\u001b[0m\u001b[33m =========================\u001b[0m\n",
      "\u001b[3m                                 Test Results                                  \u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m             Metric\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m     Average Score\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPasses\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mFailures\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mSuccess Rate\u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n",
      "│ Factual Consistency │ 0.9911543130874634 │      \u001b[32m1\u001b[0m │        \u001b[31m0\u001b[0m │      100.00% │\n",
      "│               Total │                  - │      \u001b[32m1\u001b[0m │        \u001b[31m0\u001b[0m │      100.00% │\n",
      "└─────────────────────┴────────────────────┴────────┴──────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "!deepeval test run test_chatbot.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9e1de96",
   "metadata": {},
   "source": [
    "[Link to DeepEval](https://github.com/confident-ai/deepeval)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
