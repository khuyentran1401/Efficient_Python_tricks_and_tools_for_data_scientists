{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7820a007",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57c914",
   "metadata": {},
   "source": [
    "### pytest benchmark: A Pytest Fixture to Benchmark Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c189b0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf852a8",
   "metadata": {},
   "source": [
    "If you want to benchmark your code while testing with pytest, try pytest-benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815482f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:42:17.296160Z",
     "start_time": "2021-09-11T18:42:17.288429Z"
    }
   },
   "source": [
    "To use pytest-benchmark works, add `benchmark` to the test function that you want to benchmark. \n",
    "\n",
    "```python\n",
    "# pytest_benchmark_example.py\n",
    "def list_comprehension(len_list=5):\n",
    "    return [i for i in range(len_list)]\n",
    "\n",
    "\n",
    "def test_concat(benchmark):\n",
    "    res = benchmark(list_comprehension)\n",
    "    assert res == [0, 1, 2, 3, 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4fa910",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_benchmark_example.py\n",
    "```\n",
    "Now you should see the statistics of the time it takes to execute the test functions on your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8ae8527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T16:36:26.021951Z",
     "start_time": "2021-09-12T16:36:22.946901Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_benchmark_example.py \u001b[32m.\u001b[0m\u001b[32m                                            [100%]\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33m----------------------------------------------------- benchmark: 1 tests ----------------------------------------------------\u001b[0m\n",
      "Name (time in ns)          Min         Max      Mean    StdDev    Median     IQR   Outliers  OPS (Mops/s)  Rounds  Iterations\n",
      "\u001b[33m-----------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_concat         \u001b[1m  286.4501\u001b[0m\u001b[1m  4,745.5498\u001b[0m\u001b[1m  309.3872\u001b[0m\u001b[1m  106.6583\u001b[0m\u001b[1m  297.5001\u001b[0m\u001b[1m  5.3500\u001b[0m  2686;5843\u001b[1m        3.2322\u001b[0m  162101          20\n",
      "\u001b[33m-----------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 2.47s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"!pytest pytest_benchmark_example.py \";\n",
       "                var nbb_formatted_code = \"!pytest pytest_benchmark_example.py\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pytest pytest_benchmark_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d71ab4",
   "metadata": {},
   "source": [
    "[Link to pytest-benchmark](https://github.com/ionelmc/pytest-benchmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bd092",
   "metadata": {},
   "source": [
    "### pytest.mark.parametrize: Test Your Functions with Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2d9ec",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e88ff1",
   "metadata": {},
   "source": [
    "If you want to test your function with different examples, use `pytest.mark.parametrize` decorator.\n",
    "\n",
    "To use `pytest.mark.parametrize`, add `@pytest.mark.parametrize` to the test function that you want to experiment with. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad6ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:43:09.283883Z",
     "start_time": "2021-09-11T18:43:09.273973Z"
    }
   },
   "source": [
    "```python\n",
    "# pytest_parametrize.py\n",
    "import pytest\n",
    "\n",
    "def text_contain_word(word: str, text: str):\n",
    "    '''Find whether the text contains a particular word'''\n",
    "    \n",
    "    return word in text\n",
    "\n",
    "test = [\n",
    "    ('There is a duck in this text',True),\n",
    "    ('There is nothing here', False)\n",
    "    ]\n",
    "\n",
    "@pytest.mark.parametrize('sample, expected', test)\n",
    "def test_text_contain_word(sample, expected):\n",
    "\n",
    "    word = 'duck'\n",
    "\n",
    "    assert text_contain_word(word, sample) == expected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263702d",
   "metadata": {},
   "source": [
    "In the code above, I expect the first sentence to contain the word “duck” and expect the second sentence not to contain that word. Let's see if my expectations are correct by running:\n",
    "```bash\n",
    "$ pytest pytest_parametrize.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "031ac02c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:43:10.924823Z",
     "start_time": "2021-09-11T18:43:10.494719Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\r\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\n",
      "rootdir: /home/khuyen/book/book/Chapter4\r\n",
      "plugins: benchmark-3.4.1, anyio-3.3.0\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r\n",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "pytest_parametrize.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                 [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_parametrize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b4f51",
   "metadata": {},
   "source": [
    "Sweet! 2 tests passed when running pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a725d73",
   "metadata": {},
   "source": [
    "[Link to my article about pytest](https://towardsdatascience.com/pytest-for-data-scientists-2990319e55e6?sk=2d3a81903b154db0c7ca832b9f29fee8).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b236f2",
   "metadata": {},
   "source": [
    "### pytest parametrize twice: Test All Possible Combinations of Two Sets of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef317c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ccdbf9",
   "metadata": {},
   "source": [
    "If you want to test the combinations of two sets of parameters, writing all possible combinations can be time-consuming and is difficult to read. \n",
    "\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "def perc_difference(n1, n2):\n",
    "    return (n2 - n1)/n1 * 100\n",
    "\n",
    "# Test the combinations of operations and inputs\n",
    "@pytest.mark.parametrize(\"operation, n1, n2\", [(average, 1, 2), (average, 2, 3), (perc_difference, 1, 2), (perc_difference, 2, 3)])\n",
    "def test_is_float(operation, n1, n2):\n",
    "    assert isinstance(operation(n1, n2), float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c28b30c",
   "metadata": {},
   "source": [
    "You can save your time by using `pytest.mark.parametrize` twice instead.\n",
    "```python\n",
    "# pytest_combination.py\n",
    "import pytest\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "def perc_difference(n1, n2):\n",
    "    return (n2 - n1)/n1 * 100\n",
    "\n",
    "# Test the combinations of operations and inputs\n",
    "@pytest.mark.parametrize(\"operation\", [average, perc_difference])\n",
    "@pytest.mark.parametrize(\"n1, n2\", [(1, 2), (2, 3)])\n",
    "def test_is_float(operation, n1, n2):\n",
    "    assert isinstance(operation(n1, n2), float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c881522",
   "metadata": {},
   "source": [
    "On your terminal, run:\n",
    "```bash\n",
    "$ pytest -v pytest_combination.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c94aef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-23T15:06:56.991305Z",
     "start_time": "2022-02-23T15:06:56.158431Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1 -- /home/khuyen/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/khuyen/book/book/Chapter5/.hypothesis/examples')\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, typeguard-2.13.3\n",
      "collected 4 items                                                              \u001b[0m\n",
      "\n",
      "pytest_combination.py::test_is_float[1-2-average] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 25%]\u001b[0m\n",
      "pytest_combination.py::test_is_float[1-2-perc_difference] \u001b[32mPASSED\u001b[0m\u001b[32m         [ 50%]\u001b[0m\n",
      "pytest_combination.py::test_is_float[2-3-average] \u001b[32mPASSED\u001b[0m\u001b[32m                 [ 75%]\u001b[0m\n",
      "pytest_combination.py::test_is_float[2-3-perc_difference] \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_combination.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2041b7e5",
   "metadata": {},
   "source": [
    "From the output above, we can see that all possible combinations of the given operations and inputs are tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378ff50",
   "metadata": {},
   "source": [
    "### Assign IDs to Test Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059a737d",
   "metadata": {},
   "source": [
    "When using pytest parametrize, it can be difficult to understand the role of each test case.\n",
    "\n",
    "```python\n",
    "# pytest_without_ids.py\n",
    "\n",
    "from pytest import mark\n",
    "\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "@mark.parametrize(\n",
    "    \"n1, n2\",\n",
    "    [(-1, -2), (2, 3), (0, 0)],\n",
    ")\n",
    "def test_is_float(n1, n2):\n",
    "    assert isinstance(average(n1, n2), float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8fa45",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest -v pytest_without_ids.py \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1502853a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-16T13:34:28.749494Z",
     "start_time": "2022-03-16T13:34:27.844208Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1 -- /home/khuyen/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/khuyen/book/book/Chapter5/.hypothesis/examples')\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, cases-3.6.10, typeguard-2.13.3\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "pytest_without_ids.py::test_is_float[-1--2] \u001b[32mPASSED\u001b[0m\u001b[32m                       [ 33%]\u001b[0m\n",
      "pytest_without_ids.py::test_is_float[2-3] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 66%]\u001b[0m\n",
      "pytest_without_ids.py::test_is_float[0-0] \u001b[32mPASSED\u001b[0m\u001b[32m                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.26s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_without_ids.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d748a",
   "metadata": {},
   "source": [
    "You can add `ids` to pytest parametrize to assign a name to each test case.\n",
    "\n",
    "```python\n",
    "# pytest_ids.py\n",
    "\n",
    "from pytest import mark\n",
    "\n",
    "def average(n1, n2):\n",
    "    return (n1 + n2) / 2\n",
    "\n",
    "@mark.parametrize(\n",
    "    \"n1, n2\",\n",
    "    [(-1, -2), (2, 3), (0, 0)],\n",
    "    ids=[\"neg and neg\", \"pos and pos\", \"zero and zero\"],\n",
    ")\n",
    "def test_is_float(n1, n2):\n",
    "    assert isinstance(average(n1, n2), float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7872c3a6",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest -v pytest_ids.py \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2c76e2",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1 -- /home/khuyen/book/venv/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/khuyen/book/book/Chapter5/.hypothesis/examples')\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, cases-3.6.10, typeguard-2.13.3\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "pytest_ids.py::test_is_float[neg and neg] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 33%]\u001b[0m\n",
      "pytest_ids.py::test_is_float[pos and pos] \u001b[32mPASSED\u001b[0m\u001b[32m                         [ 66%]\u001b[0m\n",
      "pytest_ids.py::test_is_float[zero and zero] \u001b[32mPASSED\u001b[0m\u001b[32m                       [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -v pytest_ids.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392f742",
   "metadata": {},
   "source": [
    "We can see that instead of `[-1--2]`, the first test case is shown as `neg and neg`. This makes it easier for others to understand the roles of your test cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac57677",
   "metadata": {},
   "source": [
    "### Pytest Fixtures: Use The Same Data for Different Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c745a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5256a1b",
   "metadata": {},
   "source": [
    "If you want to use the same data to test different functions, use pytest fixtures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce9cb2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:45:26.995394Z",
     "start_time": "2021-09-11T18:45:26.472717Z"
    }
   },
   "source": [
    "To use pytest fixtures,  add the decorator `@pytest.fixture` to the function that creates the data you want to reuse.\n",
    "\n",
    "```python\n",
    "# pytest_fixture.py\n",
    "import pytest \n",
    "from textblob import TextBlob\n",
    "\n",
    "def extract_sentiment(text: str):\n",
    "    \"\"\"Extract sentimetn using textblob. Polarity is within range [-1, 1]\"\"\"\n",
    "    \n",
    "    text = TextBlob(text)\n",
    "    return text.sentiment.polarity\n",
    "\n",
    "@pytest.fixture \n",
    "def example_data():\n",
    "    return 'Today I found a duck and I am happy'\n",
    "\n",
    "def test_extract_sentiment(example_data):\n",
    "    sentiment = extract_sentiment(example_data)\n",
    "    assert sentiment > 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29564978",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_fixture.py\n",
    "```\n",
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79658c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:45:46.386950Z",
     "start_time": "2021-09-11T18:45:45.299302Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: benchmark-3.4.1, anyio-3.3.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_fixture.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.53s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_fixture.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff3fcd1",
   "metadata": {},
   "source": [
    "### Pytest skipif: Skip a Test When a Condition is Not Met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338ccba",
   "metadata": {},
   "source": [
    "If you want to skip a test when a condition is not met, use pytest `skipif`. For example, in the code below, I use `skipif` to skip a test if the python version is less than 3.9.\n",
    "\n",
    "```python\n",
    "# pytest_skip.py\n",
    "import sys\n",
    "import pytest \n",
    "\n",
    "def add_two(num: int):\n",
    "    return num + 2 \n",
    "\n",
    "@pytest.mark.skipif(sys.version_info < (3, 9), reason=\"Eequires Python 3.9 or higher\")\n",
    "def test_add_two(): \n",
    "    assert add_two(3) == 5\n",
    "```\n",
    "\n",
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_skip.py -v \n",
    "```\n",
    "\n",
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3df6c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-05T14:38:54.377174Z",
     "start_time": "2022-05-05T14:38:53.709196Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.10, pytest-7.1.2, pluggy-1.0.0 -- /Users/khuyen/book/venv/bin/python3\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /Users/khuyen/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "pytest_skip.py::test_add_two \u001b[33mSKIPPED\u001b[0m (Eequires Python 3.9 or higher)\u001b[33m     [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[33m============================== \u001b[33m\u001b[1m1 skipped\u001b[0m\u001b[33m in 0.01s\u001b[0m\u001b[33m ==============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_skip.py -v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb08f7",
   "metadata": {},
   "source": [
    "### Pytest repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8f66f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ad279",
   "metadata": {},
   "source": [
    "It is a good practice to test your functions to make sure they work as expected, but sometimes you need to test 100 times until you found the rare cases when the test fails. That is when pytest-repeat comes in handy.\n",
    "\n",
    "To use pytest-repeat, add the decorator `@pytest.mark.repeat(N)` to the test function you want to repeat `N` times\n",
    "\n",
    "```python\n",
    "# pytest_repeat_example.py\n",
    "import pytest \n",
    "import random \n",
    "\n",
    "def generate_numbers():\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "@pytest.mark.repeat(100)\n",
    "def test_generate_numbers():\n",
    "    assert generate_numbers() > 1 and generate_numbers() < 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae78f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:50:57.169539Z",
     "start_time": "2021-09-11T18:50:57.161819Z"
    }
   },
   "source": [
    "```python\n",
    "# pytest_repeat_example.py\n",
    "import pytest \n",
    "import random \n",
    "\n",
    "def generate_numbers():\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "@pytest.mark.repeat(100)\n",
    "def test_generate_numbers():\n",
    "    assert generate_numbers() > 1 and generate_numbers() < 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518854b9",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "pytest pytest_repeat_example.py\n",
    "```\n",
    "We can see that 100 experiments are executed and passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7291ca8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:51:02.831644Z",
     "start_time": "2021-09-11T18:51:02.331407Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0\n",
      "collected 100 items                                                            \u001b[0m\n",
      "\n",
      "pytest_repeat_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 47%]\n",
      "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m100 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_repeat_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb66973",
   "metadata": {},
   "source": [
    "[Link to pytest-repeat](https://github.com/pytest-dev/pytest-repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0eb505",
   "metadata": {},
   "source": [
    "### pytest-sugar: Show the Failures and Errors Instantly With a Progress Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c47f15",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-sugar "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933a0f98",
   "metadata": {},
   "source": [
    "It can be frustrating to wait for a lot of tests to run before knowing the status of the tests. If you want to see the failures and errors instantly with a progress bar, use pytest-sugar. \n",
    "\n",
    "pytest-sugar is a plugin for pytest. The code below shows how the outputs will look like when running pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51987496",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8669e10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-29T13:59:12.920920Z",
     "start_time": "2021-11-29T13:59:09.573556Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.8.10, pytest 6.2.5, pytest-sugar 0.9.4)\u001b[0m\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, sugar-0.9.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_benchmark_example.py\u001b[0m \u001b[32m✓\u001b[0m                  \u001b[32m1% \u001b[0m\u001b[40m\u001b[32m▏\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_fixture.py\u001b[0m \u001b[32m✓\u001b[0m                            \u001b[32m2% \u001b[0m\u001b[40m\u001b[32m▎\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_parametrize.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m                       \u001b[32m4% \u001b[0m\u001b[40m\u001b[32m▍\u001b[0m\u001b[40m\u001b[32m         \u001b[0m\n",
      " \u001b[36mpytest_sugar_example/\u001b[0mtest_repeat_example.py\u001b[0m \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m23% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▍\u001b[0m\u001b[40m\u001b[32m       \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m42% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▎\u001b[0m\u001b[40m\u001b[32m     \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m62% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▎\u001b[0m\u001b[40m\u001b[32m   \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m \u001b[32m81% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m▏ \u001b[0m\n",
      "                                             \u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m✓\u001b[0m\u001b[32m100% \u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\u001b[40m\u001b[32m█\u001b[0m\n",
      "\n",
      "\u001b[33m---------------------------------------------------- benchmark: 1 tests ---------------------------------------------------\u001b[0m\n",
      "Name (time in ns)          Min         Max      Mean   StdDev    Median     IQR  Outliers  OPS (Mops/s)  Rounds  Iterations\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_concat         \u001b[1m  302.8003\u001b[0m\u001b[1m  3,012.5000\u001b[0m\u001b[1m  328.2844\u001b[0m\u001b[1m  97.9087\u001b[0m\u001b[1m  321.5999\u001b[0m\u001b[1m  8.2495\u001b[0m  866;2220\u001b[1m        3.0461\u001b[0m   90868          20\n",
      "\u001b[33m---------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\n",
      "Results (2.63s):\n",
      "\u001b[32m     104 passed\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_sugar_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768bb279",
   "metadata": {},
   "source": [
    "[Link to pytest-sugar](https://github.com/Teemu/pytest-sugar)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6382f46",
   "metadata": {},
   "source": [
    "### pytest-steps: Share Data Between Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca87dbb",
   "metadata": {},
   "source": [
    "Have you ever wanted to use the result of one test for another test? That is when pytest_steps comes in handy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4810c8b5",
   "metadata": {},
   "source": [
    "![](../img/pytest_steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a25187",
   "metadata": {},
   "source": [
    "In the code below, I use the result of `sum_test` as the input of `average_2_nums`. The argument `steps_data` allows me to share the data between 2 tests. \n",
    "\n",
    "```python\n",
    "from pytest_steps import test_steps\n",
    "\n",
    "\n",
    "def sum(n1, n2):\n",
    "    return n1 + n2\n",
    "\n",
    "\n",
    "def average_2_nums(sum):\n",
    "    return sum / 2\n",
    "\n",
    "\n",
    "def sum_test(steps_data):\n",
    "    res = sum(1, 3)\n",
    "    assert res == 4\n",
    "    steps_data.res = res\n",
    "\n",
    "\n",
    "def perc_difference_test(steps_data):\n",
    "    avg = average_2_nums(steps_data.res)\n",
    "    assert avg == 2\n",
    "\n",
    "\n",
    "@test_steps(sum_test, perc_difference_test)\n",
    "def test_calc_suite(test_step, steps_data):\n",
    "    if test_step == 'sum_test':\n",
    "        sum_test(steps_data)\n",
    "    elif test_step == 'perc_difference_test':\n",
    "        perc_difference_test(steps_data)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8218cfd3",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_steps.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b123df70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-25T14:02:08.595254Z",
     "start_time": "2022-04-25T14:02:07.505819Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.8.10, pytest-7.1.2, pluggy-0.13.1\r\n",
      "rootdir: /Users/khuyen/book/Efficient_Python_tricks_and_tools_for_data_scientists/Chapter5\r\n",
      "plugins: anyio-3.5.0, steps-1.8.0, typeguard-2.12.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r\n",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "test_steps.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                         [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest test_steps.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7aa74",
   "metadata": {},
   "source": [
    "[Link to pytest_steps](https://smarie.github.io/python-pytest-steps/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc33f68",
   "metadata": {},
   "source": [
    "### Pandera: a Python Library to Validate Your Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3906a7",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pandera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdc0b4",
   "metadata": {},
   "source": [
    "The outputs of your pandas DataFrame might not be like what you expected either due to the error in your code or the change in the data format. Using data that is different from what you expected can cause errors or lead to decrease performance.\n",
    "\n",
    "Thus, it is important to validate your data before using it. A good tool to validate pandas DataFrame is pandera. Pandera is easy to read and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c45a2573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:52:57.721905Z",
     "start_time": "2021-09-11T18:52:57.661626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>text_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>text_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>text_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1    col2\n",
       "0   5.0  text_1\n",
       "1   8.0  text_2\n",
       "2  10.0  text_3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"import pandera as pa\\nfrom pandera import check_input\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\\\"col1\\\": [5.0, 8.0, 10.0], \\\"col2\\\": [\\\"text_1\\\", \\\"text_2\\\", \\\"text_3\\\"]})\\nschema = pa.DataFrameSchema(\\n    {\\n        \\\"col1\\\": pa.Column(float, pa.Check(lambda minute: 5 <= minute)),\\n        \\\"col2\\\": pa.Column(str, pa.Check.str_startswith(\\\"text_\\\")),\\n    }\\n)\\nvalidated_df = schema(df)\\nvalidated_df\";\n",
       "                var nbb_formatted_code = \"import pandera as pa\\nfrom pandera import check_input\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\\\"col1\\\": [5.0, 8.0, 10.0], \\\"col2\\\": [\\\"text_1\\\", \\\"text_2\\\", \\\"text_3\\\"]})\\nschema = pa.DataFrameSchema(\\n    {\\n        \\\"col1\\\": pa.Column(float, pa.Check(lambda minute: 5 <= minute)),\\n        \\\"col2\\\": pa.Column(str, pa.Check.str_startswith(\\\"text_\\\")),\\n    }\\n)\\nvalidated_df = schema(df)\\nvalidated_df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandera as pa\n",
    "from pandera import check_input\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"col1\": [5.0, 8.0, 10.0], \"col2\": [\"text_1\", \"text_2\", \"text_3\"]})\n",
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"col1\": pa.Column(float, pa.Check(lambda minute: 5 <= minute)),\n",
    "        \"col2\": pa.Column(str, pa.Check.str_startswith(\"text_\")),\n",
    "    }\n",
    ")\n",
    "validated_df = schema(df)\n",
    "validated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416a669",
   "metadata": {},
   "source": [
    "You can also use the pandera’s decorator check_input to validates input pandas DataFrame before entering the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69badd3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:53:11.865113Z",
     "start_time": "2021-09-11T18:53:11.828443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col1_plus_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>text_1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>text_2</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>text_3</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1    col2  col1_plus_3\n",
       "0   5.0  text_1          8.0\n",
       "1   8.0  text_2         11.0\n",
       "2  10.0  text_3         13.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"@check_input(schema)\\ndef plus_three(df):\\n\\tdf['col1_plus_3'] = df['col1'] + 3\\n\\treturn df \\n\\nplus_three(df)\";\n",
       "                var nbb_formatted_code = \"@check_input(schema)\\ndef plus_three(df):\\n    df[\\\"col1_plus_3\\\"] = df[\\\"col1\\\"] + 3\\n    return df\\n\\n\\nplus_three(df)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@check_input(schema)\n",
    "def plus_three(df):\n",
    "    df[\"col1_plus_3\"] = df[\"col1\"] + 3\n",
    "    return df\n",
    "\n",
    "\n",
    "plus_three(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1e7ae",
   "metadata": {},
   "source": [
    "[Link to Pandera](https://pandera.readthedocs.io/en/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b11c7d",
   "metadata": {},
   "source": [
    "### DeepDiff Find Deep Differences of Python Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63860a6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:17:50.028351Z",
     "start_time": "2021-11-22T13:17:44.520188Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deepdiff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd83ce9",
   "metadata": {},
   "source": [
    "When testing the outputs of your functions, it can be frustrated to see your tests fail because of something you don't care too much about such as: \n",
    "\n",
    "- order of items in a list\n",
    "\n",
    "- different ways to specify the same thing such as abbreviation\n",
    "\n",
    "- exact value up to the last decimal point, etc\n",
    "\n",
    "\n",
    "Is there a way that you can exclude certain parts of the object from the comparison? That is when DeepDiff comes in handy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87f3301f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:18:00.366533Z",
     "start_time": "2021-11-22T13:18:00.121414Z"
    }
   },
   "outputs": [],
   "source": [
    "from deepdiff import DeepDiff "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9481da",
   "metadata": {},
   "source": [
    "DeepDiff can output a meaningful comparison like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e1e7fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:24:27.583499Z",
     "start_time": "2021-11-22T13:24:27.572205Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'values_changed': {\"root['banana'][0]\": {'new_value': 2, 'old_value': 3},\n",
       "  \"root['banana'][1]\": {'new_value': 3, 'old_value': 2}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price1 = {'apple': 2, 'orange': 3, 'banana': [3, 2]}\n",
    "price2 = {'apple': 2, 'orange': 3, 'banana': [2, 3]}\n",
    "\n",
    "DeepDiff(price1, price2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9132debf",
   "metadata": {},
   "source": [
    "With DeepDiff, you also have full control of which characteristics of the Python object DeepDiff should ignore. In the example below, since the order is ignored `[3, 2]` is equivalent to `[2, 3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d47c262",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:24:31.055481Z",
     "start_time": "2021-11-22T13:24:31.045776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignore orders \n",
    "\n",
    "DeepDiff(price1, price2, ignore_order=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3123f38",
   "metadata": {},
   "source": [
    "We can also exclude certain part of our object from the comparison. In the code below, we ignore `ml` and `machine learning` since `ml` is a abbreviation of `machine learning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93d493b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:27:04.545073Z",
     "start_time": "2021-11-22T13:27:04.514993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 25;\n",
       "                var nbb_unformatted_code = \"experience1 = {\\\"machine learning\\\": 2, \\\"python\\\": 3}\\nexperience2 = {\\\"ml\\\": 2, \\\"python\\\": 3}\\n\\nDeepDiff(\\n    experience1,\\n    experience2,\\n    exclude_paths={\\\"root['ml']\\\", \\\"root['machine learning']\\\"},\\n)\";\n",
       "                var nbb_formatted_code = \"experience1 = {\\\"machine learning\\\": 2, \\\"python\\\": 3}\\nexperience2 = {\\\"ml\\\": 2, \\\"python\\\": 3}\\n\\nDeepDiff(\\n    experience1,\\n    experience2,\\n    exclude_paths={\\\"root['ml']\\\", \\\"root['machine learning']\\\"},\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experience1 = {\"machine learning\": 2, \"python\": 3}\n",
    "experience2 = {\"ml\": 2, \"python\": 3}\n",
    "\n",
    "DeepDiff(\n",
    "    experience1,\n",
    "    experience2,\n",
    "    exclude_paths={\"root['ml']\", \"root['machine learning']\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e8ea7",
   "metadata": {},
   "source": [
    "Cmpare 2 numbers up to a specific decimal point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87f96e5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-22T13:28:38.951195Z",
     "start_time": "2021-11-22T13:28:38.932287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 34;\n",
       "                var nbb_unformatted_code = \"num1 = 0.258\\nnum2 = 0.259\\n\\nDeepDiff(num1, num2, significant_digits=2)\";\n",
       "                var nbb_formatted_code = \"num1 = 0.258\\nnum2 = 0.259\\n\\nDeepDiff(num1, num2, significant_digits=2)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num1 = 0.258\n",
    "num2 = 0.259\n",
    "\n",
    "DeepDiff(num1, num2, significant_digits=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8cfa53",
   "metadata": {},
   "source": [
    "[Link to DeepDiff](https://github.com/seperman/deepdiff)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59c4732",
   "metadata": {},
   "source": [
    "### hypothesis: Property-based Testing in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb458fe8",
   "metadata": {
    "tags": [
     "hide-code"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5711c6",
   "metadata": {},
   "source": [
    "If you want to test some properties or assumptions, it can be cumbersome to write a wide range of scenarios. To automatically run your tests against a wide range of scenarios and find edge cases in your code that you would otherwise have missed, use hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d864f27f",
   "metadata": {},
   "source": [
    "In the code below, I test if the addition of two floats is commutative. The test fails when either `x` or `y` is `NaN`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d724e527",
   "metadata": {},
   "source": [
    "```python\n",
    "# test_hypothesis.py \n",
    "\n",
    "from hypothesis import given\n",
    "from hypothesis.strategies import floats\n",
    "\n",
    "\n",
    "\n",
    "@given(floats(), floats())\n",
    "def test_floats_are_commutative(x, y):\n",
    "    assert x + y == y + x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e041cdb0",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ pytest test_hypothesis.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04224a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T14:05:11.335715Z",
     "start_time": "2021-12-15T14:05:10.283884Z"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTest session starts (platform: linux, Python 3.8.10, pytest 6.2.5, pytest-sugar 0.9.4)\u001b[0m\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter5\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0, hypothesis-6.31.6, sugar-0.9.4\n",
      "\u001b[1mcollecting ... \u001b[0m\n",
      "\n",
      "――――――――――――――――――――――――― test_floats_are_commutative ――――――――――――――――――――――――――\n",
      "\n",
      "    \u001b[37m@given\u001b[39;49;00m(floats(), floats())\n",
      ">   \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_floats_are_commutative\u001b[39;49;00m(x, y):\n",
      "\n",
      "\u001b[1m\u001b[31mtest_hypothesis.py\u001b[0m:7: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = 0.0, y = nan\n",
      "\n",
      "    \u001b[37m@given\u001b[39;49;00m(floats(), floats())\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_floats_are_commutative\u001b[39;49;00m(x, y):\n",
      ">       \u001b[94massert\u001b[39;49;00m x + y == y + x\n",
      "\u001b[1m\u001b[31mE       assert (0.0 + nan) == (nan + 0.0)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_hypothesis.py\u001b[0m:8: AssertionError\n",
      "---------------------------------- Hypothesis ----------------------------------\n",
      "Falsifying example: test_floats_are_commutative(\n",
      "    x=0.0, y=nan,  # Saw 1 signaling NaN\n",
      ")\n",
      "\n",
      " \u001b[36m\u001b[0mtest_hypothesis.py\u001b[0m \u001b[31m⨯\u001b[0m                                            \u001b[31m100% \u001b[0m\u001b[40m\u001b[31m█\u001b[0m\u001b[40m\u001b[31m█████████\u001b[0m\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_hypothesis.py::test_floats_are_commutative - assert (0.0 + nan) =...\n",
      "\n",
      "Results (0.38s):\n",
      "\u001b[31m       1 failed\u001b[0m\n",
      "         - \u001b[36m\u001b[0mtest_hypothesis.py\u001b[0m:6 \u001b[31mtest_floats_are_commutative\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_hypothesis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b8929b",
   "metadata": {},
   "source": [
    "Now I can rewrite my code to make it more robust against these edge cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56dd3da",
   "metadata": {},
   "source": [
    "[Link to hypothesis](https://hypothesis.readthedocs.io/en/latest/quickstart.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030f9a2",
   "metadata": {},
   "source": [
    "### Deepchecks: Check Category Mismatch Between Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c8fa6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:33:45.159767Z",
     "start_time": "2022-01-21T15:33:31.410396Z"
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install deepchecks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9c3e8",
   "metadata": {},
   "source": [
    "Sometimes, it is important to know if your test set contains the same categories in the train set. If you want to check the category mismatch between the train and test set, use Deepchecks's `CategoryMismatchTrainTest`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23406d85",
   "metadata": {},
   "source": [
    "In the example below, the result shows that there are 2 new categories in the test set. They are 'd' and 'e'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e214c8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:41:30.697748Z",
     "start_time": "2022-01-21T15:41:30.682598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"from deepchecks.checks.integrity.new_category import CategoryMismatchTrainTest\\nfrom deepchecks.base import Dataset\\nimport pandas as pd\";\n",
       "                var nbb_formatted_code = \"from deepchecks.checks.integrity.new_category import CategoryMismatchTrainTest\\nfrom deepchecks.base import Dataset\\nimport pandas as pd\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.checks.integrity.new_category import CategoryMismatchTrainTest\n",
    "from deepchecks.base import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d7f941c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:41:31.957360Z",
     "start_time": "2022-01-21T15:41:31.947954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"train = pd.DataFrame({'col1': ['a', 'b', 'c']})\\ntest = pd.DataFrame({'col1': ['c', 'd', 'e']})\\n\\ntrain_ds = Dataset(train, cat_features=['col1'])\\ntest_ds = Dataset(test, cat_features=['col1'])\";\n",
       "                var nbb_formatted_code = \"train = pd.DataFrame({\\\"col1\\\": [\\\"a\\\", \\\"b\\\", \\\"c\\\"]})\\ntest = pd.DataFrame({\\\"col1\\\": [\\\"c\\\", \\\"d\\\", \\\"e\\\"]})\\n\\ntrain_ds = Dataset(train, cat_features=[\\\"col1\\\"])\\ntest_ds = Dataset(test, cat_features=[\\\"col1\\\"])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.DataFrame({\"col1\": [\"a\", \"b\", \"c\"]})\n",
    "test = pd.DataFrame({\"col1\": [\"c\", \"d\", \"e\"]})\n",
    "\n",
    "train_ds = Dataset(train, cat_features=[\"col1\"])\n",
    "test_ds = Dataset(test, cat_features=[\"col1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "465d2e2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-21T15:44:25.958451Z",
     "start_time": "2022-01-21T15:44:25.930984Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Category Mismatch Train Test</h4><p>Find new categories in the test set.</p><h5>Additional Outputs</h5><style type=\"text/css\">\n",
       "#T_1a4e2_ table {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ thead {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ tbody {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ th {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "#T_1a4e2_ td {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_1a4e2_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >Number of new categories</th>\n",
       "      <th class=\"col_heading level0 col1\" >Percent of new categories in sample</th>\n",
       "      <th class=\"col_heading level0 col2\" >New categories examples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Column</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_1a4e2_level0_row0\" class=\"row_heading level0 row0\" >col1</th>\n",
       "      <td id=\"T_1a4e2_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_1a4e2_row0_col1\" class=\"data row0 col1\" >66.67%</td>\n",
       "      <td id=\"T_1a4e2_row0_col2\" class=\"data row0 col2\" >['d', 'e']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 22;\n",
       "                var nbb_unformatted_code = \"CategoryMismatchTrainTest().run(train_ds, test_ds)\";\n",
       "                var nbb_formatted_code = \"CategoryMismatchTrainTest().run(train_ds, test_ds)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CategoryMismatchTrainTest().run(train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22584fe0",
   "metadata": {},
   "source": [
    "[Link to Deepchecks](https://docs.deepchecks.com/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
