{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7820a007",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823bd55f",
   "metadata": {},
   "source": [
    "### snoop : Smart Print to Debug Your Python Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7506c43a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install snoop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40b859",
   "metadata": {},
   "source": [
    "If you want to figure out what is happening in your code without adding many print statements, try snoop.\n",
    "\n",
    "To use snoop, simply add the `@snoop` decorator to a function you want to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b76a289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T17:46:24.653813Z",
     "start_time": "2021-09-11T17:46:24.332084Z"
    }
   },
   "outputs": [],
   "source": [
    "import snoop \n",
    "\n",
    "@snoop\n",
    "def factorial(x: int):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    else: \n",
    "        return (x * factorial(x-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14667edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T17:46:29.322992Z",
     "start_time": "2021-09-11T17:46:29.300152Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:46:29.30 >>> Call to factorial in File \"/tmp/ipykernel_72663/20705592.py\", line 4\n",
      "12:46:29.30 ...... x = 2\n",
      "12:46:29.30    4 | def factorial(x: int):\n",
      "12:46:29.30    5 |     if x == 1:\n",
      "12:46:29.30    8 |         return (x * factorial(x-1))\n",
      "    12:46:29.30 >>> Call to factorial in File \"/tmp/ipykernel_72663/20705592.py\", line 4\n",
      "    12:46:29.30 ...... x = 1\n",
      "    12:46:29.30    4 | def factorial(x: int):\n",
      "    12:46:29.31    5 |     if x == 1:\n",
      "    12:46:29.31    6 |         return 1\n",
      "    12:46:29.31 <<< Return value from factorial: 1\n",
      "12:46:29.31    8 |         return (x * factorial(x-1))\n",
      "12:46:29.31 <<< Return value from factorial: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The factorial of 2 is 2\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "print(f'The factorial of {num} is {factorial(num)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477c531",
   "metadata": {},
   "source": [
    "[Link to my article about snoop](https://towardsdatascience.com/3-tools-to-track-and-visualize-the-execution-of-your-python-code-666a153e435e)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c57c914",
   "metadata": {},
   "source": [
    "### pytest benchmark: A Pytest Fixture to Benchmark Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c189b0",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf852a8",
   "metadata": {},
   "source": [
    "If you want to benchmark your code while testing with pytest, try pytest-benchmark. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815482f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:42:17.296160Z",
     "start_time": "2021-09-11T18:42:17.288429Z"
    }
   },
   "source": [
    "To understand how pytest-benchmark works, add the code below to the file named `pytest_benchmark_example.py`. Note that we add `benchmark` to the test function that we want to benchmark. \n",
    "```python\n",
    "# pytest_benchmark_example.py\n",
    "def list_comprehension(len_list=5):\n",
    "    return [i for i in range(len_list)]\n",
    "\n",
    "\n",
    "def test_concat(benchmark):\n",
    "    res = benchmark(list_comprehension)\n",
    "    assert res == [0, 1, 2, 3, 4]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4fa910",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_benchmark_example.py\n",
    "```\n",
    "Now you should see the statistics of the time it takes to excecute the test functions on your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8ae8527",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-12T16:36:26.021951Z",
     "start_time": "2021-09-12T16:36:22.946901Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-0.13.1\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: hydra-core-1.1.1, Faker-8.12.1, benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_benchmark_example.py \u001b[32m.\u001b[0m\u001b[32m                                            [100%]\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[33m----------------------------------------------------- benchmark: 1 tests ----------------------------------------------------\u001b[0m\n",
      "Name (time in ns)          Min         Max      Mean    StdDev    Median     IQR   Outliers  OPS (Mops/s)  Rounds  Iterations\n",
      "\u001b[33m-----------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "test_concat         \u001b[1m  286.4501\u001b[0m\u001b[1m  4,745.5498\u001b[0m\u001b[1m  309.3872\u001b[0m\u001b[1m  106.6583\u001b[0m\u001b[1m  297.5001\u001b[0m\u001b[1m  5.3500\u001b[0m  2686;5843\u001b[1m        3.2322\u001b[0m  162101          20\n",
      "\u001b[33m-----------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      "\n",
      "Legend:\n",
      "  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.\n",
      "  OPS: Operations Per Second, computed as 1 / Mean\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 2.47s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 42;\n",
       "                var nbb_unformatted_code = \"!pytest pytest_benchmark_example.py \";\n",
       "                var nbb_formatted_code = \"!pytest pytest_benchmark_example.py\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pytest pytest_benchmark_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d71ab4",
   "metadata": {},
   "source": [
    "[Link to pytest-benchmark](https://github.com/ionelmc/pytest-benchmark)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5bd092",
   "metadata": {},
   "source": [
    "### pytest.mark.parametrize: Test Your Functions with Multiple Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c2d9ec",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e88ff1",
   "metadata": {},
   "source": [
    "If you want to test your function with different examples, use `pytest.mark.parametrize` decorator.\n",
    "\n",
    "To understand how `pytest.mark.parametrize` works, start with adding the code below to `pytest_parametrize.py`. `@pytest.mark.parametrize` is added to the test function `test_text_contain_word` to tell pytest to experiment with different pairs of inputs and expected outputs specified in its arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ad6ba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:43:09.283883Z",
     "start_time": "2021-09-11T18:43:09.273973Z"
    }
   },
   "source": [
    "```python\n",
    "# pytest_parametrize.py\n",
    "import pytest\n",
    "\n",
    "def text_contain_word(word: str, text: str):\n",
    "    '''Find whether the text contains a particular word'''\n",
    "    \n",
    "    return word in text\n",
    "\n",
    "test = [\n",
    "    ('There is a duck in this text',True),\n",
    "    ('There is nothing here', False)\n",
    "    ]\n",
    "\n",
    "@pytest.mark.parametrize('sample, expected', test)\n",
    "def test_text_contain_word(sample, expected):\n",
    "\n",
    "    word = 'duck'\n",
    "\n",
    "    assert text_contain_word(word, sample) == expected\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4263702d",
   "metadata": {},
   "source": [
    "In the code above, I expect the first sentence to contain the word “duck” and expect the second sentence not to contain that word. Let's see if my expectations are correct by running:\n",
    "```bash\n",
    "$ pytest pytest_parametrize.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "031ac02c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:43:10.924823Z",
     "start_time": "2021-09-11T18:43:10.494719Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\r\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\r\n",
      "rootdir: /home/khuyen/book/book/Chapter4\r\n",
      "plugins: benchmark-3.4.1, anyio-3.3.0\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r\n",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "pytest_parametrize.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                 [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_parametrize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b4f51",
   "metadata": {},
   "source": [
    "Sweet! 2 tests passed when running pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a725d73",
   "metadata": {},
   "source": [
    "[Link to my article about pytest](https://towardsdatascience.com/pytest-for-data-scientists-2990319e55e6?sk=2d3a81903b154db0c7ca832b9f29fee8).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac57677",
   "metadata": {},
   "source": [
    "### Pytest Fixtures: Use The Same Data for Different Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236c745a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5256a1b",
   "metadata": {},
   "source": [
    "Sometimes, you might want to use the same data to test different functions. If you are using pytest, use pytest fixtures to provide data to different test functions instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997f7a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:45:26.995394Z",
     "start_time": "2021-09-11T18:45:26.472717Z"
    }
   },
   "source": [
    "To understand how to use pytest fixtures, start with adding the code below to the file named `pytest_fixture.py`. We use the decorator `@pytest.fixture` to create the data we want to use many times.\n",
    "```python\n",
    "# pytest_fixture.py\n",
    "import pytest \n",
    "from textblob import TextBlob\n",
    "\n",
    "def extract_sentiment(text: str):\n",
    "    \"\"\"Extract sentimetn using textblob. Polarity is within range [-1, 1]\"\"\"\n",
    "    \n",
    "    text = TextBlob(text)\n",
    "    return text.sentiment.polarity\n",
    "\n",
    "@pytest.fixture \n",
    "def example_data():\n",
    "    return 'Today I found a duck and I am happy'\n",
    "\n",
    "def test_extract_sentiment(example_data):\n",
    "    sentiment = extract_sentiment(example_data)\n",
    "    assert sentiment > 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29564978",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "$ pytest pytest_fixture.py\n",
    "```\n",
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79658c9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:45:46.386950Z",
     "start_time": "2021-09-11T18:45:45.299302Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: benchmark-3.4.1, anyio-3.3.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "pytest_fixture.py \u001b[32m.\u001b[0m\u001b[32m                                                      [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.53s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_fixture.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb08f7",
   "metadata": {},
   "source": [
    "### Pytest repeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb8f66f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytest-repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ad279",
   "metadata": {},
   "source": [
    "It is a good practice to test your functions to make sure they work as expected, but sometimes you need to test 100 times until you found the rare cases when the test fails. That is when pytest-repeat comes in handy.\n",
    "\n",
    "To understand how pytest-repeat works, start with putting the code below to the file named `pytest_repeat_example.py`. The `@pytest.mark.repeat(100)` decorator tells pytest to repeat the experiment 100 times. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae78f78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:50:57.169539Z",
     "start_time": "2021-09-11T18:50:57.161819Z"
    }
   },
   "source": [
    "```python\n",
    "# pytest_repeat_example.py\n",
    "import pytest \n",
    "import random \n",
    "\n",
    "def generate_numbers():\n",
    "    return random.randint(1, 100)\n",
    "\n",
    "@pytest.mark.repeat(100)\n",
    "def test_generate_numbers():\n",
    "    assert generate_numbers() > 1 and generate_numbers() < 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518854b9",
   "metadata": {},
   "source": [
    "On your terminal, type:\n",
    "```bash\n",
    "pytest pytest_repeat_example.py\n",
    "```\n",
    "We can see that 100 experiments are executed and passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7291ca8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:51:02.831644Z",
     "start_time": "2021-09-11T18:51:02.331407Z"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.10, pytest-6.2.5, py-1.10.0, pluggy-1.0.0\n",
      "benchmark: 3.4.1 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)\n",
      "rootdir: /home/khuyen/book/book/Chapter4\n",
      "plugins: benchmark-3.4.1, repeat-0.9.1, anyio-3.3.0\n",
      "collected 100 items                                                            \u001b[0m\n",
      "\n",
      "pytest_repeat_example.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m [ 47%]\n",
      "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                    [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================= \u001b[32m\u001b[1m100 passed\u001b[0m\u001b[32m in 0.07s\u001b[0m\u001b[32m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest pytest_repeat_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb66973",
   "metadata": {},
   "source": [
    "[Link to pytest-repeat](https://github.com/pytest-dev/pytest-repeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc33f68",
   "metadata": {},
   "source": [
    "### Pandera: a Python Library to Validate Your Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3906a7",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pandera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdc0b4",
   "metadata": {},
   "source": [
    "The outputs of your pandas DataFrame might not be like what you expected either due to the error in your code or the change in the data format. Using data that is different from what you expected can cause errors or lead to decrease performance.\n",
    "\n",
    "Thus, it is important to validate your data before using it. A good tool to validate pandas DataFrame is pandera. Pandera is easy to read and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c45a2573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:52:57.721905Z",
     "start_time": "2021-09-11T18:52:57.661626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>text_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>text_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>text_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1    col2\n",
       "0   5.0  text_1\n",
       "1   8.0  text_2\n",
       "2  10.0  text_3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 40;\n",
       "                var nbb_unformatted_code = \"import pandera as pa\\nfrom pandera import check_input\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\\\"col1\\\": [5.0, 8.0, 10.0], \\\"col2\\\": [\\\"text_1\\\", \\\"text_2\\\", \\\"text_3\\\"]})\\nschema = pa.DataFrameSchema(\\n    {\\n        \\\"col1\\\": pa.Column(float, pa.Check(lambda minute: 5 <= minute)),\\n        \\\"col2\\\": pa.Column(str, pa.Check.str_startswith(\\\"text_\\\")),\\n    }\\n)\\nvalidated_df = schema(df)\\nvalidated_df\";\n",
       "                var nbb_formatted_code = \"import pandera as pa\\nfrom pandera import check_input\\nimport pandas as pd\\n\\ndf = pd.DataFrame({\\\"col1\\\": [5.0, 8.0, 10.0], \\\"col2\\\": [\\\"text_1\\\", \\\"text_2\\\", \\\"text_3\\\"]})\\nschema = pa.DataFrameSchema(\\n    {\\n        \\\"col1\\\": pa.Column(float, pa.Check(lambda minute: 5 <= minute)),\\n        \\\"col2\\\": pa.Column(str, pa.Check.str_startswith(\\\"text_\\\")),\\n    }\\n)\\nvalidated_df = schema(df)\\nvalidated_df\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandera as pa\n",
    "from pandera import check_input\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"col1\": [5.0, 8.0, 10.0], \"col2\": [\"text_1\", \"text_2\", \"text_3\"]})\n",
    "schema = pa.DataFrameSchema(\n",
    "    {\n",
    "        \"col1\": pa.Column(float, pa.Check(lambda minute: 5 <= minute)),\n",
    "        \"col2\": pa.Column(str, pa.Check.str_startswith(\"text_\")),\n",
    "    }\n",
    ")\n",
    "validated_df = schema(df)\n",
    "validated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f416a669",
   "metadata": {},
   "source": [
    "You can also use the pandera’s decorator check_inputto validates input pandas DataFrame before entering the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69badd3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-11T18:53:11.865113Z",
     "start_time": "2021-09-11T18:53:11.828443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col1_plus_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>text_1</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>text_2</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>text_3</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1    col2  col1_plus_3\n",
       "0   5.0  text_1          8.0\n",
       "1   8.0  text_2         11.0\n",
       "2  10.0  text_3         13.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 41;\n",
       "                var nbb_unformatted_code = \"@check_input(schema)\\ndef plus_three(df):\\n\\tdf['col1_plus_3'] = df['col1'] + 3\\n\\treturn df \\n\\nplus_three(df)\";\n",
       "                var nbb_formatted_code = \"@check_input(schema)\\ndef plus_three(df):\\n    df[\\\"col1_plus_3\\\"] = df[\\\"col1\\\"] + 3\\n    return df\\n\\n\\nplus_three(df)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@check_input(schema)\n",
    "def plus_three(df):\n",
    "    df[\"col1_plus_3\"] = df[\"col1\"] + 3\n",
    "    return df\n",
    "\n",
    "\n",
    "plus_three(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d1e7ae",
   "metadata": {},
   "source": [
    "[Link to Pandera](https://pandera.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
