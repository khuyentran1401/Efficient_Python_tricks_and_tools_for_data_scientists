{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63364389",
   "metadata": {},
   "source": [
    "## Large Language Model (LLM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ae67f3c",
   "metadata": {},
   "source": [
    "### Simplify LLM Integration with Magentic's @prompt Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ccb1a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install magentic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb377c57",
   "metadata": {},
   "source": [
    "To enhance your code's natural language skills with LLM effortlessly, try magentic. \n",
    "\n",
    "With magentic, you can use the `@prompt` decorator to create functions that return organized LLM results, keeping your code neat and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941b061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23398199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yo dude, how's it going?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from magentic import prompt\n",
    "\n",
    "\n",
    "@prompt('Add more \"dude\"ness to: {phrase}')\n",
    "def dudeify(phrase: str) -> str:\n",
    "    ...  # No function body as this is never executed\n",
    "\n",
    "\n",
    "dudeify(\"Hello, how are you?\")\n",
    "# \"Hey, dude! What's up? How's it going, my man?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5abd3e94",
   "metadata": {},
   "source": [
    "The `@prompt` decorator will consider the return type annotation, including those supported by pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7adc676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MilkTea(tea='green tea', sweetness_percentage=100.0, topping='boba')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from magentic import prompt, FunctionCall\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class MilkTea(BaseModel):\n",
    "    tea: str\n",
    "    sweetness_percentage: float\n",
    "    topping: str\n",
    "\n",
    "\n",
    "@prompt(\"Create a milk tea with the following tea {tea}.\")\n",
    "def create_milk_tea(tea: str) -> MilkTea:\n",
    "    ...\n",
    "\n",
    "\n",
    "create_milk_tea(\"green tea\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca84b0ed",
   "metadata": {},
   "source": [
    "The `@prompt` decorator also considers a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f205c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frothing milk to 60 F with texture foamy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def froth_milk(temperature: int, texture: Literal[\"foamy\", \"hot\", \"cold\"]) -> str:\n",
    "    \"\"\"Froth the milk to the desired temperature and texture.\"\"\"\n",
    "    return f\"Frothing milk to {temperature} F with texture {texture}\"\n",
    "\n",
    "\n",
    "@prompt(\n",
    "    \"Prepare the milk for my {coffee_type}\",\n",
    "    functions=[froth_milk],\n",
    ")\n",
    "def configure_coffee(coffee_type: str) -> FunctionCall[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "output = configure_coffee(\"latte!\")\n",
    "output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded189dc",
   "metadata": {},
   "source": [
    "[Link to magentic](https://github.com/jackmpcollins/magentic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087fd8f",
   "metadata": {},
   "source": [
    "### Outlines: Ensuring Consistent Outputs from Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2576146",
   "metadata": {},
   "source": [
    "The Outlines library enables controlling the outputs of language models. This makes the outputs more predictable, ensuring the reliability of systems using large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9a563",
   "metadata": {},
   "source": [
    "```python\n",
    "import outlines\n",
    "\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"\"\"You are a sentiment-labelling assistant.\n",
    "Is the following review positive or negative?\n",
    "\n",
    "Review: This restaurant is just awesome!\n",
    "\"\"\"\n",
    "# Only return a choice between multiple possibilities\n",
    "answer = outlines.generate.choice(model, [\"Positive\", \"Negative\"])(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c1bb6",
   "metadata": {},
   "source": [
    "```python\n",
    "# Only return integers or floats\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"1+1=\"\n",
    "answer = outlines.generate.format(model, int)(prompt)\n",
    "\n",
    "prompt = \"sqrt(2)=\"\n",
    "answer = outlines.generate.format(model, float)(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88991d52",
   "metadata": {},
   "source": [
    "[Link to Outlines](https://github.com/outlines-dev/outlines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff519c06",
   "metadata": {},
   "source": [
    "### Mirascope: Extract Structured Data Extraction From LLM Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12a009",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mirascope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9cc6f",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are powerful at producing human-like text, but their outputs lack structure, which can limit their usefulness in many practical applications that require organized data.\n",
    "\n",
    "Mirascope offers a solution by enabling the extraction of structured information from LLM outputs reliably.\n",
    "\n",
    "The following code uses Mirascope to extract meeting details such as topic, date, time, and participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c7cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f19428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic='Upcoming product launch' date='June 15th' time='3 PM' participants=['John', 'Sarah', 'Mike']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Type\n",
    "from pydantic import BaseModel\n",
    "from mirascope.openai import OpenAIExtractor\n",
    "\n",
    "\n",
    "class MeetingDetails(BaseModel):\n",
    "    topic: str\n",
    "    date: str\n",
    "    time: str\n",
    "    participants: List[str]\n",
    "\n",
    "\n",
    "class MeetingExtractor(OpenAIExtractor[MeetingDetails]):\n",
    "    extract_schema: Type[MeetingDetails] = MeetingDetails\n",
    "    prompt_template = \"\"\"\n",
    "    Extract the meeting details from the following description:\n",
    "    {description}\n",
    "    \"\"\"\n",
    "\n",
    "    description: str\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "description = \"Discuss the upcoming product launch on June 15th at 3 PM with John, Sarah, and Mike.\"\n",
    "meeting_details = MeetingExtractor(description=description).extract()\n",
    "assert isinstance(meeting_details, MeetingDetails)\n",
    "print(meeting_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d2dd5",
   "metadata": {},
   "source": [
    "[Link to Mirascope](https://bit.ly/4bkciv3)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
