{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2d6075",
   "metadata": {},
   "source": [
    "## Feature Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d38a782",
   "metadata": {},
   "source": [
    "![](../img/feature_engineer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab8fbc",
   "metadata": {},
   "source": [
    "This section covers some libraries for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b32f1d3",
   "metadata": {},
   "source": [
    "### Split Data in a Stratified Fashion in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203a3fe",
   "metadata": {},
   "source": [
    "Normally, after using scikit-learn's `train_test_split`, the proportion of values in the sample will be different from the proportion of values in the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349b5bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "np.bincount(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16573eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfa81a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of each class in the train set\n",
    "\n",
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of each class in the test set\n",
    "\n",
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb92ee",
   "metadata": {},
   "source": [
    "If you want to keep the proportion of classes in the sample the same as the proportion of classes in the entire dataset, add `stratify=y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10db899",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d478430",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23da3669",
   "metadata": {},
   "source": [
    "### Avoiding Data Leakage in Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e7827b",
   "metadata": {},
   "source": [
    "In time-sensitive datasets, a random split can cause data leakage by including future data in the training set, which biases the model. To prevent this, split data chronologically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85bad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Sample data\n",
    "data = {'customer_id': [1, 2, 3, 4, 5],\n",
    "        'amount': [10.00, 20.00, 15.00, 25.00, 30.00],\n",
    "        'date': ['2021-01-01', '2021-01-02', '2021-01-03', '2021-01-04', '2021-01-05']}\n",
    "df = pd.DataFrame(data)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Random split\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=42)\n",
    "print(\"Random split:\\n\")\n",
    "print(\"Train data:\\n\", train_data)  # May contain future dates\n",
    "print(\"Test data:\\n\", test_data)\n",
    "\n",
    "# Time-based split\n",
    "cutoff_date = datetime(2021, 1, 4)\n",
    "train_data = df[df['date'] < cutoff_date]\n",
    "test_data = df[df['date'] >= cutoff_date]\n",
    "print(\"\\n\\nTime-based split:\\n\")\n",
    "print(\"Train data:\\n\", train_data)  # Data before the cutoff\n",
    "print(\"Test data:\\n\", test_data)   # Data after the cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076db038",
   "metadata": {},
   "source": [
    "### TimeSeriesSplit for Cross-Validation in Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1952758",
   "metadata": {},
   "source": [
    "For time series data, using `TimeSeriesSplit` ensures the temporal order is maintained during cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8446126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(tscv.split(X)):\n",
    "    print(f\"Fold {i}: Train={train_index}, Test={test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e30689",
   "metadata": {},
   "source": [
    "This approach ensures:\n",
    "\n",
    "1.  **Temporal Integrity**: Respects the data order.\n",
    "2.  **Growing Training Set**: The training set increases with each fold.\n",
    "3.  **Forward-Moving Test Set**: The test set is always a future sample.\n",
    "4.  **No Data Leakage**: Future information is never used to predict past events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f49de5",
   "metadata": {},
   "source": [
    "### Enhancing Data Handling with scikit-learn's DataFrame Support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ca9c4",
   "metadata": {},
   "source": [
    "By default, scikit-learn transformers return NumPy arrays. To return pandas DataFrames, use the `set_output` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7020495b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = {\"age\": [25, 30, None, 35], \"income\": [50000, 60000, 70000, None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "scaler = StandardScaler().set_output(transform='pandas')\n",
    "print(scaler.fit_transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc293d3",
   "metadata": {},
   "source": [
    "You can apply this in pipelines too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325de2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([(\"imputer\", SimpleImputer(strategy=\"mean\")), \n",
    "                     (\"scaler\", StandardScaler())]).set_output(transform=\"pandas\")\n",
    "\n",
    "print(pipeline.fit_transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8934f5",
   "metadata": {},
   "source": [
    "### Efficient Feature Transformation with make_column_transformer in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c05ebe9",
   "metadata": {},
   "source": [
    "`make_column_transformer` allows you to apply different transformations to specific feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285de8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    \"cat1\": [\"A\", \"B\", \"A\", np.nan, \"C\"],\n",
    "    \"cat2\": [\"X\", \"Y\", np.nan, \"X\", \"Z\"],\n",
    "    \"num1\": [10, np.nan, 15, 25, 30],\n",
    "    \"num2\": [1.5, 2.0, np.nan, 2.2, 1.9],\n",
    "}\n",
    "\n",
    "X = pd.DataFrame(data)\n",
    "y = pd.Series([0, 1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0451ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Define the numeric and categorical features\n",
    "numeric_features = [\"num1\", \"num2\"]\n",
    "categorical_features = [\"cat1\", \"cat2\"]\n",
    "\n",
    "# Define the transformers and their corresponding columns\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(sparse_output=False)\n",
    ")\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    "    verbose_feature_names_out=False,\n",
    ").set_output(transform=\"pandas\")\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ed7b5",
   "metadata": {},
   "source": [
    "You can integrate this into a pipeline with a machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d850cb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(preprocessor, LogisticRegression())\n",
    "pipe.fit(X, y)\n",
    "pipe.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b9343f",
   "metadata": {},
   "source": [
    "This streamlines feature preprocessing and modeling in one unified workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fd5e0e",
   "metadata": {},
   "source": [
    "### FunctionTransformer: Build Robust Preprocessing Pipelines with Custom Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b763beb9",
   "metadata": {},
   "source": [
    "If you need to apply custom transformations within a scikit-learn pipeline, the `FunctionTransformer` is a useful tool to wrap any function for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7976edf5",
   "metadata": {},
   "source": [
    "This allows you to seamlessly integrate custom functions into a pipeline, maintaining consistency across transformations for different datasets.\n",
    "\n",
    "Here's an example of using `FunctionTransformer` in a full pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2244f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple pandas DataFrame\n",
    "data = {\n",
    "    \"feature1\": [1, 2, 3, 4, 5],\n",
    "    \"feature2\": [6, 7, 8, 9, 10],\n",
    "    \"target\": [0, 0, 1, 1, 1],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the DataFrame into features and target\n",
    "X = df[[\"feature1\", \"feature2\"]]\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Define the FunctionTransformer\n",
    "log_transformer = FunctionTransformer(np.log1p)\n",
    "\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(\n",
    "    [(\"log_transform\", log_transformer), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "# Fit the pipeline on the data\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# Make predictions on new data\n",
    "new_data = {\"feature1\": [6, 7], \"feature2\": [11, 12]}\n",
    "new_df = pd.DataFrame(new_data)\n",
    "predictions = pipeline.predict(new_df)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f778ed2b",
   "metadata": {},
   "source": [
    "### Drop Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197d6f10",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install feature_engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54ac43",
   "metadata": {},
   "source": [
    "To remove highly correlated features, you can use `DropCorrelatedFeatures` from the `feature-engine` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c10de4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from feature_engine.selection import DropCorrelatedFeatures\n",
    "\n",
    "# Create a dataset with correlated variables\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=6, n_redundant=3, n_clusters_per_class=1, class_sep=2, random_state=0\n",
    ")\n",
    "\n",
    "# Convert the dataset to a DataFrame\n",
    "colnames = [\"var_\" + str(i) for i in range(6)]\n",
    "X = pd.DataFrame(X, columns=colnames)\n",
    "\n",
    "# Drop features with correlation above 0.8\n",
    "tr = DropCorrelatedFeatures(threshold=0.8)\n",
    "Xt = tr.fit_transform(X)\n",
    "\n",
    "# Check correlated features\n",
    "print(\"Correlated features:\", tr.correlated_feature_sets_)\n",
    "print(\"Remaining features:\", Xt.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945f910",
   "metadata": {},
   "source": [
    "[Link to feature-engine](https://feature-engine.trainindata.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73cf7a",
   "metadata": {},
   "source": [
    "### Encode Rare Labels with Feature-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ab4f75",
   "metadata": {},
   "source": [
    "Handling rare categories in high-cardinality categorical features can be simplified using the `RareLabelEncoder`. This encoder groups infrequent categories into a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb2769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from feature_engine.encoding import RareLabelEncoder\n",
    "\n",
    "data = fetch_openml('dating_profile')['data']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b3c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in 'education' column\n",
    "\n",
    "processed = data.dropna(subset=['education'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df4cb6",
   "metadata": {},
   "source": [
    "In the code below, \n",
    "- `tol` species the minimum frequency below which a category is considered rare. \n",
    "- `replace_with` species the value to be used to replace rare categories.\n",
    "- `variables` specify the list of categorical variables that will be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9643d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = RareLabelEncoder(tol=0.05, variables=[\"education\"], replace_with=\"Other\")\n",
    "encoded = encoder.fit_transform(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc96e0",
   "metadata": {},
   "source": [
    "Now the rare categories in the column `education` are replaced with \"Other\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e0f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded['education'].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb1582",
   "metadata": {},
   "source": [
    "[Link to feature-engine](https://feature-engine.trainindata.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e447cd",
   "metadata": {},
   "source": [
    "### Encode Categorical Data Using Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae79df",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install feature-engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea70c0",
   "metadata": {},
   "source": [
    "Sometimes, encoding categorical variables based on frequency or count can improve model performance. `CountFrequencyEncoder` from `feature-engine` helps achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee352a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = sns.load_dataset(\"diamonds\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data[\"price\"], random_state=0)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98690434",
   "metadata": {},
   "source": [
    "Encode `color` and `clarity`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdbf258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate an encoder\n",
    "encoder = CountFrequencyEncoder(\n",
    "    encoding_method=\"frequency\", variables=[\"color\", \"clarity\"]\n",
    ")\n",
    "\n",
    "# fit the encoder\n",
    "encoder.fit(X_train)\n",
    "\n",
    "# process the data\n",
    "p_train = encoder.transform(X_train)\n",
    "p_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f9ad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a114a124",
   "metadata": {},
   "source": [
    "[Link to feature-engine](https://feature-engine.trainindata.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179b4be",
   "metadata": {},
   "source": [
    "### Similarity Encoding for Dirty Categories Using dirty_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc69ef",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install dirty-cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ac820e",
   "metadata": {},
   "source": [
    "To handle dirty categorical variables, use dirty\\_cat's `SimilarityEncoder`. This captures similarities between categories that may contain typos or variations.\n",
    "\n",
    "Example using the employee_salaries dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1105e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dirty_cat.datasets import fetch_employee_salaries\n",
    "from dirty_cat import SimilarityEncoder\n",
    "\n",
    "X = fetch_employee_salaries().X\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52078af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_column = \"employee_position_title\"\n",
    "X_dirty = df[dirty_column].values\n",
    "X_dirty[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23382228",
   "metadata": {},
   "source": [
    "We can see that titles such as 'Master Police Officer' and 'Police Officer III' are similar. We can use `SimilaryEncoder` to get an array that encodes the similarity between different job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef01123",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = SimilarityEncoder(similarity=\"ngram\")\n",
    "X_enc = enc.fit_transform(X_dirty[:10].reshape(-1, 1))\n",
    "X_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67348ac4",
   "metadata": {},
   "source": [
    "To better visualize the similarity, create a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "def plot_similarity(labels, features):\n",
    "  \n",
    "    normalized_features = normalize(features)\n",
    "    \n",
    "    # Create correction matrix\n",
    "    corr = np.inner(normalized_features, normalized_features)\n",
    "    \n",
    "    # Plot\n",
    "    figsize(10, 10)\n",
    "    sns.set(font_scale=1.2)\n",
    "    g = sns.heatmap(corr, xticklabels=labels, yticklabels=labels, vmin=0,\n",
    "        vmax=1, cmap=\"YlOrRd\", annot=True, annot_kws={\"size\": 10})\n",
    "        \n",
    "    g.set_xticklabels(labels, rotation=90)\n",
    "    g.set_title(\"Similarity\")\n",
    "\n",
    "\n",
    "def encode_and_plot(labels):\n",
    "  \n",
    "    enc = SimilarityEncoder(similarity=\"ngram\") # Encode\n",
    "    X_enc = enc.fit_transform(labels.reshape(-1, 1))\n",
    "    \n",
    "    plot_similarity(labels, X_enc) # Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ac60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_and_plot(X_dirty[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4676539",
   "metadata": {},
   "source": [
    "As we can see from the matrix above,\n",
    "- The similarity between the same strings such as 'Office Services Coordinator' and 'Office Services Coordinator' is 1\n",
    "- The similarity between somewhat similar strings such as 'Office Services Coordinator' and 'Master Police Officer' is 0.41\n",
    "- The similarity between two very different strings such as 'Social Worker IV' and 'Polic Aide' is 0.028"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e264ab",
   "metadata": {},
   "source": [
    "[Link to dirty-cat](https://dirty-cat.github.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ced6f",
   "metadata": {},
   "source": [
    "### How to Handle Misspellings in Real-World Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ced63",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/skrub-data/skrub.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc5297",
   "metadata": {},
   "source": [
    "Real-world datasets often contain misspellings and variations in categorical variables, especially when data is manually entered. This can cause issues with data analysis steps that require exact matching, such as GROUP BY operations.\n",
    "\n",
    "skrub's `deduplicate()` function helps solve this problem by using unsupervised learning to cluster similar strings and automatically correct misspellings. \n",
    "\n",
    "To demonstrate the deduplicate function, start with generating a duplicated dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub.datasets import make_deduplication_data\n",
    "import pandas as pd\n",
    "\n",
    "duplicated_food = make_deduplication_data(\n",
    "    examples=[\"Chocolate\", \"Broccoli\", 'Jalapeno', 'Zucchini'],  \n",
    "    entries_per_example=[100, 200, 300, 200],  # their respective number of occurrences\n",
    "    prob_mistake_per_letter=0.05,  # 5% probability of typo per letter\n",
    "    random_state=42,  # set seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95452afb",
   "metadata": {},
   "source": [
    "Get the most common food names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a605df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(duplicated_food).value_counts()[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31228041",
   "metadata": {},
   "source": [
    "The most common words in the dataset are 'Jalapeno', 'Zucchini', and 'Broccoli'. Therefore, skub's deduplicate function replaces misspelled words with the closest matching word from this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c068520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import deduplicate\n",
    "\n",
    "# create a table that maps original to corrected categories\n",
    "translation_table = deduplicate(duplicated_food)\n",
    "\n",
    "# remove duplicated rows\n",
    "translation_table = translation_table.reset_index().rename(columns={'index': 'mispelled_food', 0: 'corrected_food'}).drop_duplicates()\n",
    "\n",
    "# view 10 sample rows\n",
    "print(translation_table.sample(10, random_state=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b366c",
   "metadata": {},
   "source": [
    "[Link to skub](https://github.com/skrub-data/skrub/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ddd31",
   "metadata": {},
   "source": [
    "### Solving Data Mismatches: Joining Tables with Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029524f8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/skrub-data/skrub.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fafa1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# We will ignore the warnings:\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e8999",
   "metadata": {},
   "source": [
    "When joining two tables with non-exact matching entries, such as \"Yemen\\*\" in one table and \"Yemen, Rep.\" in another, use skrub's `fuzzy_join()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0d44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/skrub-data/datasets/master/data/Happiness_report_2022.csv\",\n",
    "    thousands=\",\",\n",
    ")\n",
    "df = df[[\"Country\", \"Happiness score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb7e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub.datasets import fetch_world_bank_indicator\n",
    "gdppc = fetch_world_bank_indicator(indicator_id=\"NY.GDP.PCAP.CD\").X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.sort_values(by=\"Country\").tail(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104f7cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdppc.sort_values(by=\"Country Name\").tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da91e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skrub import fuzzy_join\n",
    "\n",
    "df1 = fuzzy_join(\n",
    "    df,  # our table to join\n",
    "    gdppc,  # the table to join with\n",
    "    left_on=\"Country\",  # the first join key column\n",
    "    right_on=\"Country Name\",  # the second join key column\n",
    "    return_score=True,\n",
    ")\n",
    "\n",
    "print(df1[['Country', 'Country Name', \"matching_score\"]].tail(20))\n",
    "# We merged the first WB table to our initial one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9068b1",
   "metadata": {},
   "source": [
    "[Link to skrub](https://github.com/skrub-data/skrub/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704142a5",
   "metadata": {},
   "source": [
    "### Snorkel — Programmatically Build Training Data in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fd8a9b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install snorkel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee557ad",
   "metadata": {},
   "source": [
    "Let’s say you want to detect fake job postings based on a few assumptions:\n",
    "\n",
    "*   Few or no requirements make a job likely fake.\n",
    "*   No company profile or logo is a red flag.\n",
    "*   Real jobs usually require education or experience.\n",
    "\n",
    "You can load a sample dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "train_df = pd.read_pickle(\n",
    "    \"https://github.com/khuyentran1401/Data-science/blob/master/feature_engineering/snorkel_example/train_fake_jobs.pkl?raw=true\"\n",
    ")\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc1ccb",
   "metadata": {},
   "source": [
    "Snorkel helps programmatically label data. Start by defining labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b78a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function, PandasLFApplier, LFAnalysis\n",
    "\n",
    "FAKE = 1\n",
    "REAL = 0\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b21d0",
   "metadata": {},
   "source": [
    "You can create labeling functions to reflect your assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34747800",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def no_company_profile(x: pd.Series):\n",
    "    return FAKE if x.company_profile == \"\" else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def no_company_logo(x: pd.Series):\n",
    "    return FAKE if x.has_company_logo == 0 else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def required_experience(x: pd.Series):\n",
    "    return REAL if x.required_experience else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function()\n",
    "def required_education(x: pd.Series):\n",
    "    return REAL if x.required_education else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114c330",
   "metadata": {},
   "source": [
    "Use these functions to label your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d9a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [\n",
    "    no_company_profile,\n",
    "    no_company_logo,\n",
    "    required_experience,\n",
    "    required_education,\n",
    "]\n",
    "\n",
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd96e93",
   "metadata": {},
   "source": [
    "Finally, evaluate the accuracy of your labeling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b5ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "LFAnalysis(L=L_train, lfs=lfs).lf_summary(Y=train_df.fraudulent.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64f6c3",
   "metadata": {},
   "source": [
    "This analysis provides insights on coverage, accuracy, and overlaps between labeling functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe9effe",
   "metadata": {},
   "source": [
    "[Link to Snorkel](https://www.snorkel.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26864e1",
   "metadata": {},
   "source": [
    "### sketch: AI Code-Writing Assistant That Understands Data Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f98756a",
   "metadata": {},
   "source": [
    "Wouldn't it be nice if you could get insights into your data by simply asking a question? Sketch allows you to do exactly that.\n",
    "\n",
    "Sketch is an AI code-writing assistant for pandas users that understands the context of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17828fcd",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b787c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "import sketch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset('taxis')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21650a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.ask(\n",
    "    \"Can you give me friendly names for each column?\" \n",
    "    \"(Output as an HTML list)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.ask(\n",
    "    \"Which payment is the most popular payment?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.howto(\"Create some features from the pickup column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0d26e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for the hour of the pickup\n",
    "data['pickup_hour'] = data['pickup'].dt.hour\n",
    "\n",
    "# Create a new column for the day of the week of the pickup\n",
    "data['pickup_day'] = data['pickup'].dt.weekday\n",
    "\n",
    "# Create a new column for the month of the pickup\n",
    "data['pickup_month'] = data['pickup'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb5d54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sketch.howto(\n",
    "    \"Create some features from the pickup_zone column\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9cfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called 'pickup_zone_count'\n",
    "data['pickup_zone_count'] = data.groupby('pickup_zone')['pickup_zone'].transform('count')\n",
    "\n",
    "# Create a new column called 'pickup_zone_fare'\n",
    "data['pickup_zone_fare'] = data.groupby('pickup_zone')['fare'].transform('mean')\n",
    "\n",
    "# Create a new column called 'pickup_zone_distance'\n",
    "data['pickup_zone_distance'] = data.groupby('pickup_zone')['distance'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebd00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5eca69",
   "metadata": {},
   "source": [
    "[Link to Sketch](https://github.com/approximatelabs/sketch)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   20,
   24,
   28,
   32,
   41,
   45,
   51,
   55,
   59,
   63,
   67,
   69,
   73,
   77,
   103,
   107,
   111,
   122,
   131,
   135,
   139,
   148,
   152,
   160,
   164,
   168,
   183,
   211,
   215,
   219,
   223,
   227,
   231,
   238,
   244,
   282,
   286,
   290,
   294,
   315,
   319,
   323,
   327,
   335,
   339,
   346,
   349,
   353,
   355,
   359,
   363,
   367,
   371,
   380,
   384,
   398,
   400,
   404,
   408,
   412,
   418,
   426,
   430,
   434,
   438,
   442,
   473,
   475,
   482,
   486,
   490,
   494,
   502,
   512,
   516,
   520,
   522,
   526,
   537,
   541,
   545,
   551,
   558,
   562,
   572,
   577,
   581,
   585,
   598,
   602,
   606,
   610,
   620,
   630,
   634,
   640,
   644,
   663,
   667,
   677,
   681,
   683,
   687,
   691,
   695,
   701,
   707,
   713,
   718,
   725,
   731,
   735,
   747,
   753,
   765,
   767
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}