{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b7d7d95",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ad6df",
   "metadata": {},
   "source": [
    "![](../img/machine_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ba721",
   "metadata": {},
   "source": [
    "### Scikit-LLM: Supercharge Text Analysis with ChatGPT and scikit-learn Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af418b94",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install scikit-llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cdc44",
   "metadata": {},
   "source": [
    "To integrate advanced language models with scikit-learn for enhanced text analysis tasks, use Scikit-LLM.\n",
    "\n",
    "Scikit-LLM's `ZeroShotGPTClassifier` enables text classification on unseen classes without requiring re-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bbc4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skllm.config import SKLLMConfig\n",
    "\n",
    "SKLLMConfig.set_openai_key(\"<YOUR_KEY>\")\n",
    "SKLLMConfig.set_openai_org(\"<YOUR_ORGANISATION>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774a0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skllm.datasets import get_classification_dataset\n",
    "from skllm import ZeroShotGPTClassifier\n",
    "\n",
    "# demo sentiment analysis dataset\n",
    "# labels: positive, negative, neutral\n",
    "X, y = get_classification_dataset()\n",
    "\n",
    "clf = ZeroShotGPTClassifier(openai_model=\"gpt-3.5-turbo\")\n",
    "clf.fit(X, y)\n",
    "labels = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0982ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"Accuracy: {accuracy_score(y, labels):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375faa8a",
   "metadata": {},
   "source": [
    "[Link to Scikit-LLM](https://github.com/iryna-kondr/scikit-llm)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a28a0e",
   "metadata": {},
   "source": [
    "### Create a Readable Machine Learning Pipeline in One Line of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8b66a8",
   "metadata": {},
   "source": [
    "If you want to create a readable machine learning pipeline in a single line of code, try the `make_pipeline` function in scikit-learn.\n",
    "`make_pipeline` is especially useful when working with complex pipelines that involve many different transformers and estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef46588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X, y = make_classification(random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Create a pipeline that scales the data and fits a logistic regression model\n",
    "pipeline = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline on the test data\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14242cc7",
   "metadata": {},
   "source": [
    "### Pipeline + GridSearchCV: Prevent Data Leakage when Scaling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bad5b9",
   "metadata": {},
   "source": [
    "Scaling the data before using GridSearchCV can lead to data leakage since the scaling tells some information about the entire data. To prevent this, assemble both the scaler and machine learning models in a pipeline then use it as the estimator for GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# load data\n",
    "df = load_iris()\n",
    "X = df.data\n",
    "y = df.target\n",
    "\n",
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Create a pipeline variable\n",
    "make_pipe = make_pipeline(StandardScaler(), SVC())\n",
    "\n",
    "# Defining parameters grid\n",
    "grid_params = {\"svc__C\": [0.1, 1, 10, 100, 1000], \"svc__gamma\": [0.1, 1, 10, 100]}\n",
    "\n",
    "# hypertuning\n",
    "grid = GridSearchCV(make_pipe, grid_params, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c66e06",
   "metadata": {},
   "source": [
    "The estimator is now the entire pipeline instead of just the machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f751f3ac",
   "metadata": {},
   "source": [
    "### squared=False: Get RMSE from Sklearn’s mean_squared_error method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18235e82",
   "metadata": {},
   "source": [
    "To obtain the root mean squared error (RMSE) using scikit-learn, use the `squared=False` parameter in the `mean_squared_error` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_actual = [1, 2, 3]\n",
    "y_predicted = [1.5, 2.5, 3.5]\n",
    "rmse = mean_squared_error(y_actual, y_predicted, squared=False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa211b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### modelkit: Build Production ML Systems in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54662c1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install modelkit textblob "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e35b3ad",
   "metadata": {},
   "source": [
    "If you want your ML models to be fast, type-safe, testable, and fast to deploy to production, try modelkit. modelkit allows you to incorporate all of these features into your model in several lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee3e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelkit import ModelLibrary, Model \n",
    "from textblob import TextBlob, WordList\n",
    "# import nltk\n",
    "# nltk.download('brown')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1333af8a",
   "metadata": {},
   "source": [
    "To define a modelkit Model, you need to:\n",
    "\n",
    "- create class inheriting from `modelkit.Model`\n",
    "- implement a `_predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e12f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NounPhraseExtractor(Model):\n",
    "\n",
    "    # Give model a name\n",
    "    CONFIGURATIONS = {\"noun_phrase_extractor\": {}}\n",
    "\n",
    "    def _predict(self, text):\n",
    "        blob = TextBlob(text)\n",
    "        return blob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594765f8",
   "metadata": {},
   "source": [
    "You can now instantiate and use the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a81c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_extractor = NounPhraseExtractor()\n",
    "noun_extractor(\"What are your learning strategies?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09ea50",
   "metadata": {},
   "source": [
    "You can also create test cases for your model and make sure all test cases are passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NounPhraseExtractor(Model):\n",
    "\n",
    "    # Give model a name\n",
    "    CONFIGURATIONS = {\"noun_phrase_extractor\": {}}\n",
    "\n",
    "    TEST_CASES = [\n",
    "        {\"item\": \"There is a red apple on the tree\", \"result\": WordList([\"red apple\"])}\n",
    "    ]\n",
    "\n",
    "    def _predict(self, text):\n",
    "        blob = TextBlob(text)\n",
    "        return blob.noun_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83282fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_extractor = NounPhraseExtractor()\n",
    "noun_extractor.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbecf027",
   "metadata": {},
   "source": [
    "modelkit also allows you to organize a group of models using `ModelLibrary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e1aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(Model):\n",
    "    \n",
    "    # Give model a name\n",
    "    CONFIGURATIONS = {\"sentiment_analyzer\": {}}\n",
    "\n",
    "    def _predict(self, text):\n",
    "        blob = TextBlob(text)\n",
    "        return blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550536d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_models = ModelLibrary(models=[NounPhraseExtractor, SentimentAnalyzer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0404f5",
   "metadata": {},
   "source": [
    "Get and use the models from `nlp_models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_extractor = model_collections.get(\"noun_phrase_extractor\")\n",
    "noun_extractor(\"What are your learning strategies?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e45e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = model_collections.get(\"sentiment_analyzer\")\n",
    "sentiment_analyzer(\"Today is a beautiful day!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2881c14a",
   "metadata": {},
   "source": [
    "[Link to modelkit](https://github.com/Cornerstone-OnDemand/modelkit/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dde48",
   "metadata": {},
   "source": [
    "### Decomposing High-Dimensional Data into Two or Three Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dab35c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7d311",
   "metadata": {},
   "source": [
    "If you want to decompose high dimensional data into two or three dimensions to visualize it, what should you do? A common technique is PCA. Even though PCA is useful, it can be complicated to create a PCA plot.\n",
    "\n",
    "Lucikily, Yellowbrick allows you visualize PCA in a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dba6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets import load_credit\n",
    "from yellowbrick.features import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7889bfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_credit()\n",
    "classes = [\"account in defaut\", \"current with bills\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc36024",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = PCA(scale=True, classes=classes)\n",
    "visualizer.fit_transform(X, y)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d4a886",
   "metadata": {},
   "source": [
    "[Link to Yellowbrick](https://www.scikit-yb.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b58b73",
   "metadata": {},
   "source": [
    "### Visualize Feature Importances with Yellowbrick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0e731a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df524d01",
   "metadata": {},
   "source": [
    "More features don't always mean a better model. Too many features can lead to overfitting. To improve your model, you might need to identify and eliminate less important features. Yellowbrick's `FeatureImportances` can help with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd76c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from yellowbrick.datasets import load_occupancy\n",
    "from yellowbrick.model_selection import FeatureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8765dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_occupancy()\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "viz = FeatureImportances(model)\n",
    "viz.fit(X, y)\n",
    "viz.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3c8f1a",
   "metadata": {},
   "source": [
    "From the plot above, it seems like the light is the most important feature to DecisionTreeClassifier, followed by CO2, temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a758ae",
   "metadata": {},
   "source": [
    "[Link to Yellowbrick](https://www.scikit-yb.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb713f",
   "metadata": {},
   "source": [
    "### Validation Curve: Determine if an Estimator Is Underfitting Over Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5636eb",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install yellowbrick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769eade6",
   "metadata": {},
   "source": [
    "To determine if your model is overfitting or underfitting, use Yellowbrick's validation curve. This technique helps find the optimal hyperparameter value where the model performs well on both training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd3a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.datasets.loaders import load_occupancy\n",
    "from yellowbrick.model_selection import validation_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34baad66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y = load_occupancy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706be0f",
   "metadata": {},
   "source": [
    "In the code below, we choose the range of `max_depth` to be from 1 to 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = validation_curve(\n",
    "    DecisionTreeClassifier(), X, y, param_name=\"max_depth\",\n",
    "    param_range=np.arange(1, 11), cv=10, scoring=\"f1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d26ba",
   "metadata": {},
   "source": [
    "As we can see from the plot above, although `max_depth` > 2 has a higher training score but a lower cross-validation score. This indicates that the model is overfitting. \n",
    "\n",
    "Thus, the sweet spot will be where the cross-validation score neither increases nor decreases, which is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477dd83",
   "metadata": {},
   "source": [
    "[Link to Yellowbrick](https://www.scikit-yb.org/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b4f4c",
   "metadata": {},
   "source": [
    "### Mlxtend: Plot Decision Regions of Your ML Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0102b8a3",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mlxtend  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4209e7f9",
   "metadata": {},
   "source": [
    "To understand how your classifier makes decisions, plotting decision regions can be insightful. Mlxtend's `plot_decision_regions` function makes this easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d783b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import itertools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Initializing Classifiers\n",
    "clf1 = LogisticRegression(random_state=0)\n",
    "clf2 = RandomForestClassifier(random_state=0)\n",
    "clf3 = SVC(random_state=0, probability=True)\n",
    "eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[2, 1, 1], voting='soft')\n",
    "\n",
    "# Loading some example data\n",
    "X, y = iris_data()\n",
    "X = X[:,[0, 2]]\n",
    "\n",
    "# Plotting Decision Regions\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "for clf, lab, grd in zip([clf1, clf2, clf3, eclf],\n",
    "                         ['Logistic Regression', 'Random Forest', 'RBF kernel SVM', 'Ensemble'],\n",
    "                         itertools.product([0, 1], repeat=2)):\n",
    "    clf.fit(X, y)\n",
    "    ax = plt.subplot(gs[grd[0], grd[1]])\n",
    "    fig = plot_decision_regions(X=X, y=y, clf=clf, legend=2)\n",
    "    plt.title(lab)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e2c3d",
   "metadata": {},
   "source": [
    "Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks. \n",
    "\n",
    "Find other useful functionalities of Mlxtend [here](https://github.com/rasbt/mlxtend)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff5954",
   "metadata": {},
   "source": [
    "### imbalanced-learn: Deal with an Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e37fe",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install imbalanced-learn==0.10.0 mlxtend==0.21.0 scikit-learn==1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072afa4",
   "metadata": {},
   "source": [
    "In machine learning, imbalanced datasets can lead to biased models that perform poorly on minority classes. This is particularly problematic in critical applications like fraud detection or disease diagnosis.\n",
    "\n",
    "With imbalanced-learn, you can rebalance your dataset using various sampling techniques that work seamlessly with scikit-learn.\n",
    "\n",
    "To demonstrate this, let's generate a sample dataset with 5000 samples, 2 features, and 4 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9353be52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4b5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Libraries for machine learning\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa9437",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "    n_samples=5000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.01, 0.04, 0.5, 0.90],\n",
    "    class_sep=0.8,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bfbb65",
   "metadata": {},
   "source": [
    "Resample the dataset using the `RandomOverSampler` class from imbalanced-learn to balance the class distribution. This technique works by duplicating minority samples until they match the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641bed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac6166",
   "metadata": {},
   "source": [
    "Plot the decision regions of the dataset before and after resampling using a LinearSVC classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Decision Regions\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1, sharey=True, figsize=(6, 10))\n",
    "\n",
    "for Xi, yi, ax, title in zip(\n",
    "    [X, X_resampled],\n",
    "    [y, y_resampled],\n",
    "    [ax0, ax1],\n",
    "    [\"Without resampling\", \"Using RandomOverSampler\"],\n",
    "):\n",
    "    clf = LinearSVC()\n",
    "    clf.fit(Xi, yi)\n",
    "    fig = plot_decision_regions(X=Xi, y=yi, clf=clf, legend=2, ax=ax, colors='#E583B6,#72FCDB,#72BEFA,#FFFF99')\n",
    "    plt.title(title)\n",
    "    ax.set_title(title, color='#000000')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811c1a9a",
   "metadata": {},
   "source": [
    "The plot reveals that the resampling process has added more data points to the minority class (green), effectively balancing the class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a1baf1",
   "metadata": {},
   "source": [
    "[Link to imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6bf756",
   "metadata": {},
   "source": [
    "### Estimate Prediction Intervals in Scikit-Learn Models with MAPIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e71faed",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mapie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25e2e8e",
   "metadata": {},
   "source": [
    "To estimate prediction intervals for scikit-learn models, use the MAPIE library:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456f2f61",
   "metadata": {},
   "source": [
    "In the code below, we use `MapieRegressor` to estimate prediction intervals for a scikit-learn regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847d296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapie.regression import MapieRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Create data\n",
    "X, y = make_regression(n_samples=200, n_features=1, noise=50, random_state=0)\n",
    "\n",
    "# Train and predict\n",
    "alpha = [0.05, 0.32]\n",
    "mapie = MapieRegressor(LinearRegression())\n",
    "mapie.fit(X, y)\n",
    "y_pred, y_pis = mapie.predict(X, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the coverage of the prediction intervals\n",
    "from mapie.metrics import regression_coverage_score\n",
    "\n",
    "coverage_scores = [\n",
    "    regression_coverage_score(y, y_pis[:, 0, i], y_pis[:, 1, i])\n",
    "    for i, _ in enumerate(alpha)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e86af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the estimated prediction intervals\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(11, 7))\n",
    "plt.xlabel(\"x\", fontsize=20)\n",
    "plt.ylabel(\"y\", fontsize=20)\n",
    "plt.xticks(fontsize=20) \n",
    "plt.yticks(fontsize=20) \n",
    "plt.scatter(X, y, alpha=0.3, c='#06B1CF', s=80)\n",
    "plt.plot(X, y_pred, color=\"#E48789\", linewidth=3)\n",
    "order = np.argsort(X[:, 0])\n",
    "plt.plot(X[order], y_pis[order][:, 0, 1], color=\"#E48789\", ls=\"--\", linewidth=3)\n",
    "plt.plot(X[order], y_pis[order][:, 1, 1], color=\"#E48789\", ls=\"--\", linewidth=3)\n",
    "plt.fill_between(\n",
    "    X[order].ravel(),\n",
    "    y_pis[order][:, 0, 0].ravel(),\n",
    "    y_pis[order][:, 1, 0].ravel(),\n",
    "    alpha=0.2,\n",
    "    color=\"#E48789\"\n",
    ")\n",
    "plt.title(\n",
    "    f\"Target coverages for \"\n",
    "    f\"alpha={alpha[0]:.2f}: ({1-alpha[0]:.3f}, {coverage_scores[0]:.3f})\\n\"\n",
    "    f\"Target coverages for \"\n",
    "    f\"alpha={alpha[1]:.2f}: ({1-alpha[1]:.3f}, {coverage_scores[1]:.3f})\",\n",
    "    fontsize=20,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de454da",
   "metadata": {},
   "source": [
    "[Link to MAPIE](https://github.com/scikit-learn-contrib/MAPIE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10fac98",
   "metadata": {},
   "source": [
    "### mlforecast: Scalable Machine Learning for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdf45ae",
   "metadata": {},
   "source": [
    "If you want to perform time series forecasting using machine learning models and scale to massive amounts of data with distributed training, try mlforecast. \n",
    "\n",
    "```python\n",
    "from mlforecast.distributed import DistributedMLForecast\n",
    "from mlforecast.distributed.models.dask.lgb import DaskLGBMForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "\n",
    "# Create Dask Dataframe\n",
    "series_ddf = ...\n",
    "\n",
    "# Perform distributed training\n",
    "fcst = DistributedMLForecast(\n",
    "    models=DaskLGBMForecast(),\n",
    "    freq='D', # daily frequency\n",
    "    lags=[7], \n",
    "    target_transforms=[Differences([1])],\n",
    ")\n",
    "fcst.fit(series_ddf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b5cb98",
   "metadata": {},
   "source": [
    "[Full code of the example above](https://nixtla.github.io/mlforecast/docs/quick_start_distributed.html).\n",
    "\n",
    "[Link to mlforecast](https://github.com/Nixtla/mlforecast)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88139aa5",
   "metadata": {},
   "source": [
    "### MLEM: Capture Your Machine Learning Model's Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf5e18",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mlem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad9d76f",
   "metadata": {},
   "source": [
    "The metadata of a machine learning model provides important information about the model such as:\n",
    "\n",
    "- Hash value\n",
    "- Model methods\n",
    "- Input data schema\n",
    "- Python requirements used to train the model. \n",
    "\n",
    "This information enables others to reproduce the model and its results. \n",
    "\n",
    "With MLEM, you can save both the model and its metadata in a single line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75d918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8f230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlem.api import save \n",
    "\n",
    "# Instead of joblib.dump(model, 'model/diabetes_model')\n",
    "save(model, 'model/diabetes_model', sample_data=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9bdec",
   "metadata": {},
   "source": [
    "Running the code above will create two files: a model file and a metadata file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d61116c",
   "metadata": {},
   "source": [
    "```bash\n",
    "model\n",
    " ├──  diabetes_model  \n",
    " └──  diabetes_model.mlem  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ca746",
   "metadata": {},
   "source": [
    "Here is what the metadata file looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b4010",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# model/diabetes_model.mlem\n",
    "\n",
    "artifacts:\n",
    "  data:\n",
    "    hash: c57e456e8a0768326655a8b52cde4f47\n",
    "    size: 563\n",
    "    uri: diabetes_model\n",
    "call_orders:\n",
    "  predict:\n",
    "  - - model\n",
    "    - predict\n",
    "object_type: model\n",
    "processors:\n",
    "  model:\n",
    "    methods:\n",
    "      predict:\n",
    "        args:\n",
    "        - name: X\n",
    "          type_:\n",
    "            dtype: float64\n",
    "            shape:\n",
    "            - null\n",
    "            - 10\n",
    "            type: ndarray\n",
    "        name: predict\n",
    "        returns:\n",
    "          dtype: float64\n",
    "          shape:\n",
    "          - null\n",
    "          type: ndarray\n",
    "    type: sklearn\n",
    "requirements:\n",
    "- module: sklearn\n",
    "  package_name: scikit-learn\n",
    "  version: 1.2.1\n",
    "- module: numpy\n",
    "  version: 1.24.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fab006",
   "metadata": {},
   "source": [
    "[Link to MLEM](https://mlem.ai/).\n",
    "\n",
    "[How to deploy your model with MLEM](https://towardsdatascience.com/automate-machine-learning-deployment-with-github-actions-f752766981b1?sk=6e234bb505e6bc426bb4760e4b20da1b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafb669",
   "metadata": {},
   "source": [
    "### Distributed Machine Learning with MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9ae9d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606c116f",
   "metadata": {},
   "source": [
    "If you want to perform distributed machine learning tasks and handle large-scale datasets, use MLlib. It's designed to work seamlessly with Apache Spark, making it a powerful tool for scalable machine learning.\n",
    "\n",
    "Similar to scikit-learn, MLlib provides the following tools:\n",
    "- ML Algorithms: Classification, regression, clustering, and collaborative filtering\n",
    "- Featurization: Feature extraction, transformation, dimensionality reduction, and selection\n",
    "- Pipelines: Construction, evaluation, and tuning of ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0703a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = spark.createDataFrame(\n",
    "    [\n",
    "        (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "        (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "        (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "        (1.0, Vectors.dense([0.0, 1.2, -0.5])),\n",
    "    ],\n",
    "    [\"label\", \"features\"],\n",
    ")\n",
    "\n",
    "# Prepare test data\n",
    "test = spark.createDataFrame(\n",
    "    [\n",
    "        (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "        (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "        (1.0, Vectors.dense([0.0, 2.2, -1.5])),\n",
    "    ],\n",
    "    [\"label\", \"features\"],\n",
    ")\n",
    "\n",
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "\n",
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)\n",
    "\n",
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "prediction = model1.transform(test)\n",
    "result = prediction.select(\"features\", \"label\", \"probability\", \"prediction\").collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\n",
    "        \"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "        % (row.features, row.label, row.probability, row.prediction)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b42e94",
   "metadata": {},
   "source": [
    "[Link to MLlib](https://spark.apache.org/docs/latest/ml-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc4e54",
   "metadata": {},
   "source": [
    "### Rapid Prototyping and Comparison of Basic Models with Lazy Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8922ba3",
   "metadata": {},
   "source": [
    "Lazy Predict enables rapid prototyping and comparison of multiple basic models without extensive manual coding or parameter tuning. \n",
    "\n",
    "This helps data scientists identify promising approaches and iterate on them more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ee84e",
   "metadata": {},
   "source": [
    "```python\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y= data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.5,random_state =123)\n",
    "\n",
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)\n",
    "```\n",
    "```bash\n",
    "| Model                          |   Accuracy |   Balanced Accuracy |   ROC AUC |   F1 Score |   Time Taken |\n",
    "|:-------------------------------|-----------:|--------------------:|----------:|-----------:|-------------:|\n",
    "| LinearSVC                      |   0.989474 |            0.987544 |  0.987544 |   0.989462 |    0.0150008 |\n",
    "| SGDClassifier                  |   0.989474 |            0.987544 |  0.987544 |   0.989462 |    0.0109992 |\n",
    "| MLPClassifier                  |   0.985965 |            0.986904 |  0.986904 |   0.985994 |    0.426     |\n",
    "| Perceptron                     |   0.985965 |            0.984797 |  0.984797 |   0.985965 |    0.0120046 |\n",
    "| LogisticRegression             |   0.985965 |            0.98269  |  0.98269  |   0.985934 |    0.0200036 |\n",
    "| LogisticRegressionCV           |   0.985965 |            0.98269  |  0.98269  |   0.985934 |    0.262997  |\n",
    "| SVC                            |   0.982456 |            0.979942 |  0.979942 |   0.982437 |    0.0140011 |\n",
    "| CalibratedClassifierCV         |   0.982456 |            0.975728 |  0.975728 |   0.982357 |    0.0350015 |\n",
    "| PassiveAggressiveClassifier    |   0.975439 |            0.974448 |  0.974448 |   0.975464 |    0.0130005 |\n",
    "| LabelPropagation               |   0.975439 |            0.974448 |  0.974448 |   0.975464 |    0.0429988 |\n",
    "| LabelSpreading                 |   0.975439 |            0.974448 |  0.974448 |   0.975464 |    0.0310006 |\n",
    "| RandomForestClassifier         |   0.97193  |            0.969594 |  0.969594 |   0.97193  |    0.033     |\n",
    "| GradientBoostingClassifier     |   0.97193  |            0.967486 |  0.967486 |   0.971869 |    0.166998  |\n",
    "| QuadraticDiscriminantAnalysis  |   0.964912 |            0.966206 |  0.966206 |   0.965052 |    0.0119994 |\n",
    "| HistGradientBoostingClassifier |   0.968421 |            0.964739 |  0.964739 |   0.968387 |    0.682003  |\n",
    "| RidgeClassifierCV              |   0.97193  |            0.963272 |  0.963272 |   0.971736 |    0.0130029 |\n",
    "| RidgeClassifier                |   0.968421 |            0.960525 |  0.960525 |   0.968242 |    0.0119977 |\n",
    "| AdaBoostClassifier             |   0.961404 |            0.959245 |  0.959245 |   0.961444 |    0.204998  |\n",
    "| ExtraTreesClassifier           |   0.961404 |            0.957138 |  0.957138 |   0.961362 |    0.0270066 |\n",
    "| KNeighborsClassifier           |   0.961404 |            0.95503  |  0.95503  |   0.961276 |    0.0560005 |\n",
    "| BaggingClassifier              |   0.947368 |            0.954577 |  0.954577 |   0.947882 |    0.0559971 |\n",
    "| BernoulliNB                    |   0.950877 |            0.951003 |  0.951003 |   0.951072 |    0.0169988 |\n",
    "| LinearDiscriminantAnalysis     |   0.961404 |            0.950816 |  0.950816 |   0.961089 |    0.0199995 |\n",
    "| GaussianNB                     |   0.954386 |            0.949536 |  0.949536 |   0.954337 |    0.0139935 |\n",
    "| NuSVC                          |   0.954386 |            0.943215 |  0.943215 |   0.954014 |    0.019989  |\n",
    "| DecisionTreeClassifier         |   0.936842 |            0.933693 |  0.933693 |   0.936971 |    0.0170023 |\n",
    "| NearestCentroid                |   0.947368 |            0.933506 |  0.933506 |   0.946801 |    0.0160074 |\n",
    "| ExtraTreeClassifier            |   0.922807 |            0.912168 |  0.912168 |   0.922462 |    0.0109999 |\n",
    "| CheckingClassifier             |   0.361404 |            0.5      |  0.5      |   0.191879 |    0.0170043 |\n",
    "| DummyClassifier                |   0.512281 |            0.489598 |  0.489598 |   0.518924 |    0.0119965 |\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dad983",
   "metadata": {},
   "source": [
    "[Link to Lazy Predict](https://github.com/shankarpandala/lazypredict)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abfff52",
   "metadata": {},
   "source": [
    "### AutoGluon: Fast and Accurate ML in 3 Lines of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1733777",
   "metadata": {},
   "source": [
    "The traditional scikit-learn approach requires extensive manual work, including data preprocessing, model selection, and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146b4228",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "numeric_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Machine Learning Pipeline\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [5, 10, None],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa098162",
   "metadata": {},
   "source": [
    "In contrast, AutoGluon automates these tasks, allowing you to train and deploy accurate models in 3 lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf9810",
   "metadata": {},
   "source": [
    "```python\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "predictor = TabularPredictor(label=\"class\").fit(train_data)\n",
    "predictions = predictor.predict(test_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37877913",
   "metadata": {},
   "source": [
    "[Link to AutoGluon](https://github.com/autogluon/autogluon)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba0b517",
   "metadata": {},
   "source": [
    "### Model Logging Made Easy: MLflow vs. Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549ac51",
   "metadata": {},
   "source": [
    "Here is why using MLflow to log model is superior to using pickle to save model:\n",
    "\n",
    "1. Managing Library Versions:\n",
    "- Problem: Different models may require different versions of the same library, which can lead to conflicts. Manually tracking and setting up the correct environment for each model is time-consuming and error-prone.\n",
    "- Solution: By automatically logging dependencies, MLflow ensures that anyone can recreate the exact environment needed to run the model.\n",
    "\n",
    "2. Documenting Inputs and Outputs: \n",
    "- Problem: Often, the expected inputs and outputs of a model are not well-documented, making it difficult for others to use the model correctly.\n",
    "- Solution: By defining a clear schema for inputs and outputs, MLflow ensures that anyone using the model knows exactly what data to provide and what to expect in return.\n",
    "\n",
    "To demonstrate the advantages of MLflow, let’s implement a simple logistic regression model and log it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a76a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "with mlflow.start_run():\n",
    "    X = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\n",
    "    y = np.array([0, 0, 1, 1, 1, 0])\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    signature = infer_signature(X, lr.predict(X))\n",
    "\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr, artifact_path=\"model\", signature=signature\n",
    "    )\n",
    "\n",
    "    print(f\"Saving data to {model_info.model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3549b",
   "metadata": {},
   "source": [
    "The output indicates where the model has been saved. To use the logged model later, you can load it with the `model_uri`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aaed00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "\n",
    "model_uri = \"runs:/1e20d72afccf450faa3b8a9806a97e83/model\"\n",
    "sklearn_pyfunc = mlflow.pyfunc.load_model(model_uri=model_uri)\n",
    "\n",
    "data = np.array([-4, 1, 0, 10, -2, 1]).reshape(-1, 1)\n",
    "\n",
    "predictions = sklearn_pyfunc.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837de6fd",
   "metadata": {},
   "source": [
    "Let's inspect the artifacts saved with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e09da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd mlruns/0/1e20d72afccf450faa3b8a9806a97e83/artifacts/model\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e6c78",
   "metadata": {},
   "source": [
    "The `MLmodel` file provides essential information about the model, including dependencies and input/output specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d6a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af800be3",
   "metadata": {},
   "source": [
    "The `conda.yaml` and `python_env.yaml` files outline the environment dependencies, ensuring that the model runs in a consistent setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0149a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat conda.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f3b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat python_env.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1dad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce99fe7e",
   "metadata": {},
   "source": [
    "[Learn more about MLFlow Models](https://mlflow.org/docs/latest/models.html#python-function-python-function)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694dd4c",
   "metadata": {},
   "source": [
    "### Simplifying ML Model Integration with FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6686d74",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip3 install joblib \"fastapi[standard]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcad605",
   "metadata": {},
   "source": [
    "Imagine this scenario: You have just built a machine learning (ML) model with great performance, and you want to share this model with your team members so that they can develop a web application on top of your model.\n",
    "\n",
    "One way to share the model with your team members is to save the model to a file (e.g., using pickle, joblib, or framework-specific methods) and share the file directly\n",
    "\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "model = ...\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"model.joblib\")\n",
    "\n",
    "# Load model\n",
    "model = joblib.load(model)\n",
    "```\n",
    "\n",
    "However, this approach requires the same environment and dependencies, and it can pose potential security risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c808e",
   "metadata": {},
   "source": [
    "An alternative is creating an API for your ML model. APIs define how software components interact, allowing:\n",
    "\n",
    "1. Access from various programming languages and platforms\n",
    "2. Easier integration for developers unfamiliar with ML or Python\n",
    "3. Versatile use across different applications (web, mobile, etc.)\n",
    "\n",
    "This approach simplifies model sharing and usage, making it more accessible for diverse development needs.\n",
    "\n",
    "Let's learn how to create an ML API with FastAPI, a modern and fast web framework for building APIs with Python. \n",
    "\n",
    "Before we begin constructing an API for a machine learning model, let's first develop a basic model that our API will use. In this example, we'll create a model that predicts the median house price in California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "X, y = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean squared error: {mse:.2f}\")\n",
    "\n",
    "# Save model\n",
    "joblib.dump(model, \"lr.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafbde24",
   "metadata": {},
   "source": [
    "Once we have our model, we can create an API for it using FastAPI. We'll define a POST endpoint for making predictions and use the model to make predictions.\n",
    "\n",
    "Here's an example of how to create an API for a machine learning model using FastAPI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321561c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ml_app.py\n",
    "from fastapi import FastAPI\n",
    "import joblib\n",
    "import pandas as pd \n",
    "\n",
    "# Create a FastAPI application instance\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the pre-trained machine learning model\n",
    "model = joblib.load(\"lr.joblib\")\n",
    "\n",
    "# Define a POST endpoint for making predictions\n",
    "@app.post(\"/predict/\")\n",
    "def predict(data: list[float]):\n",
    "    # Define the column names for the input features\n",
    "    columns = [\n",
    "        \"MedInc\",\n",
    "        \"HouseAge\",\n",
    "        \"AveRooms\",\n",
    "        \"AveBedrms\",\n",
    "        \"Population\",\n",
    "        \"AveOccup\",\n",
    "        \"Latitude\",\n",
    "        \"Longitude\",\n",
    "    ]\n",
    "    \n",
    "    # Create a pandas DataFrame from the input data\n",
    "    features = pd.DataFrame([data], columns=columns)\n",
    "    \n",
    "    # Use the model to make a prediction\n",
    "    prediction = model.predict(features)[0]\n",
    "    \n",
    "    # Return the prediction as a JSON object, rounding to 2 decimal places\n",
    "    return {\"price\": round(prediction, 2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1948ebd9",
   "metadata": {},
   "source": [
    "To run your FastAPI app for development, use the `fastapi dev` command:\n",
    "```bash\n",
    "$ fastapi dev ml_app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa0b23",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!fastapi dev ml_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b9f73",
   "metadata": {},
   "source": [
    "This will start the development server and open the API documentation in your default browser.\n",
    "\n",
    "You can now use the API to make predictions by sending a POST request to the `/predict/` endpoint with the input data. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794ea5ae",
   "metadata": {},
   "source": [
    "Running this cURL command on your terminal:\n",
    "```bash\n",
    "curl -X 'POST' \\\n",
    "  'http://127.0.0.1:8000/predict/' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '[\n",
    "  1.68, 25, 4, 2, 1400, 3, 36.06, -119.01\n",
    "]'\n",
    "```\n",
    "This will return the predicted price as a JSON object, rounded to 2 decimal places:\n",
    "```python\n",
    "{\"price\":1.51}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432c778",
   "metadata": {},
   "source": [
    "### imodels: Simplifying Machine Learning with Interpretable Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb66b6e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install imodels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1ce206",
   "metadata": {},
   "source": [
    "Interpreting decisions made by complex modern machine learning models can be challenging.\n",
    "\n",
    "imodels, a Python package, replaces black-box models (e.g. random forests) with simpler and interpretable alternatives (e.g. rule lists) without losing accuracy.\n",
    "\n",
    "imodels works like scikit-learn models, making it easy to integrate into existing workflows.\n",
    "\n",
    "Here's an example of fitting an interpretable decision tree to predict juvenile delinquency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78606de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from imodels import get_clean_dataset, HSTreeClassifierCV \n",
    "\n",
    "# Prepare data\n",
    "X, y, feature_names = get_clean_dataset('juvenile')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Initialize a tree model and specify only 4 leaf nodes\n",
    "model = HSTreeClassifierCV(max_leaf_nodes=4)  \n",
    "model.fit(X_train, y_train, feature_names=feature_names)  \n",
    "\n",
    "# Make predictions\n",
    "preds = model.predict(X_test) \n",
    "\n",
    "# Print the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba7a347",
   "metadata": {},
   "source": [
    "This tree structure clearly shows how predictions are made based on feature values, providing transparency into the model's decision-making process. The hierarchical shrinkage technique also improves predictive performance compared to standard decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b66031",
   "metadata": {},
   "source": [
    "[Link to imodels](https://github.com/csinva/imodels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c47004",
   "metadata": {},
   "source": [
    "### How to Build a Recommendation Engine Using Surprise in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d502e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65933f40",
   "metadata": {},
   "source": [
    "Building reliable recommendation systems from scratch requires complex algorithms and data handling, which results in spending significant time implementing common algorithms like SVD or handling cross-validation procedures.\n",
    "\n",
    "```python\n",
    "# Without a dedicated library, you'd need to implement basic collaborative filtering:\n",
    "import numpy as np\n",
    "\n",
    "def calculate_similarity(ratings_matrix):\n",
    "    # Complex similarity calculations\n",
    "    pass\n",
    "\n",
    "def predict_rating(user_id, item_id, ratings, similarities):\n",
    "    # Complex prediction logic\n",
    "    pass\n",
    "\n",
    "# Handle cross-validation manually\n",
    "def cross_validate(ratings_data, n_folds):\n",
    "    # Manual implementation of CV\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1743786",
   "metadata": {},
   "source": [
    "Surprise helps you build and evaluate recommendation systems with just a few lines of code. You can use built-in algorithms, handle datasets easily, and evaluate models using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from surprise import Dataset, SVD, Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "\n",
    "# Creation of the dataframe. Column names are irrelevant.\n",
    "ratings_dict = {\n",
    "    \"itemID\": [1, 1, 1, 2, 2],\n",
    "    \"userID\": [9, 32, 2, 45, 4],\n",
    "    \"rating\": [3, 2, 4, 3, 1],\n",
    "}\n",
    "df = pd.DataFrame(ratings_dict)\n",
    " \n",
    "# A reader is still needed but only the rating_scale param is required.\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(df[[\"userID\", \"itemID\", \"rating\"]], reader)\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "# Run 2-fold cross-validation and print results.\n",
    "result = cross_validate(algo, data, cv=2, measures=['RMSE', 'MAE'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc58a44",
   "metadata": {},
   "source": [
    "Compute the rating prediction for given user and item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8554c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_id = 302\n",
    "user_id = 196\n",
    "\n",
    "pred = algo.predict(user_id, item_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567ae69c",
   "metadata": {},
   "source": [
    "Surprise also provides a variety of basic algorithms, k-NN inspired algorithms, and matrix factorization-based algorithms. View all available algorithms [here](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a477a67a",
   "metadata": {},
   "source": [
    "[Link to Surprise](https://github.com/NicolasHug/Surprise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28622bf1",
   "metadata": {},
   "source": [
    "### PyOD: Simplifying Outlier Detection in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a393e74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pyod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bea048c",
   "metadata": {},
   "source": [
    "Detecting anomalies or outliers in datasets is often challenging and time-consuming, requiring complex statistical calculations and multiple algorithm implementations. This results in data scientists writing redundant code or potentially missing important anomalies by using overly simplistic approaches.\n",
    "\n",
    "Without specialized tools, detecting outliers often looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da40c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def detect_outliers(data):\n",
    "    # Using simple statistical method (z-score)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    print(f\"{mean=:.3f}\")\n",
    "    print(f\"{std=:.3f}\")\n",
    "    \n",
    "    z_scores = [(x - mean) / std for x in data]\n",
    "    \n",
    "    # Consider points beyond 3 standard deviations as outliers\n",
    "    outliers = [x for x, z in zip(data, z_scores) if abs(z) > 3]\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "data = [1, 2, 2, 3, 3, 4, 1000, 2, 3]\n",
    "outliers = detect_outliers(data)\n",
    "print(f\"{outliers=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43aba33",
   "metadata": {},
   "source": [
    "In this example, even though 1000 is apear to be an outlier in the `data` list, it is not in the `outliers` list. This is because the standard deviation is heavily influenced by the extreme value (1000).\n",
    "\n",
    "With PyOD, you can easily implement various state-of-the-art outlier detection algorithms, combine multiple methods, and handle multivariate data. The library provides a consistent API across different algorithms and includes both traditional statistical methods and modern deep learning approaches.\n",
    "\n",
    "Import models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7479124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "from pyod.utils.data import generate_data\n",
    "from pyod.utils.data import evaluate_print\n",
    "from pyod.utils.example import visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eb9b72",
   "metadata": {},
   "source": [
    "Generate sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb11970",
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination = 0.1  # percentage of outliers\n",
    "n_train = 200  # number of training points\n",
    "n_test = 100  # number of testing points\n",
    "\n",
    "X_train, X_test, y_train, y_test = generate_data(\n",
    "    n_train=n_train, n_test=n_test, contamination=contamination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9a097",
   "metadata": {},
   "source": [
    "Initialize a `pyod.models.knn.KNN` detector, fit the model, and make the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train kNN detector\n",
    "clf = KNN()\n",
    "clf.fit(X_train)\n",
    "\n",
    "# get the prediction labels and outlier scores of the training data\n",
    "y_train_pred = clf.labels_  # binary labels (0: inliers, 1: outliers)\n",
    "y_train_scores = clf.decision_scores_  # raw outlier scores\n",
    "\n",
    "# get the prediction on the test data\n",
    "y_test_pred = clf.predict(X_test)  # outlier labels (0 or 1)\n",
    "y_test_scores = clf.decision_function(X_test)  # outlier scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c576c45e",
   "metadata": {},
   "source": [
    "Evaluate the prediction using ROC and Precision @ Rank n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b3a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_name = 'KNN'\n",
    "print(\"\\nOn Training Data:\")\n",
    "evaluate_print(clf_name, y_train, y_train_scores)\n",
    "print(\"\\nOn Test Data:\")\n",
    "evaluate_print(clf_name, y_test, y_test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9d3467",
   "metadata": {},
   "source": [
    "Generate the visualizations by visualize function included in all examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59689f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,\n",
    "          y_test_pred, show_figure=True, save_figure=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db092315",
   "metadata": {},
   "source": [
    "[Link to pyod](https://github.com/yzhao062/pyod)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   20,
   24,
   28,
   34,
   41,
   54,
   57,
   61,
   65,
   70,
   88,
   92,
   96,
   123,
   127,
   131,
   135,
   144,
   148,
   152,
   156,
   162,
   169,
   178,
   182,
   185,
   189,
   204,
   207,
   211,
   222,
   224,
   228,
   233,
   236,
   240,
   244,
   248,
   254,
   259,
   264,
   268,
   272,
   276,
   280,
   284,
   290,
   298,
   302,
   306,
   310,
   314,
   318,
   326,
   329,
   333,
   338,
   344,
   348,
   352,
   356,
   360,
   393,
   399,
   403,
   407,
   415,
   427,
   438,
   451,
   455,
   458,
   462,
   477,
   481,
   485,
   489,
   493,
   497,
   501,
   516,
   526,
   557,
   561,
   565,
   587,
   593,
   597,
   601,
   614,
   632,
   637,
   641,
   649,
   653,
   695,
   701,
   705,
   709,
   718,
   762,
   766,
   770,
   776,
   829,
   833,
   837,
   841,
   882,
   886,
   895,
   899,
   903,
   917,
   935,
   939,
   949,
   953,
   956,
   960,
   962,
   966,
   970,
   974,
   976,
   980,
   984,
   988,
   1009,
   1023,
   1049,
   1055,
   1090,
   1097,
   1101,
   1107,
   1124,
   1128,
   1136,
   1146,
   1163,
   1167,
   1171,
   1175,
   1183,
   1205,
   1209,
   1234,
   1238,
   1243,
   1247,
   1251,
   1255,
   1263,
   1271,
   1292,
   1300,
   1305,
   1309,
   1316,
   1320,
   1332,
   1336,
   1342,
   1346,
   1351,
   1354
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}