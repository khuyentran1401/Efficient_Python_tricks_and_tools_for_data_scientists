{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc19c12a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ed0c0",
   "metadata": {},
   "source": [
    "This section covers tools to get some data for your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e68d37",
   "metadata": {},
   "source": [
    "### faker: Create Fake Data in One Line of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167aa67f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be66e40",
   "metadata": {},
   "source": [
    "For quickly generating fake data for testing, use `Faker`.\n",
    "\n",
    "Here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "fake.color_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1c230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.date_of_birth(minimum_age=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f57e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.city()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba6b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.job()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f66181",
   "metadata": {},
   "source": [
    "[Link to faker](https://faker.readthedocs.io/en/master/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a152f",
   "metadata": {},
   "source": [
    "### Silly: Produce Silly Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f2de1e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install silly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a027b46",
   "metadata": {},
   "source": [
    "For generating playful test data, try the `silly` library.\n",
    "\n",
    "Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import silly \n",
    "\n",
    "name = silly.name()\n",
    "email = silly.email()\n",
    "print(f\"Her name is {name}. Her email is {email}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f883b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "silly.a_thing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7088a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "silly.thing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "silly.things()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e724389",
   "metadata": {},
   "outputs": [],
   "source": [
    "silly.sentence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b6e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "silly.paragraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152bb15",
   "metadata": {},
   "source": [
    "[Link to silly](https://github.com/cube-drone/silly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a13ecb",
   "metadata": {},
   "source": [
    "### Random User: Generate Random User Data in One Line of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822d430e",
   "metadata": {},
   "source": [
    "Need to generate fake user data for testing? The Random User Generator API provides a simple way to get random user data. Here’s how you can fetch and use this data in your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af29fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Show 2 random users\n",
    "data = urlopen(\"https://randomuser.me/api?results=2\").read()\n",
    "users = json.loads(data)[\"results\"]\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d86ab",
   "metadata": {},
   "source": [
    "[Link to Random User Generator](https://randomuser.me/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62c26f",
   "metadata": {},
   "source": [
    "### fetch_openml: Get OpenML’s Dataset in One Line of Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7262b3a",
   "metadata": {},
   "source": [
    "OpenML offers a variety of intriguing datasets. You can easily fetch these datasets in Python using `sklearn.datasets.fetch_openml`.\n",
    "\n",
    "Here’s how to load an OpenML dataset with a single line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2548b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "monk = fetch_openml(name=\"monks-problems-2\", as_frame=True)\n",
    "print(monk[\"data\"].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512551bd",
   "metadata": {},
   "source": [
    "### Autoscraper: Automate Web Scraping in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e2ea6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install autoscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3cd8c",
   "metadata": {},
   "source": [
    "To automate web scraping with minimal code, try `autoscraper`.\n",
    "\n",
    "Here's a quick example to extract specific elements from a webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0861f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoscraper import AutoScraper\n",
    "\n",
    "url = \"https://stackoverflow.com/questions/2081586/web-scraping-with-python\"\n",
    "\n",
    "wanted_list = [\"How to check version of python modules?\"]\n",
    "\n",
    "scraper = AutoScraper()\n",
    "result = scraper.build(url, wanted_list)\n",
    "\n",
    "for res in result:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5677280",
   "metadata": {},
   "source": [
    "[Link to autoscraper](https://github.com/alirezamika/autoscraper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1776c513",
   "metadata": {},
   "source": [
    "### pandas-reader: Extract Data from Various Internet Sources Directly into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac427780",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b86063f",
   "metadata": {},
   "source": [
    "To retrieve time series data from various internet sources directly into a pandas DataFrame, use `pandas-datareader`.\n",
    "\n",
    "Here’s how you can fetch daily data of the AD indicator from 2008 to 2018:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d546c6ea",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas_datareader.data as web\n",
    "\n",
    "df = web.DataReader(\n",
    "    \"AD\",\n",
    "    \"av-daily\",\n",
    "    start=datetime(2008, 1, 1),\n",
    "    end=datetime(2018, 2, 28),\n",
    "    api_key=os.gehide-outputtenv(\"ALPHAVANTAGE_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5456a2ff",
   "metadata": {},
   "source": [
    "[Link to pandas_reader](https://pandas-datareader.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304aa73a",
   "metadata": {},
   "source": [
    "### pytrends: Get the Trend of a Keyword on Google Search Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf261117",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install pytrends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61fd891",
   "metadata": {},
   "source": [
    "To analyze keyword trends on Google Search, use pytrends.\n",
    "\n",
    "Here’s an example to track the trend of the keyword \"data science\" from 2019 to 2024:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23aa024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6195f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "pytrends.build_payload(kw_list=[\"data science\"])\n",
    "\n",
    "df = pytrends.interest_over_time()\n",
    "df[\"data science\"].plot(figsize=(20, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724ee35",
   "metadata": {},
   "source": [
    "[Link to pytrends](https://github.com/GeneralMills/pytrends)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f509f",
   "metadata": {},
   "source": [
    "### snscrape: Scrape Social Networking Services in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56878e98",
   "metadata": {},
   "source": [
    "To scrape data from social media platforms like Twitter, Facebook, or Reddit, use `snscrape`.\n",
    "\n",
    "Here's how to scrape all tweets from a user or get the latest 100 tweets with the hashtag #python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed103101",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Scrape all tweets from @KhuyenTran16\n",
    "snscrape twitter-user KhuyenTran16\n",
    "\n",
    "# Save outputs\n",
    "snscrape twitter-user KhuyenTran16 >> khuyen_tweets \n",
    "\n",
    "# Scrape 100 tweets with hashtag python\n",
    "snscrape --max-results 100 twitter-hashtag python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a4056",
   "metadata": {},
   "source": [
    "[Link to snscrape](https://github.com/JustAnotherArchivist/snscrape)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5a5934",
   "metadata": {},
   "source": [
    "### Datacommons: Get Statistics about a Location in One Line of Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3d10e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install datacommons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f9f35",
   "metadata": {},
   "source": [
    "To get statistics about a location with one line of code, use `datacommons`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d1b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacommons_pandas\n",
    "import plotly.express as px \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1409e0c",
   "metadata": {},
   "source": [
    "Here’s how to find median income in California over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce8ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_income = datacommons_pandas.build_time_series(\"geoId/06\", \"Median_Income_Person\")\n",
    "median_income.index = pd.to_datetime(median_income.index)\n",
    "median_income.plot(\n",
    "    figsize=(20, 10),\n",
    "    x=\"Income\",\n",
    "    y=\"Year\",\n",
    "    title=\"Median Income in California Over Time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683b2786",
   "metadata": {},
   "source": [
    "To visualize the number of people in the U.S. over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d14cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ts(statistics: str):\n",
    "    count_person = datacommons_pandas.build_time_series('country/USA', statistics)\n",
    "    count_person.index = pd.to_datetime(count_person.index)\n",
    "    count_person.name = statistics\n",
    "    return count_person "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_person_male = process_ts('Count_Person_Male')\n",
    "count_person_female = process_ts('Count_Person_Female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_person = pd.concat([count_person_female, count_person_male], axis=1)\n",
    "\n",
    "count_person.plot(\n",
    "    figsize=(20, 10),\n",
    "    title=\"Number of People in the U.S Over Time\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870f760",
   "metadata": {},
   "source": [
    "[Link to Datacommons](https://datacommons.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c912aa4",
   "metadata": {},
   "source": [
    "### Get Google News Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f26068",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install GoogleNews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c877f1b",
   "metadata": {},
   "source": [
    "To retrieve Google News results for a specific keyword and date range, use the `GoogleNews` library.\n",
    "\n",
    "Here's how you can get news articles related to a keyword within a given time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1544b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GoogleNews import GoogleNews\n",
    "googlenews = GoogleNews()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d23cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews.set_time_range('02/01/2022','03/25/2022')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a679541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews.search('funny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe0dbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "googlenews.results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c01a07",
   "metadata": {},
   "source": [
    "[Link to GoogleNews](https://pypi.org/project/GoogleNews/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a672dbb4",
   "metadata": {},
   "source": [
    "### people_also_ask: Python Wrapper for Google People Also Ask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c48984",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install people_also_ask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2334ae",
   "metadata": {},
   "source": [
    "To interact with Google's \"People Also Ask\" feature programmatically, use the `people_also_ask` library.\n",
    "\n",
    "Here’s how to retrieve related questions and answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0fa4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import people_also_ask as ask\n",
    "\n",
    "ask.get_related_questions('data science')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68698b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask.get_answer('Is data science a easy career?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10decf",
   "metadata": {},
   "source": [
    "[Link to people-also-ask](https://github.com/lagranges/people_also_ask)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587339e2",
   "metadata": {},
   "source": [
    "### Scrape Facebook Public Pages Without an API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0434761f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "pip install facebook-scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298f37c5",
   "metadata": {},
   "source": [
    "To scrape Facebook public pages without needing an API key, use the `facebook-scraper` library.\n",
    "\n",
    "You can use `facebook-scraper` to extract information from public profiles and groups. Here's how you can do it:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453d12e9",
   "metadata": {},
   "source": [
    "Get group information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5bf2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_profile, get_group_info\n",
    "\n",
    "get_group_info(\"thedachshundowners\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7415cee6",
   "metadata": {},
   "source": [
    "Get profile information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50afba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_profile(\"zuck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7702b82",
   "metadata": {},
   "source": [
    "[Link to facebook-scraper](https://github.com/kevinzg/facebook-scraper)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a9c42",
   "metadata": {},
   "source": [
    "### Recipe-Scrapers: Automate Recipe Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc4c7a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install recipe-scrapers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed821cd1",
   "metadata": {},
   "source": [
    "For automated recipe data extraction, use the `recipe-scrapers` library. It simplifies the process of gathering recipe information from various websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf77d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from recipe_scrapers import scrape_me\n",
    "\n",
    "scraper = scrape_me('https://cookieandkate.com/thai-red-curry-recipe/')\n",
    "\n",
    "scraper.host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2116fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.total_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.ingredients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f4566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d65079",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.nutrients()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e7de6",
   "metadata": {},
   "source": [
    "[Link to recipe-scrapers](https://bit.ly/3U6vw0w)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43243f08",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "### Parsera: Natural Language Web Scraping with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4925808",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install parsera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3872181",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b13562",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "Writing and maintaining web scraping code requires constant updates due to changing HTML structures and complex selectors, which results in brittle code and frequent breakages.\n",
    "\n",
    "With Parsera, you can scrape websites by simply describing what data you want to extract in plain language, letting LLMs handle the complexity of finding the right elements.\n",
    "\n",
    "Here's an example that scrapes GitHub's trending Python repositories page to collect:\n",
    "- Repository names\n",
    "- Repository owners\n",
    "- Star counts\n",
    "- Fork counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e04cf2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4321eb7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e953be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "from parsera import Parsera\n",
    "from pprint import pprint\n",
    "\n",
    "url = \"https://github.com/trending/python?since=daily\"\n",
    "elements = {\n",
    "    \"Repository\": \"Name of the repository\",\n",
    "    \"Owner\": \"Owner of the repository\",\n",
    "    \"Stars\": \"Number of stars\",\n",
    "    \"Forks\": \"Number of forks\",\n",
    "}\n",
    "\n",
    "scraper = Parsera()\n",
    "result = scraper.run(url=url, elements=elements)\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3054f14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "[Link to Parsera](https://github.com/raznem/parsera)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3341ad1a",
   "metadata": {},
   "source": [
    "### Generating Synthetic Tabular Data with TabGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a8208",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install tabgan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a274c98",
   "metadata": {},
   "source": [
    "Limited, imbalanced, or missing data in tabular datasets can lead to poor model performance and biased predictions. \n",
    "\n",
    "To address this issue, TabGAN provides a solution by generating synthetic tabular data that maintains the statistical properties and relationships of the original dataset. \n",
    "\n",
    "In this example, we will demonstrate how to use TabGAN to generate high-quality synthetic data using different generators (GAN, Diffusion, or LLM-based).\n",
    "\n",
    "\n",
    "First, we create random input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608aba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabgan.sampler import OriginalGenerator, GANGenerator, ForestDiffusionGenerator, LLMGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train = pd.DataFrame(np.random.randint(-10, 150, size=(150, 4)), columns=list(\"ABCD\"))\n",
    "target = pd.DataFrame(np.random.randint(0, 2, size=(150, 1)), columns=list(\"Y\"))\n",
    "test = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list(\"ABCD\"))\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(train.head())\n",
    "\n",
    "print(\"\\nTarget Data:\")\n",
    "print(target.head())\n",
    "\n",
    "print(\"\\nTest Data:\")\n",
    "print(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a461a",
   "metadata": {},
   "source": [
    "Next, we use the `OriginalGenerator` to generate synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b5f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train1, new_target1 = OriginalGenerator().generate_data_pipe(train, target, test)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(new_train1.head())\n",
    "\n",
    "print(\"\\nTarget Data:\")\n",
    "print(new_target1.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398eb95d",
   "metadata": {},
   "source": [
    "The `generate_data_pipe` method takes in the following parameters:\n",
    "\n",
    "*   `train_df`: The training dataframe.\n",
    "*   `target`: The target variable for the training dataset.\n",
    "*   `test_df`: The testing dataframe.\n",
    "*   `deep_copy`: A boolean indicating whether to make a deep copy of the input dataframes. Default is `True`.\n",
    "*   `only_adversarial`: A boolean indicating whether to only perform adversarial filtering on the training dataframe. Default is `False`.\n",
    "*   `use_adversarial`: A boolean indicating whether to perform adversarial filtering on the generated data. Default is `True`.\n",
    "*   `only_generated_data`: A boolean indicating whether to return only the generated data. Default is `False`.\n",
    "    \n",
    "Alternatively, we can use the `GANGenerator` to generate synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train2, new_target2 = GANGenerator(\n",
    "    gen_params={\n",
    "        \"batch_size\": 500,  # Process data in batches of 500 samples at a time\n",
    "        \"epochs\": 10,       # Train for a maximum of 10 epochs\n",
    "        \"patience\": 5       # Stop early if there is no improvement for 5 epochs\n",
    "}\n",
    ").generate_data_pipe(train, target, test)\n",
    "\n",
    "print(\"Training Data:\")\n",
    "print(new_train2.head())\n",
    "\n",
    "print(\"\\nTarget Data:\")\n",
    "print(new_target2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6ed12",
   "metadata": {},
   "source": [
    "The `GANGenerator` takes in the following parameters:\n",
    "\n",
    "*   `gen_x_times`: A float indicating how much data to generate. Default is `1.1`.\n",
    "*   `cat_cols`: A list of categorical columns. Default is `None`.\n",
    "*   `bot_filter_quantile`: A float indicating the bottom quantile for post-processing filtering. Default is `0.001`.\n",
    "*   `top_filter_quantile`: A float indicating the top quantile for post-processing filtering. Default is `0.999`.\n",
    "*   `is_post_process`: A boolean indicating whether to perform post-processing filtering. Default is `True`.\n",
    "*   `adversarial_model_params`: A dictionary of parameters for the adversarial filtering model. Default is `None`.\n",
    "*   `pregeneration_frac`: A float indicating the fraction of data to generate before post-processing filtering. Default is `2`.\n",
    "*   `gen_params`: A dictionary of parameters for the GAN training process. Default is `None`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573105bc",
   "metadata": {},
   "source": [
    "[Link to TabGan](https://github.com/Diyago/Tabular-data-generation)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.7"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   12,
   16,
   20,
   24,
   28,
   34,
   42,
   46,
   50,
   54,
   58,
   60,
   64,
   68,
   72,
   79,
   87,
   91,
   95,
   99,
   103,
   105,
   109,
   113,
   117,
   125,
   129,
   133,
   139,
   144,
   148,
   152,
   158,
   170,
   174,
   178,
   182,
   188,
   202,
   206,
   210,
   214,
   220,
   224,
   230,
   234,
   238,
   244,
   257,
   261,
   265,
   269,
   273,
   277,
   281,
   290,
   294,
   302,
   307,
   314,
   318,
   322,
   326,
   332,
   337,
   341,
   345,
   347,
   351,
   355,
   359,
   365,
   371,
   373,
   377,
   381,
   385,
   391,
   395,
   399,
   403,
   405,
   409,
   413,
   417,
   421,
   429,
   433,
   437,
   441,
   445,
   447,
   451,
   455,
   465,
   475,
   487,
   498,
   509,
   531,
   535,
   539,
   547,
   558,
   576,
   580,
   588,
   602,
   616,
   629
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}