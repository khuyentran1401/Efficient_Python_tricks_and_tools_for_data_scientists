{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf9d9ab-cf44-4dfb-8d4c-e1b2c4f2a00b",
   "metadata": {},
   "source": [
    "# DuckDB\n",
    "\n",
    "# SQL Analytics with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb80b3-267d-4c75-98a8-a4d037a596cd",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install psycopg2 duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d2d3d2-3665-4450-8d99-b3f4dc39eb43",
   "metadata": {},
   "source": [
    "SQL operations on data frames typically require setting up and maintaining separate database servers, adding complexity to analytical workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83d0bae-ace2-490d-97ee-6def7cbc64b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample data\n",
    "df = pd.DataFrame(\n",
    "    {\"customer_id\": range(1000), \"revenue\": range(1000), \"segment\": [\"A\", \"B\"] * 500}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd2b626-2902-4125-85ca-45a7537a1205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment Analysis:\n",
      "  segment  avg_revenue  customer_count\n",
      "0       B        500.0             500\n",
      "1       A        499.0             500\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a connection to PostgreSQL\n",
    "engine = create_engine(\"postgresql://postgres:postgres@localhost:5432/postgres\")\n",
    "\n",
    "# Write the DataFrame to a PostgreSQL table\n",
    "df.to_sql(\"customers\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Execute SQL query against the PostgreSQL database\n",
    "with engine.connect() as conn:\n",
    "    result = pd.read_sql(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            segment,\n",
    "            AVG(revenue) as avg_revenue,\n",
    "            COUNT(*) as customer_count\n",
    "        FROM customers\n",
    "        GROUP BY segment\n",
    "        ORDER BY avg_revenue DESC\n",
    "    \"\"\",\n",
    "        conn,\n",
    "    )\n",
    "\n",
    "print(\"Segment Analysis:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b43b6-e5bd-4a01-a66f-a3b080cb74a7",
   "metadata": {},
   "source": [
    "DuckDB simplifies this by providing direct SQL operations on DataFrames without server setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b94b71-3fdf-4ad8-a612-ff9e7df6114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment Analysis:\n",
      "  segment  avg_revenue  customer_count\n",
      "0       B        500.0             500\n",
      "1       A        499.0             500\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# Direct SQL operations on DataFrame - no server needed\n",
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        segment,\n",
    "        AVG(revenue) as avg_revenue,\n",
    "        COUNT(*) as customer_count\n",
    "    FROM df\n",
    "    GROUP BY segment\n",
    "    ORDER BY avg_revenue DESC\n",
    "\"\"\"\n",
    ").df()\n",
    "\n",
    "print(\"Segment Analysis:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3bf509-2e4a-4593-892e-ea9b68b2e015",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://github.com/duckdb/duckdb)\n",
    "\n",
    "## Efficient SQL Operations with DuckDB on Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c1a70d5",
   "metadata": {},
   "source": [
    "Using SQL with pandas empowers data scientists to leverage SQL's powerful querying capabilities alongside the data manipulation functionalities of pandas.\n",
    "\n",
    "In the code below, aggregating data using DuckDB is nearly 6 times faster compared to aggregating with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"lineitemsf1.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b11744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 ms ± 4.63 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.groupby(\"l_returnflag\").agg(\n",
    "    Sum=(\"l_extendedprice\", \"sum\"),\n",
    "    Min=(\"l_extendedprice\", \"min\"),\n",
    "    Max=(\"l_extendedprice\", \"max\"),\n",
    "    Avg=(\"l_extendedprice\", \"mean\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def27d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 ms ± 2.41 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "duckdb.query(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "      l_returnflag,\n",
    "      SUM(l_extendedprice),\n",
    "      MIN(l_extendedprice),\n",
    "      MAX(l_extendedprice),\n",
    "      AVG(l_extendedprice)\n",
    "FROM df\n",
    "GROUP BY\n",
    "        l_returnflag\n",
    "\"\"\"\n",
    ").to_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15ce6b19",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://github.com/duckdb/duckdb).\n",
    "\n",
    "## Efficiently Handle Large Datasets with DuckDB and PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610295ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deltalake duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ddd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0090e9",
   "metadata": {},
   "source": [
    "DuckDB leverages various optimizations for query execution, while PyArrow efficiently handles in-memory data processing and storage. Combining DuckDB and PyArrow allows you to efficiently process datasets larger than memory on a single machine.\n",
    "\n",
    "In the code below, we convert a Delta Lake table with over 6 million rows to a pandas DataFrame and a PyArrow dataset, which are then used by DuckDB. \n",
    "\n",
    "Running DuckDB on PyArrow dataset is approximately 2906 times faster than running DuckDB on pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84460984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from deltalake.writer import write_deltalake\n",
    "\n",
    "df = pd.read_parquet(\"lineitemsf1.snappy.parquet\")\n",
    "write_deltalake(\"delta_lake\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a98f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "\n",
    "table = DeltaTable(\"delta_lake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4090c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 s ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "quack = duckdb.df(table.to_pandas())\n",
    "quack.filter(\"l_quantity > 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70823a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954 µs ± 32.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "quack = duckdb.arrow(table.to_pyarrow_dataset())\n",
    "quack.filter(\"l_quantity > 50\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76baf2a2",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://github.com/duckdb/duckdb).\n",
    "\n",
    "## Simplify CSV Data Management with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a4e2bb-2c3c-4276-a163-a98f1b15625c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30018df",
   "metadata": {
    "tags": []
   },
   "source": [
    "Traditional database systems require a predefined table schema and a subsequent data import process when working with CSV data. \n",
    "\n",
    "To demonstrate this, let's create a CSV file called `customer.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eeb22f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [25, 32, 45, 19, 38],\n",
    "    \"city\": [\"New York\", \"London\", \"Paris\", \"Berlin\", \"Tokyo\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "df.to_csv(\"customers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22da1d-2802-487b-a745-444b5fd9b3e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "To load this CSV file in Postgres, you need to run the following query:\n",
    "\n",
    "```sql\n",
    "-- Create the table\n",
    "CREATE TABLE customers (\n",
    "    name VARCHAR(100),\n",
    "    age INT,\n",
    "    city VARCHAR(100)\n",
    ");\n",
    "\n",
    "-- Load data from CSV\n",
    "COPY customers\n",
    "FROM 'customers.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER;\n",
    "```\n",
    "\n",
    "In contrast, DuckDB allows for direct reading of CSV files from disk, eliminating the need for explicit table creation and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5970f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌─────────┬───────┬──────────┐\n",
       "│  name   │  age  │   city   │\n",
       "│ varchar │ int64 │ varchar  │\n",
       "├─────────┼───────┼──────────┤\n",
       "│ Alice   │    25 │ New York │\n",
       "│ Bob     │    32 │ London   │\n",
       "│ Charlie │    45 │ Paris    │\n",
       "│ David   │    19 │ Berlin   │\n",
       "│ Eve     │    38 │ Tokyo    │\n",
       "└─────────┴───────┴──────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "duckdb.sql(\"SELECT * FROM 'customers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372897d6",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://github.com/duckdb/duckdb).\n",
    "\n",
    "## Automate CSV Parsing with DuckDB's read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5773ce72-7c1a-448d-87c2-c6bb9c55db25",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b24669",
   "metadata": {},
   "source": [
    "Reading a CSV file using pandas without specifying parameters like the delimiter or header can lead to incorrect parsing, especially when the structure has custom delimiters or specific formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9ff4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       FlightDate|UniqueCarrier|OriginCityName|DestCityName\n",
      "1988-01-01|AA|New York  NY|Los Angeles                                                 CA  \n",
      "1988-01-02|AA|New York  NY|Los Angeles                                                 CA  \n",
      "1988-01-03|AA|New York  NY|Los Angeles                                                 CA  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example CSV content with a custom delimiter\n",
    "csv_content = \"\"\"FlightDate|UniqueCarrier|OriginCityName|DestCityName\n",
    "1988-01-01|AA|New York, NY|Los Angeles, CA\n",
    "1988-01-02|AA|New York, NY|Los Angeles, CA\n",
    "1988-01-03|AA|New York, NY|Los Angeles, CA\n",
    "\"\"\"\n",
    "\n",
    "## Writing the CSV content to a file\n",
    "with open(\"example.csv\", \"w\") as f:\n",
    "    f.write(csv_content)\n",
    "\n",
    "## Reading the CSV file with pandas without specifying the delimiter\n",
    "df = pd.read_csv(\"example.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc9532",
   "metadata": {},
   "source": [
    "The pandas code above does not specify the delimiter, leading to incorrect parsing. The default delimiter (`,`) is assumed, which results in the entire row being treated as a single column.\n",
    "\n",
    "DuckDB's `read_csv` feature addresses this issue by automatically detecting the structure of the CSV file, including delimiters, headers, and column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c9aa8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  FlightDate UniqueCarrier OriginCityName     DestCityName\n",
      "0 1988-01-01            AA   New York, NY  Los Angeles, CA\n",
      "1 1988-01-02            AA   New York, NY  Los Angeles, CA\n",
      "2 1988-01-03            AA   New York, NY  Los Angeles, CA\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "## Use DuckDB to automatically detect and read the CSV structure\n",
    "result = duckdb.query(\"SELECT * FROM read_csv('example.csv')\").to_df()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb07f0",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "- `read_csv` automatically detects the pipe (|) delimiter, identifies the column headers, and infers the correct data types for each column.\n",
    "- `duckdb.query` runs the SQL query to read the file and returns the result as a DataFrame using `.to_df()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d083afe",
   "metadata": {},
   "source": [
    "The output shows that DuckDB automatically detected the correct delimiter (`|`) and correctly parsed the data into columns.\n",
    "\n",
    "[Link to DuckDB](https://github.com/duckdb/duckdb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b77a38",
   "metadata": {},
   "source": [
    "## Multiple CSV Files Processing with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12824074",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b7c80",
   "metadata": {},
   "source": [
    "Processing multiple CSV files is a common task in data analysis and engineering workflows. Traditionally, handling multiple CSV files requires writing loops or list comprehensions, which can be verbose and error-prone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54450f7-7fd5-4547-bc18-940febd88a7a",
   "metadata": {},
   "source": [
    "To demonstrate, let's assume we have 2 CSV files in the \"folder\" directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "427acd77-88e2-4daa-a0c0-bd2b95a8969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create example dataframe for first file\n",
    "df1 = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n",
    "        \"Product\": [\"Laptop\", \"Phone\", \"Tablet\"],\n",
    "        \"Sales\": [1200, 800, 600],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create example dataframe for second file\n",
    "df2 = pd.DataFrame(\n",
    "    {\n",
    "        \"Date\": [\"2023-02-01\", \"2023-02-02\", \"2023-02-03\"],\n",
    "        \"Product\": [\"Laptop\", \"Monitor\", \"Mouse\"],\n",
    "        \"Sales\": [1500, 400, 50],\n",
    "    }\n",
    ")\n",
    "\n",
    "Path(\"data\").mkdir()\n",
    "df1.to_csv(\"data/sales_jan.csv\", index=False)\n",
    "df2.to_csv(\"data/sales_feb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2c168a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>total_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  total_sales\n",
       "0  2023-01-01         1200\n",
       "1  2023-01-02          800\n",
       "2  2023-01-03          600\n",
       "3  2023-02-01         1500\n",
       "4  2023-02-02          400\n",
       "5  2023-02-03           50"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "## Read all CSV files and concatenate them\n",
    "csv_files = glob.glob(\"data/*.csv\")\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "## Concatenate all dataframes\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "## Perform grouping and aggregation\n",
    "result = (\n",
    "    combined_df.groupby(\"Date\")[\"Sales\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"Sales\": \"total_sales\"})\n",
    "    .sort_values(\"Date\")\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f84c22",
   "metadata": {},
   "source": [
    "DuckDB simplifies reading multiple CSV files with a single line of code that efficiently processes the files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1483788-2970-46e1-a382-abf03a32dd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>total_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>1200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-01</td>\n",
       "      <td>1500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-02-03</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  total_sales\n",
       "0 2023-01-01       1200.0\n",
       "1 2023-01-02        800.0\n",
       "2 2023-01-03        600.0\n",
       "3 2023-02-01       1500.0\n",
       "4 2023-02-02        400.0\n",
       "5 2023-02-03         50.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "## Read and analyze all CSV files at once\n",
    "result = duckdb.sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        SUM(sales) as total_sales\n",
    "    FROM 'data/*.csv'\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "\"\"\"\n",
    ").df()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e28fae",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://github.com/duckdb/duckdb).\n",
    "\n",
    "## DuckDB: Query Pandas DataFrames Faster with Columnar Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d31669",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e2a53-aae2-4849-9182-80ca95ea6026",
   "metadata": {},
   "source": [
    "When analyzing data with operations like GROUP BY, SUM, or AVG on specific columns, row-based storage results in reading unnecessary data and inefficient memory usage since entire rows must be loaded even when only a few columns are needed.\n",
    "\n",
    "Example using SQLite (row-based):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1171bdc1-0009-406a-be97-896eb0ed6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "customer = pd.DataFrame(\n",
    "    {\"id\": [1, 2, 3], \"name\": [\"Alex\", \"Ben\", \"Chase\"], \"age\": [25, 30, 35]}\n",
    ")\n",
    "\n",
    "## Load data to SQLite and query\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "customer.to_sql(\"customer\", conn, index=False)\n",
    "\n",
    "## Must read all columns internally even though we only need 'age'\n",
    "query = \"SELECT age FROM customer\"\n",
    "result = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d499ac-7323-493f-859e-7921a052fdc7",
   "metadata": {},
   "source": [
    "DuckDB uses columnar storage, allowing you to efficiently read and process only the columns needed for your analysis. This improves both query speed and memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae65be7f-0e4b-485e-b403-4a2943a24578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age\n",
       "0   25\n",
       "1   30\n",
       "2   35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "customer = pd.DataFrame(\n",
    "    {\"id\": [1, 2, 3], \"name\": [\"Alex\", \"Ben\", \"Chase\"], \"age\": [25, 30, 35]}\n",
    ")\n",
    "\n",
    "\n",
    "query = \"SELECT age FROM customer\"\n",
    "result = duckdb.sql(query).df()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7a04d-783d-4599-842e-4160b3cdd58d",
   "metadata": {},
   "source": [
    "In this example, DuckDB only needs to access the 'age' column in memory, while SQLite must read all columns ('id', 'name', 'age') internally even though only 'age' is selected. DuckDB also provides a simpler workflow by querying pandas DataFrames directly.\n",
    "\n",
    "[Link to DuckDB](https://github.com/duckdb/duckdb).\n",
    "\n",
    "## Query Nested Parquet Files Easily Using DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cef30a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install duckdb pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393c6f66",
   "metadata": {},
   "source": [
    "Analyzing large, nested Parquet files often requires pre-processing to flatten the data or writing complex scripts to extract nested fields, which can be time-consuming and error-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71596788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                       details\n",
      "0   1  {'name': 'Alice', 'age': 25}\n",
      "1   2    {'name': 'Bob', 'age': 30}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a nested dataset and save it as a Parquet file\n",
    "data = {\n",
    "    \"id\": [1, 2],\n",
    "    \"details\": [{\"name\": \"Alice\", \"age\": 25}, {\"name\": \"Bob\", \"age\": 30}],\n",
    "}\n",
    "\n",
    "## Convert to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save as a nested Parquet file\n",
    "df.to_parquet(\"nested_data.parquet\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78e86a35-9701-4b21-b2e0-cc19ce9da094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   name  age\n",
      "0   1  Alice   25\n",
      "1   2    Bob   30\n"
     ]
    }
   ],
   "source": [
    "## Read the DataFrame from Parquet file\n",
    "df = pd.read_parquet(\"nested_data.parquet\")\n",
    "\n",
    "# Create a new DataFrame with the flattened structure\n",
    "flat_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": df[\"id\"],\n",
    "        \"name\": [detail[\"name\"] for detail in df[\"details\"]],\n",
    "        \"age\": [detail[\"age\"] for detail in df[\"details\"]],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(flat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14629247",
   "metadata": {},
   "source": [
    "DuckDB allows querying nested Parquet files directly using SQL without needing to flatten or preprocess the data.\n",
    "\n",
    "To load and query nested Parquet files with DuckDB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a8b846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   name  age\n",
      "0   1  Alice   25\n",
      "1   2    Bob   30\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "## Query the nested Parquet file directly\n",
    "query_result = duckdb.query(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        id, \n",
    "        details.name AS name, \n",
    "        details.age AS age \n",
    "    FROM read_parquet('nested_data.parquet')\n",
    "\"\"\"\n",
    ").to_df()\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ac113",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "- `read_parquet('nested_data.parquet')` reads the nested Parquet file.\n",
    "- SQL syntax allows you to access nested fields using dot notation (e.g., `details.name`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a37519",
   "metadata": {},
   "source": [
    "The output is a flattened representation of the nested data, directly queried from the Parquet file without additional preprocessing.\n",
    "\n",
    "[Link to DuckDB](https://github.com/duckdb/duckdb)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
