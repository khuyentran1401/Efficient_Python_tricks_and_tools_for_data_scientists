---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.7
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# SQL Libraries

## SQL Analytics with DuckDB

```{python tags=c("hide-cell")}
# !pip install psycopg2 duckdb
```

SQL operations on data frames typically require setting up and maintaining separate database servers, adding complexity to analytical workflows:

```{python}
import pandas as pd

# Create sample data
df = pd.DataFrame(
    {"customer_id": range(1000), "revenue": range(1000), "segment": ["A", "B"] * 500}
)
```

```{python}
import psycopg2
from sqlalchemy import create_engine

# Create a connection to PostgreSQL
engine = create_engine("postgresql://postgres:postgres@localhost:5432/postgres")

# Write the DataFrame to a PostgreSQL table
df.to_sql("customers", engine, if_exists="replace", index=False)

# Execute SQL query against the PostgreSQL database
with engine.connect() as conn:
    result = pd.read_sql(
        """
        SELECT 
            segment,
            AVG(revenue) as avg_revenue,
            COUNT(*) as customer_count
        FROM customers
        GROUP BY segment
        ORDER BY avg_revenue DESC
    """,
        conn,
    )

print("Segment Analysis:")
print(result)
```

DuckDB simplifies this by providing direct SQL operations on DataFrames without server setup:

```{python}
import duckdb

# Direct SQL operations on DataFrame - no server needed
result = duckdb.sql(
    """
    SELECT 
        segment,
        AVG(revenue) as avg_revenue,
        COUNT(*) as customer_count
    FROM df
    GROUP BY segment
    ORDER BY avg_revenue DESC
"""
).df()

print("Segment Analysis:")
print(result)
```

[Link to DuckDB](https://github.com/duckdb/duckdb)

## Efficient SQL Operations with DuckDB on Pandas DataFrames

```{python}
# !pip install --quiet duckdb
```

```{python}
# !wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet
```

Using SQL with pandas empowers data scientists to leverage SQL's powerful querying capabilities alongside the data manipulation functionalities of pandas.

In the code below, aggregating data using DuckDB is nearly 6 times faster compared to aggregating with pandas.

```{python}
import duckdb
import pandas as pd

df = pd.read_parquet("lineitemsf1.snappy.parquet")
```

```{python}
# %%timeit
df.groupby("l_returnflag").agg(
    Sum=("l_extendedprice", "sum"),
    Min=("l_extendedprice", "min"),
    Max=("l_extendedprice", "max"),
    Avg=("l_extendedprice", "mean"),
)
```

```{python}
# %%timeit
duckdb.query(
    """
SELECT
      l_returnflag,
      SUM(l_extendedprice),
      MIN(l_extendedprice),
      MAX(l_extendedprice),
      AVG(l_extendedprice)
FROM df
GROUP BY
        l_returnflag
"""
).to_df()
```

[Link to DuckDB](https://github.com/duckdb/duckdb).

## Efficiently Handle Large Datasets with DuckDB and PyArrow

```{python}
# !pip install deltalake duckdb
```

```{python}
# !wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet
```

DuckDB leverages various optimizations for query execution, while PyArrow efficiently handles in-memory data processing and storage. Combining DuckDB and PyArrow allows you to efficiently process datasets larger than memory on a single machine.

In the code below, we convert a Delta Lake table with over 6 million rows to a pandas DataFrame and a PyArrow dataset, which are then used by DuckDB. 

Running DuckDB on PyArrow dataset is approximately 2906 times faster than running DuckDB on pandas. 

```{python}
import duckdb
import pandas as pd
from deltalake.writer import write_deltalake

df = pd.read_parquet("lineitemsf1.snappy.parquet")
write_deltalake("delta_lake", df)
```

```{python}
from deltalake import DeltaTable

table = DeltaTable("delta_lake")
```

```{python}
# %%timeit
quack = duckdb.df(table.to_pandas())
quack.filter("l_quantity > 50")
```

```{python}
# %%timeit
quack = duckdb.arrow(table.to_pyarrow_dataset())
quack.filter("l_quantity > 50")
```

[Link to DuckDB](https://github.com/duckdb/duckdb).

## Simplify CSV Data Management with DuckDB

```{python tags=c("hide-cell")}
# !pip install duckdb
```

Traditional database systems require a predefined table schema and a subsequent data import process when working with CSV data. 

To demonstrate this, let's create a CSV file called `customer.csv`:

```{python editable=TRUE, slideshow={'slide_type': ''}}
import pandas as pd

# Create a sample dataframe
data = {
    "name": ["Alice", "Bob", "Charlie", "David", "Eve"],
    "age": [25, 32, 45, 19, 38],
    "city": ["New York", "London", "Paris", "Berlin", "Tokyo"],
}

df = pd.DataFrame(data)

# Save the dataframe as a CSV file
df.to_csv("customers.csv", index=False)
```

To load this CSV file in Postgres, you need to run the following query:

```sql
-- Create the table
CREATE TABLE customers (
    name VARCHAR(100),
    age INT,
    city VARCHAR(100)
);

-- Load data from CSV
COPY customers
FROM 'customers.csv'
DELIMITER ','
CSV HEADER;
```

In contrast, DuckDB allows for direct reading of CSV files from disk, eliminating the need for explicit table creation and data loading.

```{python}
import duckdb

duckdb.sql("SELECT * FROM 'customers.csv'")
```

[Link to DuckDB](https://github.com/duckdb/duckdb).

## Multiple CSV Files Processing with DuckDB

```{python tags=c("hide-cell")}
# !pip install duckdb
```

Processing multiple CSV files is a common task in data analysis and engineering workflows. Traditionally, handling multiple CSV files requires writing loops or list comprehensions, which can be verbose and error-prone.


To demonstrate, let's assume we have 2 CSV files in the "folder" directory:

```{python}
from pathlib import Path

import pandas as pd

# Create example dataframe for first file
df1 = pd.DataFrame(
    {
        "Date": ["2023-01-01", "2023-01-02", "2023-01-03"],
        "Product": ["Laptop", "Phone", "Tablet"],
        "Sales": [1200, 800, 600],
    }
)

# Create example dataframe for second file
df2 = pd.DataFrame(
    {
        "Date": ["2023-02-01", "2023-02-02", "2023-02-03"],
        "Product": ["Laptop", "Monitor", "Mouse"],
        "Sales": [1500, 400, 50],
    }
)

Path("data").mkdir()
df1.to_csv("data/sales_jan.csv", index=False)
df2.to_csv("data/sales_feb.csv", index=False)
```

```{python}
import glob
import os

import pandas as pd

## Read all CSV files and concatenate them
csv_files = glob.glob("data/*.csv")
dfs = []

for file in csv_files:
    df = pd.read_csv(file)
    dfs.append(df)

## Concatenate all dataframes
combined_df = pd.concat(dfs, ignore_index=True)

## Perform grouping and aggregation
result = (
    combined_df.groupby("Date")["Sales"]
    .sum()
    .reset_index()
    .rename(columns={"Sales": "total_sales"})
    .sort_values("Date")
)

result
```

DuckDB simplifies reading multiple CSV files with a single line of code that efficiently processes the files:

```{python}
import duckdb

## Read and analyze all CSV files at once
result = duckdb.sql(
    """
    SELECT 
        date,
        SUM(sales) as total_sales
    FROM 'data/*.csv'
    GROUP BY date
    ORDER BY date
"""
).df()
result
```

[Link to DuckDB](https://github.com/duckdb/duckdb).

## DuckDB: Query Pandas DataFrames Faster with Columnar Storage

```{python editable=TRUE, slideshow={'slide_type': ''}, tags=c("hide-cell")}
# !pip install duckdb
```

When analyzing data with operations like GROUP BY, SUM, or AVG on specific columns, row-based storage results in reading unnecessary data and inefficient memory usage since entire rows must be loaded even when only a few columns are needed.

Example using SQLite (row-based):

```{python}
import sqlite3

import pandas as pd

customer = pd.DataFrame(
    {"id": [1, 2, 3], "name": ["Alex", "Ben", "Chase"], "age": [25, 30, 35]}
)

## Load data to SQLite and query
conn = sqlite3.connect(":memory:")
customer.to_sql("customer", conn, index=False)

## Must read all columns internally even though we only need 'age'
query = "SELECT age FROM customer"
result = pd.read_sql(query, conn)
```

DuckDB uses columnar storage, allowing you to efficiently read and process only the columns needed for your analysis. This improves both query speed and memory usage:

```{python}
import duckdb
import pandas as pd

customer = pd.DataFrame(
    {"id": [1, 2, 3], "name": ["Alex", "Ben", "Chase"], "age": [25, 30, 35]}
)


query = "SELECT age FROM customer"
result = duckdb.sql(query).df()
result
```

In this example, DuckDB only needs to access the 'age' column in memory, while SQLite must read all columns ('id', 'name', 'age') internally even though only 'age' is selected. DuckDB also provides a simpler workflow by querying pandas DataFrames directly.

[Link to DuckDB](https://github.com/duckdb/duckdb).


## Create Dynamic SQL Statements with Python string Template

```{python}
# %%writefile query.sql
SELECT
    *
FROM
    my_table
LIMIT
    $limit
WHERE
    start_date > $start_date;
```

```{python}
import pathlib
from string import Template

## Read the query from the file
query = pathlib.Path("query.sql").read_text()

## Substitute the placeholders with the values
t = Template(query)
substitutions = {"limit": 10, "start_date": "2021-01-01"}
print(t.substitute(substitutions))
```

Loading SQL tables into DataFrames allows you to analyze and preprocess the data using the rich functionality of pandas.

To read a SQL table into a pandas DataFrame, pass the database connection obtained from the SQLAlchemy Engine to the `pandas.read_sql` method.


```{python}
import pandas as pd
import sqlalchemy

# Create a SQLAlchemy engine
engine = sqlalchemy.create_engine(
    "postgresql://username:password@host:port/database_name"
)


## Read a SQL table into a DataFrame
df = pd.read_sql("SELECT * FROM table_name", engine)
```

## FugueSQL: Use SQL to Work with Pandas, Spark, and Dask DataFrames

```{python tags=c("hide-cell")}
# !pip install fugue
```

Do you like to use both Python and SQL to manipulate data? FugueSQL is an interface that allows users to use SQL to work with Pandas, Spark, and Dask DataFrames.

```{python}
import pandas as pd
from fugue_sql import fsql

input_df = pd.DataFrame({"price": [2, 1, 3], "fruit": (["apple", "banana", "orange"])})

query = """
SELECT price, fruit FROM input_df
WHERE price > 1
PRINT
"""

fsql(query).run()
```

[Link to fugue](https://github.com/fugue-project/fugue).

## SQLModel: Simplify SQL Database Interactions in Python

```{python tags=c("hide-cell")}
# !pip install sqlmodel
```

Interacting with SQL databases from Python code can often be challenging to write and comprehend.

```{python}
import sqlite3

## Connect to the database
conn = sqlite3.connect("users.db")

# Create a cursor object
cursor = conn.cursor()

## Define the SQL statement for creating the table
create_table_sql = """
    CREATE TABLE IF NOT EXISTS membership (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        username TEXT,
        age INTEGER,
        active INTEGER
    )
"""

## Execute the SQL statement to create the table
cursor.execute(create_table_sql)

## Define the SQL statement for inserting rows
insert_rows_sql = """
    INSERT INTO membership (username, age, active)
    VALUES (?, ?, ?)
"""

## Define the rows to be inserted
rows = [("John", 25, 1), ("Jane", 30, 0), ("Mike", 35, 1)]

## Execute the SQL statement for each row
for row in rows:
    cursor.execute(insert_rows_sql, row)

## Commit the changes to the database
conn.commit()

## Close the cursor and the database connection
cursor.close()
conn.close()
```

However, by utilizing SQLModel, you can harness Pydantic-like classes that leverage Python type annotations, making the code more intuitive to write and easier to understand.

```{python}
from typing import Optional

from sqlmodel import Field, Session, SQLModel, create_engine


class Membership(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    username: str
    age: int
    active: int


# age is converted from str to int through type coercion
user1 = Membership(username="John", age="25", active=1)
user2 = Membership(username="Jane", age="30", active=0)
user3 = Membership(username="Mike", age="35", active=1)


engine = create_engine("sqlite:///users.db")


SQLModel.metadata.create_all(engine)

with Session(engine) as session:
    session.add(user1)
    session.add(user2)
    session.add(user3)
    session.commit()
```

[Link to SQLModel](https://github.com/tiangolo/sqlmodel).

## SQLFluff: A Linter and Auto-Formatter for Your SQL Code

```{python tags=c("hide-cell")}
# !pip install sqlfluff
```

Inconsistent SQL formatting and style errors can reduce code readability and make code reviews more difficult.

SQLFluff solves this problem by automatically linting and fixing SQL code formatting issues across multiple dialects, including ANSI, MySQL, PostgreSQL, BigQuery, Databricks, Oracle, and more.

To demonstrate, let's create a sample SQL file named `sqlfluff_example.sql` with a simple `SELECT` statement.

```{python}
# %%writefile sqlfluff_example.sql
SELECT a+b  AS foo,
c AS bar from my_table
```

Next, run the SQLFluff linter on the `sqlfluff_example.sql` file using the PostgreSQL dialect. 

```bash
$ sqlfluff lint sqlfluff_example.sql --dialect postgres
```

```{python}
# !sqlfluff lint sqlfluff_example.sql --dialect postgres
```

To fix the style errors and inconsistencies found by the linter, we can run the `fix` command.

```bash
$ sqlfluff fix sqlfluff_example.sql --dialect postgres
```

```{python}
# %cat sqlfluff_example.sql
```

Now, the SQL code is formatted and readable.

[Link to SQLFluff](https://github.com/sqlfluff/sqlfluff).

## PostgresML: Integrate Machine Learning with PostgreSQL

If you want to seamlessly integrate machine learning models into your PostgreSQL database, use PostgresML. 

**Sentiment Analysis:**

```sql
SELECT pgml.transform(
    task   => 'text-classification',
    inputs => ARRAY[
        'I love how amazingly simple ML has become!', 
        'I hate doing mundane and thankless tasks. ☹️'
    ]
) AS positivity;
```

Output:

```python
                    positivity
------------------------------------------------------
[
    {"label": "POSITIVE", "score": 0.9995759129524232}, 
    {"label": "NEGATIVE", "score": 0.9903519749641418}
]
```
**Training a classification model**

Training: 

```sql
SELECT * FROM pgml.train(
    'My Classification Project',
    task => 'classification',
    relation_name => 'pgml.digits',
    y_column_name => 'target',
    algorithm => 'xgboost',
    hyperparams => '{
        "n_estimators": 25
    }'
);
```

Inference:

```sql
SELECT 
    target,
    pgml.predict('My Classification Project', image) AS prediction
FROM pgml.digits
LIMIT 5;
```

[Link to PostgresML](https://github.com/postgresml/postgresml).s

## sql-metadata: Extract Components From a SQL Statement in Python

```{python tags=c("hide-cell")}
# !pip install sql-metadata
```

If you want to extract specific components of a SQL statement for downstream Python tasks, use sql_metdata. 

```{python}
from sql_metadata import Parser

parsed_query = Parser(
    "SELECT foo.value as alias1 FROM foo JOIN bar ON foo.id = bar.id LIMIT 10"
)

print(f"Columns: {parsed_query.columns}")
print(f"Tables: {parsed_query.tables}")
print(f"Columns dict: {parsed_query.columns_dict}")
print(f"Aliases: {parsed_query.columns_aliases}")
print(f"Limit: {parsed_query.limit_and_offset}")
```

[Link to sql-metadata](https://github.com/macbre/sql-metadata).

## SQLGlot: Write Once, Run Anywhere SQL

```{python tags=c("hide-cell")}
# !pip install "sqlglot[rs]"
```

SQL dialects vary across databases, making it challenging to port queries between different database systems.

SQLGlot addresses this by providing a parser and transpiler supporting 21 dialects. This enables automatic SQL translation between systems, eliminating the need for manual query rewrites.

```{python}
import sqlglot
```

Convert a DuckDB-specific date formatting query into an equivalent query in Hive SQL:

```{python}
sqlglot.transpile("SELECT STRFTIME(x, '%y-%-m-%S')", read="duckdb", write="hive")[0]
```

Convert a SQL query to Spark SQL:

```{python}
## Spark SQL requires backticks (`) for delimited identifiers and uses `FLOAT` over `REAL`
sql = "SELECT id, name, CAST(price AS REAL) AS converted_price FROM products"

## Translates the query into Spark SQL, formats it, and delimits all of its identifiers
print(sqlglot.transpile(sql, write="spark", identify=True, pretty=True)[0])
```

[Link to SQLGlot](https://bit.ly/4dGyTmP).

## SQliteDict: Reducing SQLite Complexity with Dictionary-Style Operations

```{python tags=c("hide-cell")}
# !pip install sqlitedict
```

Writing data to SQLite directly and reading it back requires verbose SQL statements, schema definitions, and type handling, which can be tedious when storing complex Python objects or making frequent changes results in complex code.

```{python}
import sqlite3

products_to_update = [
    ("P1", "Laptop", 999.99, 50),
    ("P2", "Mouse", 29.99, 100),
    ("P3", "Keyboard", 59.99, 75),
]

with sqlite3.connect("example.db") as conn:
    cursor = conn.cursor()
    cursor.execute(
        """CREATE TABLE IF NOT EXISTS products 
                     (id TEXT PRIMARY KEY, name TEXT, price REAL, stock INTEGER)"""
    )
    cursor.executemany(
        """INSERT OR REPLACE INTO products (id, name, price, stock) 
                         VALUES (?, ?, ?, ?)""",
        products_to_update,
    )
```

```{python}
with sqlite3.connect("example.db") as conn:
    cursor = conn.cursor()
    cursor.execute("SELECT id, name, price, stock FROM products")
    for row in cursor.fetchall():
        product_data = {"name": row[1], "price": row[2], "stock": row[3]}
        print(f"{row[0]}={product_data}")
```

You can use SqliteDict to handle all the SQL and serialization complexity with a familiar dictionary interface:

```{python}
from sqlitedict import SqliteDict

products_to_update = {
    "P1": {"name": "Laptop", "price": 999.99, "stock": 50},
    "P2": {"name": "Mouse", "price": 29.99, "stock": 100},
    "P3": {"name": "Keyboard", "price": 59.99, "stock": 75},
}

with SqliteDict("example2.db") as db:
    # Update multiple records in a batch
    for product_id, product_data in products_to_update.items():
        db[product_id] = product_data

    # Single commit for all updates
    db.commit()
```

```{python}
with SqliteDict("example2.db") as db:
    for key, item in db.items():
        print(f"{key}={item}")
```

The example shows how SqliteDict eliminates the need for explicit SQL statements, cursor management, and serialization. The tool handles schema creation, data type conversion, and connection management internally, while providing a Pythonic interface. This is particularly useful when you need to frequently store and retrieve complex Python objects without dealing with the underlying database complexity.

[Link to SqliteDict](https://github.com/piskvorky/sqlitedict).
