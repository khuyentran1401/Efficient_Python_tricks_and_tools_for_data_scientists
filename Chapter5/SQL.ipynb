{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dd36fcd",
   "metadata": {},
   "source": [
    "## SQL Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "648764ef",
   "metadata": {},
   "source": [
    "### Create Dynamic SQL Statements with Python string Template"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0ded535",
   "metadata": {},
   "source": [
    "If you want to create dynamic SQL statements with Python variables, use Python string Template. \n",
    "\n",
    "string Template supports $-based substitutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4213f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing query.sql\n"
     ]
    }
   ],
   "source": [
    "%%writefile query.sql\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    my_table\n",
    "LIMIT\n",
    "    $limit\n",
    "WHERE\n",
    "    start_date > $start_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f445bfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "    *\n",
      "FROM\n",
      "    my_table\n",
      "LIMIT\n",
      "    10\n",
      "WHERE\n",
      "    start_date > 2021-01-01;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from string import Template\n",
    "\n",
    "# Read the query from the file\n",
    "query = pathlib.Path(\"query.sql\").read_text()\n",
    "\n",
    "# Substitute the placeholders with the values\n",
    "t = Template(query)\n",
    "substitutions = {\"limit\": 10, \"start_date\": \"2021-01-01\"}\n",
    "print(t.substitute(substitutions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7832641b",
   "metadata": {},
   "source": [
    "### Read Data From a SQL Table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94a9b5c5",
   "metadata": {},
   "source": [
    "Loading SQL tables into DataFrames allows you to analyze and preprocess the data using the rich functionality of pandas.\n",
    "\n",
    "To read a SQL table into a pandas DataFrame, pass the database connection obtained from the SQLAlchemy Engine to the `pandas.read_sql` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d09ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = sqlalchemy.create_engine(\n",
    "    \"postgresql://username:password@host:port/database_name\"\n",
    ")\n",
    "\n",
    "\n",
    "# Read a SQL table into a DataFrame\n",
    "df = pd.read_sql(\"SELECT * FROM table_name\", engine)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a14c44c",
   "metadata": {},
   "source": [
    "### FugueSQL: Use SQL to Work with Pandas, Spark, and Dask DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fe777",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install fugue "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e7c873c",
   "metadata": {},
   "source": [
    "Do you like to use both Python and SQL to manipulate data? FugueSQL is an interface that allows users to use SQL to work with Pandas, Spark, and Dask DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd91c3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PandasDataFrame\n",
      "price:long|fruit:str\n",
      "----------+---------\n",
      "2         |apple    \n",
      "3         |orange   \n",
      "Total count: 2\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrames()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fugue_sql import fsql\n",
    "\n",
    "input_df = pd.DataFrame({\"price\": [2, 1, 3], \"fruit\": ([\"apple\", \"banana\", \"orange\"])})\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT price, fruit FROM input_df\n",
    "WHERE price > 1\n",
    "PRINT\n",
    "\"\"\"\n",
    "\n",
    "fsql(query).run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93163c1a",
   "metadata": {},
   "source": [
    "[Link to fugue](https://github.com/fugue-project/fugue)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c99bfba9",
   "metadata": {},
   "source": [
    "### SQLModel: Simplify SQL Database Interactions in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb088e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install sqlmodel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1d64645",
   "metadata": {},
   "source": [
    "Interacting with SQL databases from Python code can often be challenging to write and comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556c62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect('users.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the SQL statement for creating the table\n",
    "create_table_sql = '''\n",
    "    CREATE TABLE IF NOT EXISTS membership (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        username TEXT,\n",
    "        age INTEGER,\n",
    "        active INTEGER\n",
    "    )\n",
    "'''\n",
    "\n",
    "# Execute the SQL statement to create the table\n",
    "cursor.execute(create_table_sql)\n",
    "\n",
    "# Define the SQL statement for inserting rows\n",
    "insert_rows_sql = '''\n",
    "    INSERT INTO membership (username, age, active)\n",
    "    VALUES (?, ?, ?)\n",
    "'''\n",
    "\n",
    "# Define the rows to be inserted\n",
    "rows = [\n",
    "    ('John', 25, 1),\n",
    "    ('Jane', 30, 0),\n",
    "    ('Mike', 35, 1)\n",
    "]\n",
    "\n",
    "# Execute the SQL statement for each row\n",
    "for row in rows:\n",
    "    cursor.execute(insert_rows_sql, row)\n",
    "\n",
    "# Commit the changes to the database\n",
    "conn.commit()\n",
    "\n",
    "# Close the cursor and the database connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd5364a6",
   "metadata": {},
   "source": [
    "However, by utilizing SQLModel, you can harness Pydantic-like classes that leverage Python type annotations, making the code more intuitive to write and easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a921b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from sqlmodel import Field, Session, SQLModel, create_engine\n",
    "\n",
    "\n",
    "class Membership(SQLModel, table=True):\n",
    "    id: Optional[int] = Field(default=None, primary_key=True)\n",
    "    username: str \n",
    "    age: int \n",
    "    active: int\n",
    "    \n",
    "# age is converted from str to int through type coercion\n",
    "user1 = Membership(username=\"John\", age=\"25\", active=1) \n",
    "user2 = Membership(username=\"Jane\", age=\"30\", active=0)\n",
    "user3 = Membership(username=\"Mike\", age=\"35\", active=1)\n",
    "\n",
    "\n",
    "engine = create_engine(\"sqlite:///users.db\")\n",
    "\n",
    "\n",
    "SQLModel.metadata.create_all(engine)\n",
    "\n",
    "with Session(engine) as session:\n",
    "    session.add(user1)\n",
    "    session.add(user2)\n",
    "    session.add(user3)\n",
    "    session.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54e86d04",
   "metadata": {},
   "source": [
    "[Link to SQLModel](https://github.com/tiangolo/sqlmodel)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d629481",
   "metadata": {},
   "source": [
    "### SQLFluff: A Linter and Auto-Formatter for Your SQL Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad118bdb",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install sqlfluff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c421db69",
   "metadata": {},
   "source": [
    "Linting helps ensure that code follows consistent style conventions, making it easier to understand and maintain. With SQLFluff, you can automatically lint your SQL code and correct most linting errors, freeing you up to focus on more important tasks.\n",
    "\n",
    "SQLFluff supports various SQL dialects such as ANSI, MySQL, PostgreSQL, BigQuery, Databricks, Oracle, Teradata, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4efc819a",
   "metadata": {},
   "source": [
    "In the code below, we use SQLFLuff to lint and fix the SQL code in the file `sqlfluff_example.sql`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sqlfluff_example.sql\n",
    "SELECT a+b  AS foo,\n",
    "c AS bar from my_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6f66b3c",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ sqlfluff lint sqlfluff_example.sql --dialect postgres\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b3b30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== [\u001b[30m\u001b[1msqlfluff_example.sql\u001b[0m] \u001b[31mFAIL\u001b[0m                            \n",
      "\u001b[34mL:   1 | P:   1 | LT09 | \u001b[0mSelect targets should be on a new line unless there is\n",
      "                       \u001b[34m| \u001b[0monly one select target.\n",
      "                       \u001b[34m| \u001b[0m[\u001b[30m\u001b[1mlayout.select_targets\u001b[0m]\n",
      "\u001b[34mL:   1 | P:   1 | ST06 | \u001b[0mSelect wildcards then simple targets before calculations\n",
      "                       \u001b[34m| \u001b[0mand aggregates. [\u001b[30m\u001b[1mstructure.column_order\u001b[0m]\n",
      "\u001b[34mL:   1 | P:   7 | LT02 | \u001b[0mExpected line break and indent of 4 spaces before 'a'.\n",
      "                       \u001b[34m| \u001b[0m[\u001b[30m\u001b[1mlayout.indent\u001b[0m]\n",
      "\u001b[34mL:   1 | P:   9 | LT01 | \u001b[0mExpected single whitespace between naked identifier and\n",
      "                       \u001b[34m| \u001b[0mbinary operator '+'. [\u001b[30m\u001b[1mlayout.spacing\u001b[0m]\n",
      "\u001b[34mL:   1 | P:  10 | LT01 | \u001b[0mExpected single whitespace between binary operator '+'\n",
      "                       \u001b[34m| \u001b[0mand naked identifier. [\u001b[30m\u001b[1mlayout.spacing\u001b[0m]\n",
      "\u001b[34mL:   1 | P:  11 | LT01 | \u001b[0mExpected only single space before 'AS' keyword. Found ' \n",
      "                       \u001b[34m| \u001b[0m'. [\u001b[30m\u001b[1mlayout.spacing\u001b[0m]\n",
      "\u001b[34mL:   2 | P:   1 | LT02 | \u001b[0mExpected indent of 4 spaces.\n",
      "                       \u001b[34m| \u001b[0m[\u001b[30m\u001b[1mlayout.indent\u001b[0m]\n",
      "\u001b[34mL:   2 | P:   9 | LT02 | \u001b[0mExpected line break and no indent before 'from'.\n",
      "                       \u001b[34m| \u001b[0m[\u001b[30m\u001b[1mlayout.indent\u001b[0m]\n",
      "\u001b[34mL:   2 | P:  10 | CP01 | \u001b[0mKeywords must be consistently upper case.\n",
      "                       \u001b[34m| \u001b[0m[\u001b[30m\u001b[1mcapitalisation.keywords\u001b[0m]\n",
      "All Finished ğŸ“œ ğŸ‰!\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!sqlfluff lint sqlfluff_example.sql --dialect postgres"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f99e21b",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ sqlfluff fix sqlfluff_example.sql --dialect postgres\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7099ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "    c AS bar,\n",
      "    a + b AS foo\n",
      "FROM my_table\n"
     ]
    }
   ],
   "source": [
    "%cat sqlfluff_example.sql"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3285265",
   "metadata": {},
   "source": [
    "[Link to SQLFluff](https://github.com/sqlfluff/sqlfluff)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e250b735",
   "metadata": {},
   "source": [
    "### PostgresML: Integrate Machine Learning with PostgreSQL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b134ef12",
   "metadata": {},
   "source": [
    "If you want to seamlessly integrate machine learning models into your PostgreSQL database, use PostgresML. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95e03050",
   "metadata": {},
   "source": [
    "**Sentiment Analysis:**\n",
    "\n",
    "```sql\n",
    "SELECT pgml.transform(\n",
    "    task   => 'text-classification',\n",
    "    inputs => ARRAY[\n",
    "        'I love how amazingly simple ML has become!', \n",
    "        'I hate doing mundane and thankless tasks. â˜¹ï¸'\n",
    "    ]\n",
    ") AS positivity;\n",
    "```\n",
    "\n",
    "Output:\n",
    "```python\n",
    "                    positivity\n",
    "------------------------------------------------------\n",
    "[\n",
    "    {\"label\": \"POSITIVE\", \"score\": 0.9995759129524232}, \n",
    "    {\"label\": \"NEGATIVE\", \"score\": 0.9903519749641418}\n",
    "]\n",
    "```\n",
    "**Training a classification model**\n",
    "\n",
    "Training: \n",
    "\n",
    "```sql\n",
    "SELECT * FROM pgml.train(\n",
    "    'My Classification Project',\n",
    "    task => 'classification',\n",
    "    relation_name => 'pgml.digits',\n",
    "    y_column_name => 'target',\n",
    "    algorithm => 'xgboost',\n",
    "    hyperparams => '{\n",
    "        \"n_estimators\": 25\n",
    "    }'\n",
    ");\n",
    "```\n",
    "\n",
    "Inference:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    target,\n",
    "    pgml.predict('My Classification Project', image) AS prediction\n",
    "FROM pgml.digits\n",
    "LIMIT 5;\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b701730",
   "metadata": {},
   "source": [
    "[Link to PostgresML](https://github.com/postgresml/postgresml)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5039c6a",
   "metadata": {},
   "source": [
    "### Efficient SQL Operations with DuckDB on Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79d7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c1a70d5",
   "metadata": {},
   "source": [
    "Using SQL with pandas empowers data scientists to leverage SQL's powerful querying capabilities alongside the data manipulation functionalities of pandas.\n",
    "\n",
    "However, traditional database systems often demand the management of a separate DBMS server, introducing additional complexity to the workflow.\n",
    "\n",
    "With DuckDB, you can efficiently run SQL operations on pandas DataFrames without the need to manage a separate DBMS server.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "mydf = pd.DataFrame({'a' : [1, 2, 3]})\n",
    "print(duckdb.query(\"SELECT SUM(a) FROM mydf\").to_df())\n",
    "```\n",
    "\n",
    "In the code below, aggregating data using DuckDB is nearly 6 times faster compared to aggregating with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd1c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "df = pd.read_parquet(\"lineitemsf1.snappy.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b11744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226 ms Â± 4.63 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "df.groupby('l_returnflag').agg(\n",
    "  Sum=('l_extendedprice', 'sum'),\n",
    "  Min=('l_extendedprice', 'min'),\n",
    "  Max=('l_extendedprice', 'max'),\n",
    "  Avg=('l_extendedprice', 'mean')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def27d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 ms Â± 2.41 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "duckdb.query(\"\"\"\n",
    "SELECT\n",
    "      l_returnflag,\n",
    "      SUM(l_extendedprice),\n",
    "      MIN(l_extendedprice),\n",
    "      MAX(l_extendedprice),\n",
    "      AVG(l_extendedprice)\n",
    "FROM df\n",
    "GROUP BY\n",
    "        l_returnflag\n",
    "\"\"\").to_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15ce6b19",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://github.com/duckdb/duckdb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44a362e1",
   "metadata": {},
   "source": [
    "### Efficiently Handle Large Datasets with DuckDB and PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610295ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deltalake duckdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ddd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/cwida/duckdb-data/releases/download/v1.0/lineitemsf1.snappy.parquet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af0090e9",
   "metadata": {},
   "source": [
    "DuckDB leverages various optimizations for query execution, while PyArrow efficiently handles in-memory data processing and storage. Combining DuckDB and PyArrow allows you to efficiently process datasets larger than memory on a single machine.\n",
    "\n",
    "In the code below, we convert a Delta Lake table with over 6 million rows to a pandas DataFrame and a PyArrow dataset, which are then used by DuckDB. \n",
    "\n",
    "Running DuckDB on PyArrow dataset is approximately 2906 times faster than running DuckDB on pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84460984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "from deltalake.writer import write_deltalake\n",
    "\n",
    "df = pd.read_parquet(\"lineitemsf1.snappy.parquet\")\n",
    "write_deltalake(\"delta_lake\", df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a98f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deltalake import DeltaTable\n",
    "\n",
    "table = DeltaTable(\"delta_lake\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4090c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 s Â± 108 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "quack = duckdb.df(table.to_pandas())\n",
    "quack.filter(\"l_quantity > 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70823a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "954 Âµs Â± 32.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "quack = duckdb.arrow(table.to_pyarrow_dataset())\n",
    "quack.filter(\"l_quantity > 50\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76baf2a2",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://github.com/duckdb/duckdb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300d0507",
   "metadata": {},
   "source": [
    "### Simplify CSV Data Management with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f897e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install duckdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eeb22f1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a sample dataframe\n",
    "data = {\n",
    "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\"],\n",
    "    \"age\": [25, 32, 45, 19, 38],\n",
    "    \"city\": [\"New York\", \"London\", \"Paris\", \"Berlin\", \"Tokyo\"],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "df.to_csv(\"customers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30018df",
   "metadata": {},
   "source": [
    "Traditional database systems, such as Postgres, require a predefined table schema and a subsequent data import process when working with CSV data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de593b3",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE TABLE customers (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    age INTEGER\n",
    ");\n",
    "\n",
    "COPY customers(name, age)\n",
    "FROM 'customers.csv'\n",
    "DELIMITER ','\n",
    "CSV HEADER;\n",
    "\n",
    "SELECT * FROM customers;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf4b63",
   "metadata": {},
   "source": [
    "In contrast, DuckDB allows for direct reading of CSV files from disk, eliminating the need for explicit table creation and data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5970f01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚  name   â”‚  age  â”‚   city   â”‚\n",
       "â”‚ varchar â”‚ int64 â”‚ varchar  â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ Alice   â”‚    25 â”‚ New York â”‚\n",
       "â”‚ Bob     â”‚    32 â”‚ London   â”‚\n",
       "â”‚ Charlie â”‚    45 â”‚ Paris    â”‚\n",
       "â”‚ David   â”‚    19 â”‚ Berlin   â”‚\n",
       "â”‚ Eve     â”‚    38 â”‚ Tokyo    â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "duckdb.sql(\"SELECT * FROM 'customers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b77a38",
   "metadata": {},
   "source": [
    "[Link to DuckDB](https://bit.ly/4dJxNHV)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d23ba4e",
   "metadata": {},
   "source": [
    "### sql-metadata: Extract Components From a SQL Statement in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6ba230",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install sql-metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348f085f",
   "metadata": {},
   "source": [
    "If you want to extract specific components of a SQL statement for downstream Python tasks, use sql_metdata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92904d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['foo.value', 'foo.id', 'bar.id']\n",
      "Tables: ['foo', 'bar']\n",
      "Columns dict: {'select': ['foo.value'], 'join': ['foo.id', 'bar.id']}\n",
      "Aliases: {'alias1': 'foo.value'}\n",
      "Limit: (10, 0)\n"
     ]
    }
   ],
   "source": [
    "from sql_metadata import Parser\n",
    "\n",
    "parsed_query = Parser(\n",
    "    \"SELECT foo.value as alias1 FROM foo JOIN bar ON foo.id = bar.id LIMIT 10\"\n",
    ")\n",
    "\n",
    "print(f\"Columns: {parsed_query.columns}\")\n",
    "print(f\"Tables: {parsed_query.tables}\")\n",
    "print(f\"Columns dict: {parsed_query.columns_dict}\")\n",
    "print(f\"Aliases: {parsed_query.columns_aliases}\")\n",
    "print(f\"Limit: {parsed_query.limit_and_offset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a96382",
   "metadata": {},
   "source": [
    "[Link to sql-metadata](https://github.com/macbre/sql-metadata)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e3474",
   "metadata": {},
   "source": [
    "### SQLGlot: Write Once, Run Anywhere SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d257860",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install \"sqlglot[rs]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7f225",
   "metadata": {},
   "source": [
    "SQL dialects vary across databases, making it challenging to port queries between different database systems.\n",
    "\n",
    "SQLGlot addresses this by providing a parser and transpiler supporting 21 dialects. This enables automatic SQL translation between systems, eliminating the need for manual query rewrites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72644346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ad01f",
   "metadata": {},
   "source": [
    "Convert a DuckDB-specific date formatting query into an equivalent query in Hive SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f46784d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT DATE_FORMAT(x, 'yy-M-ss')\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlglot.transpile(\"SELECT STRFTIME(x, '%y-%-m-%S')\", read=\"duckdb\", write=\"hive\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123e7f07",
   "metadata": {},
   "source": [
    "Convert a SQL query to Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "397512c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT\n",
      "  `id`,\n",
      "  `name`,\n",
      "  CAST(`price` AS FLOAT) AS `converted_price`\n",
      "FROM `products`\n"
     ]
    }
   ],
   "source": [
    "# Spark SQL requires backticks (`) for delimited identifiers and uses `FLOAT` over `REAL`\n",
    "sql = \"SELECT id, name, CAST(price AS REAL) AS converted_price FROM products\"\n",
    "\n",
    "# Translates the query into Spark SQL, formats it, and delimits all of its identifiers\n",
    "print(sqlglot.transpile(sql, write=\"spark\", identify=True, pretty=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a983e21",
   "metadata": {},
   "source": [
    "[Link to SQLGlot](https://bit.ly/4dGyTmP)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
