{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63364389",
   "metadata": {},
   "source": [
    "## Large Language Model (LLM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ae67f3c",
   "metadata": {},
   "source": [
    "### Simplify LLM Integration with Magentic's @prompt Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ccb1a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install magentic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb377c57",
   "metadata": {},
   "source": [
    "To enhance your code's natural language skills with LLM effortlessly, try magentic. \n",
    "\n",
    "With magentic, you can use the `@prompt` decorator to create functions that return organized LLM results, keeping your code neat and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941b061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23398199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yo dude, how's it going?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from magentic import prompt\n",
    "\n",
    "\n",
    "@prompt('Add more \"dude\"ness to: {phrase}')\n",
    "def dudeify(phrase: str) -> str:\n",
    "    ...  # No function body as this is never executed\n",
    "\n",
    "\n",
    "dudeify(\"Hello, how are you?\")\n",
    "# \"Hey, dude! What's up? How's it going, my man?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5abd3e94",
   "metadata": {},
   "source": [
    "The `@prompt` decorator will consider the return type annotation, including those supported by pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7adc676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MilkTea(tea='green tea', sweetness_percentage=100.0, topping='boba')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from magentic import prompt, FunctionCall\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class MilkTea(BaseModel):\n",
    "    tea: str\n",
    "    sweetness_percentage: float\n",
    "    topping: str\n",
    "\n",
    "\n",
    "@prompt(\"Create a milk tea with the following tea {tea}.\")\n",
    "def create_milk_tea(tea: str) -> MilkTea:\n",
    "    ...\n",
    "\n",
    "\n",
    "create_milk_tea(\"green tea\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca84b0ed",
   "metadata": {},
   "source": [
    "The `@prompt` decorator also considers a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f205c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frothing milk to 60 F with texture foamy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def froth_milk(temperature: int, texture: Literal[\"foamy\", \"hot\", \"cold\"]) -> str:\n",
    "    \"\"\"Froth the milk to the desired temperature and texture.\"\"\"\n",
    "    return f\"Frothing milk to {temperature} F with texture {texture}\"\n",
    "\n",
    "\n",
    "@prompt(\n",
    "    \"Prepare the milk for my {coffee_type}\",\n",
    "    functions=[froth_milk],\n",
    ")\n",
    "def configure_coffee(coffee_type: str) -> FunctionCall[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "output = configure_coffee(\"latte!\")\n",
    "output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded189dc",
   "metadata": {},
   "source": [
    "[Link to magentic](https://github.com/jackmpcollins/magentic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087fd8f",
   "metadata": {},
   "source": [
    "### Outlines: Ensuring Consistent Outputs from Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2576146",
   "metadata": {},
   "source": [
    "The Outlines library enables controlling the outputs of language models. This makes the outputs more predictable, ensuring the reliability of systems using large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9a563",
   "metadata": {},
   "source": [
    "```python\n",
    "import outlines\n",
    "\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"\"\"You are a sentiment-labelling assistant.\n",
    "Is the following review positive or negative?\n",
    "\n",
    "Review: This restaurant is just awesome!\n",
    "\"\"\"\n",
    "# Only return a choice between multiple possibilities\n",
    "answer = outlines.generate.choice(model, [\"Positive\", \"Negative\"])(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c1bb6",
   "metadata": {},
   "source": [
    "```python\n",
    "# Only return integers or floats\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"1+1=\"\n",
    "answer = outlines.generate.format(model, int)(prompt)\n",
    "\n",
    "prompt = \"sqrt(2)=\"\n",
    "answer = outlines.generate.format(model, float)(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88991d52",
   "metadata": {},
   "source": [
    "[Link to Outlines](https://github.com/outlines-dev/outlines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff519c06",
   "metadata": {},
   "source": [
    "### Mirascope: Extract Structured Data Extraction From LLM Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12a009",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mirascope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9cc6f",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are powerful at producing human-like text, but their outputs lack structure, which can limit their usefulness in many practical applications that require organized data.\n",
    "\n",
    "Mirascope offers a solution by enabling the extraction of structured information from LLM outputs reliably.\n",
    "\n",
    "The following code uses Mirascope to extract meeting details such as topic, date, time, and participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c7cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f19428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic='Upcoming product launch' date='June 15th' time='3 PM' participants=['John', 'Sarah', 'Mike']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Type\n",
    "from pydantic import BaseModel\n",
    "from mirascope.openai import OpenAIExtractor\n",
    "\n",
    "\n",
    "class MeetingDetails(BaseModel):\n",
    "    topic: str\n",
    "    date: str\n",
    "    time: str\n",
    "    participants: List[str]\n",
    "\n",
    "\n",
    "class MeetingExtractor(OpenAIExtractor[MeetingDetails]):\n",
    "    extract_schema: Type[MeetingDetails] = MeetingDetails\n",
    "    prompt_template = \"\"\"\n",
    "    Extract the meeting details from the following description:\n",
    "    {description}\n",
    "    \"\"\"\n",
    "\n",
    "    description: str\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "description = \"Discuss the upcoming product launch on June 15th at 3 PM with John, Sarah, and Mike.\"\n",
    "meeting_details = MeetingExtractor(description=description).extract()\n",
    "assert isinstance(meeting_details, MeetingDetails)\n",
    "print(meeting_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d2dd5",
   "metadata": {},
   "source": [
    "[Link to Mirascope](https://bit.ly/4bkciv3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc77a1",
   "metadata": {},
   "source": [
    "### Maximize Accuracy and Relevance with External Data and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518900b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U mirascope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5be483",
   "metadata": {},
   "source": [
    "Combining external data and an LLM offers the best of both worlds: accuracy and relevance. External data provides up-to-date information, while an LLM can generate text based on input prompts. Together, they enable a system to respond helpfully to a wider range of queries.\n",
    "\n",
    "Mirascope simplifies this combination with Pythonic code. In the example below, we use an LLM to process natural language prompts and query the database for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "620f3cf4",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created with sample data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Set up database and table for the example below\n",
    "conn = sqlite3.connect(\"grocery.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the 'grocery_items' table\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS grocery_items (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT NOT NULL,\n",
    "        category TEXT NOT NULL,\n",
    "        price REAL NOT NULL\n",
    "    )\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Insert some sample data\n",
    "items = [\n",
    "    (\"apple\", \"Fruits\", 0.75),\n",
    "    (\"banana\", \"Fruits\", 0.50),\n",
    "    (\"carrot\", \"Vegetables\", 1.20),\n",
    "]\n",
    "\n",
    "cursor.executemany(\n",
    "    \"INSERT INTO grocery_items (name, category, price) VALUES (?, ?, ?)\", items\n",
    ")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"Database created with sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd431ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da316438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The price for banana is 0.5.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mirascope.openai import OpenAICall, OpenAICallParams\n",
    "import sqlite3\n",
    "\n",
    "# Assume you have a SQLite database with a 'grocery_items' table\n",
    "conn = sqlite3.connect(\"grocery.db\")\n",
    "\n",
    "\n",
    "def get_item_info(table: str, item: str, info: str) -> dict:\n",
    "    \"\"\"Get `info` from the `table` table based on `item`.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        row = cursor.execute(\n",
    "            f\"SELECT {info} FROM {table} WHERE name = ?\", (item,)\n",
    "        ).fetchone()\n",
    "        return f\"The {info} for {item} is {row[0]}.\"\n",
    "    except TypeError:\n",
    "        return f\"Sorry but {item} doesn't exist in the database.\"\n",
    "\n",
    "\n",
    "class GroceryItemQuery(OpenAICall):\n",
    "    prompt_template = \"\"\"\n",
    "    SYSTEM:\n",
    "    Your task is to query a database given a user's input.\n",
    "\n",
    "    USER:\n",
    "    {input}\n",
    "    \"\"\"\n",
    "    input: str\n",
    "    call_params = OpenAICallParams(tools=[get_item_info])\n",
    "\n",
    "\n",
    "text = \"What's the price for banana in the grocery_items table?\"\n",
    "query_tool = GroceryItemQuery(input=text).call().tool\n",
    "result = query_tool.fn(**query_tool.args)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff418e55",
   "metadata": {},
   "source": [
    "[Link to Mirascope](https://bit.ly/4awfNhg)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
