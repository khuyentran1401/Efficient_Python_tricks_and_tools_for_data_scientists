{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63364389",
   "metadata": {},
   "source": [
    "## Large Language Model (LLM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ae67f3c",
   "metadata": {},
   "source": [
    "### Simplify LLM Integration with Magentic's @prompt Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ccb1a",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install magentic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb377c57",
   "metadata": {},
   "source": [
    "To enhance your code's natural language skills with LLM effortlessly, try magentic. \n",
    "\n",
    "With magentic, you can use the `@prompt` decorator to create functions that return organized LLM results, keeping your code neat and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941b061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"sk-...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23398199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yo dude, how's it going?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from magentic import prompt\n",
    "\n",
    "\n",
    "@prompt('Add more \"dude\"ness to: {phrase}')\n",
    "def dudeify(phrase: str) -> str:\n",
    "    ...  # No function body as this is never executed\n",
    "\n",
    "\n",
    "dudeify(\"Hello, how are you?\")\n",
    "# \"Hey, dude! What's up? How's it going, my man?\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5abd3e94",
   "metadata": {},
   "source": [
    "The `@prompt` decorator will consider the return type annotation, including those supported by pydantic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7adc676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MilkTea(tea='green tea', sweetness_percentage=100.0, topping='boba')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from magentic import prompt, FunctionCall\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class MilkTea(BaseModel):\n",
    "    tea: str\n",
    "    sweetness_percentage: float\n",
    "    topping: str\n",
    "\n",
    "\n",
    "@prompt(\"Create a milk tea with the following tea {tea}.\")\n",
    "def create_milk_tea(tea: str) -> MilkTea:\n",
    "    ...\n",
    "\n",
    "\n",
    "create_milk_tea(\"green tea\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca84b0ed",
   "metadata": {},
   "source": [
    "The `@prompt` decorator also considers a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f205c95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Frothing milk to 60 F with texture foamy'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def froth_milk(temperature: int, texture: Literal[\"foamy\", \"hot\", \"cold\"]) -> str:\n",
    "    \"\"\"Froth the milk to the desired temperature and texture.\"\"\"\n",
    "    return f\"Frothing milk to {temperature} F with texture {texture}\"\n",
    "\n",
    "\n",
    "@prompt(\n",
    "    \"Prepare the milk for my {coffee_type}\",\n",
    "    functions=[froth_milk],\n",
    ")\n",
    "def configure_coffee(coffee_type: str) -> FunctionCall[str]:\n",
    "    ...\n",
    "\n",
    "\n",
    "output = configure_coffee(\"latte!\")\n",
    "output()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ded189dc",
   "metadata": {},
   "source": [
    "[Link to magentic](https://github.com/jackmpcollins/magentic)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087fd8f",
   "metadata": {},
   "source": [
    "### Outlines: Ensuring Consistent Outputs from Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2576146",
   "metadata": {},
   "source": [
    "The Outlines library enables controlling the outputs of language models. This makes the outputs more predictable, ensuring the reliability of systems using large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9a563",
   "metadata": {},
   "source": [
    "```python\n",
    "import outlines\n",
    "\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"\"\"You are a sentiment-labelling assistant.\n",
    "Is the following review positive or negative?\n",
    "\n",
    "Review: This restaurant is just awesome!\n",
    "\"\"\"\n",
    "# Only return a choice between multiple possibilities\n",
    "answer = outlines.generate.choice(model, [\"Positive\", \"Negative\"])(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042c1bb6",
   "metadata": {},
   "source": [
    "```python\n",
    "# Only return integers or floats\n",
    "model = outlines.models.transformers(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "prompt = \"1+1=\"\n",
    "answer = outlines.generate.format(model, int)(prompt)\n",
    "\n",
    "prompt = \"sqrt(2)=\"\n",
    "answer = outlines.generate.format(model, float)(prompt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88991d52",
   "metadata": {},
   "source": [
    "[Link to Outlines](https://github.com/outlines-dev/outlines)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff519c06",
   "metadata": {},
   "source": [
    "### Mirascope: Extract Structured Data Extraction From LLM Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc12a009",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install mirascope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9cc6f",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are powerful at producing human-like text, but their outputs lack structure, which can limit their usefulness in many practical applications that require organized data.\n",
    "\n",
    "Mirascope offers a solution by enabling the extraction of structured information from LLM outputs reliably.\n",
    "\n",
    "The following code uses Mirascope to extract meeting details such as topic, date, time, and participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c7cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f19428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic='Upcoming product launch' date='June 15th' time='3 PM' participants=['John', 'Sarah', 'Mike']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Type\n",
    "from pydantic import BaseModel\n",
    "from mirascope.openai import OpenAIExtractor\n",
    "\n",
    "\n",
    "class MeetingDetails(BaseModel):\n",
    "    topic: str\n",
    "    date: str\n",
    "    time: str\n",
    "    participants: List[str]\n",
    "\n",
    "\n",
    "class MeetingExtractor(OpenAIExtractor[MeetingDetails]):\n",
    "    extract_schema: Type[MeetingDetails] = MeetingDetails\n",
    "    prompt_template = \"\"\"\n",
    "    Extract the meeting details from the following description:\n",
    "    {description}\n",
    "    \"\"\"\n",
    "\n",
    "    description: str\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "description = \"Discuss the upcoming product launch on June 15th at 3 PM with John, Sarah, and Mike.\"\n",
    "meeting_details = MeetingExtractor(description=description).extract()\n",
    "assert isinstance(meeting_details, MeetingDetails)\n",
    "print(meeting_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d2dd5",
   "metadata": {},
   "source": [
    "[Link to Mirascope](https://bit.ly/4bkciv3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc77a1",
   "metadata": {},
   "source": [
    "### Maximize Accuracy and Relevance with External Data and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1518900b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U mirascope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5be483",
   "metadata": {},
   "source": [
    "Combining external data and an LLM offers the best of both worlds: accuracy and relevance. External data provides up-to-date information, while an LLM can generate text based on input prompts. Together, they enable a system to respond helpfully to a wider range of queries.\n",
    "\n",
    "Mirascope simplifies this combination with Pythonic code. In the example below, we use an LLM to process natural language prompts and query the database for data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "620f3cf4",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created with sample data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Set up database and table for the example below\n",
    "conn = sqlite3.connect(\"grocery.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create the 'grocery_items' table\n",
    "cursor.execute(\n",
    "    \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS grocery_items (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        name TEXT NOT NULL,\n",
    "        category TEXT NOT NULL,\n",
    "        price REAL NOT NULL\n",
    "    )\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Insert some sample data\n",
    "items = [\n",
    "    (\"apple\", \"Fruits\", 0.75),\n",
    "    (\"banana\", \"Fruits\", 0.50),\n",
    "    (\"carrot\", \"Vegetables\", 1.20),\n",
    "]\n",
    "\n",
    "cursor.executemany(\n",
    "    \"INSERT INTO grocery_items (name, category, price) VALUES (?, ?, ?)\", items\n",
    ")\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"Database created with sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd431ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "da316438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The price for banana is 0.5.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mirascope.openai import OpenAICall, OpenAICallParams\n",
    "import sqlite3\n",
    "\n",
    "# Assume you have a SQLite database with a 'grocery_items' table\n",
    "conn = sqlite3.connect(\"grocery.db\")\n",
    "\n",
    "\n",
    "def get_item_info(table: str, item: str, info: str) -> dict:\n",
    "    \"\"\"Get `info` from the `table` table based on `item`.\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        row = cursor.execute(\n",
    "            f\"SELECT {info} FROM {table} WHERE name = ?\", (item,)\n",
    "        ).fetchone()\n",
    "        return f\"The {info} for {item} is {row[0]}.\"\n",
    "    except TypeError:\n",
    "        return f\"Sorry but {item} doesn't exist in the database.\"\n",
    "\n",
    "\n",
    "class GroceryItemQuery(OpenAICall):\n",
    "    prompt_template = \"\"\"\n",
    "    SYSTEM:\n",
    "    Your task is to query a database given a user's input.\n",
    "\n",
    "    USER:\n",
    "    {input}\n",
    "    \"\"\"\n",
    "    input: str\n",
    "    call_params = OpenAICallParams(tools=[get_item_info])\n",
    "\n",
    "\n",
    "text = \"What's the price for banana in the grocery_items table?\"\n",
    "query_tool = GroceryItemQuery(input=text).call().tool\n",
    "result = query_tool.fn(**query_tool.args)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff418e55",
   "metadata": {},
   "source": [
    "[Link to Mirascope](https://bit.ly/4awfNhg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b97ce-89fa-4789-ab7a-684bb6e86544",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb 'numpy<2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560d4baa-ac88-4e0c-b984-d3a0db13b140",
   "metadata": {},
   "source": [
    "Managing and querying large collections of text data using traditional databases or simple search methods results in poor semantic matches and complex implementation. This causes difficulties in building AI applications that need to find contextually similar content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54481d9c-2dde-4f6a-96ba-0fe7166e5800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The weather is great today']\n"
     ]
    }
   ],
   "source": [
    "# Traditional approach with basic text search\n",
    "documents = [\n",
    "    \"The weather is great today\",\n",
    "    \"The climate is excellent\",\n",
    "    \"Machine learning models are fascinating\",\n",
    "]\n",
    "\n",
    "# Search by exact match or simple substring\n",
    "query = \"How's the weather?\"\n",
    "results = [doc for doc in documents if \"weather\" in doc.lower()]\n",
    "\n",
    "# Only finds documents with exact word \"weather\", misses semantically similar ones\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ccc52-93a4-4c1c-9e53-056bf27af0eb",
   "metadata": {},
   "source": [
    "You can use Chroma to easily store and query documents using their semantic meaning through embeddings. The tool handles the embedding creation and similarity search automatically, making it simple to build AI applications with semantic search capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93f64f0f-e706-4b43-82d4-80c9fa60ed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize client and collection\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\"documents\")\n",
    "\n",
    "# Add documents\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"The weather is great today\",\n",
    "        \"The climate is excellent\",\n",
    "        \"Machine learning models are fascinating\"\n",
    "    ],\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\"]\n",
    ")\n",
    "\n",
    "# Query semantically similar documents\n",
    "results = collection.query(\n",
    "    query_texts=[\"How's the weather?\"],\n",
    "    n_results=2\n",
    ")\n",
    "# Returns both weather and climate documents due to semantic similarity\n",
    "print(results['documents'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e812d7-74e1-49ed-9f32-f53d84932e48",
   "metadata": {},
   "source": [
    "The example shows how Chroma automatically converts text into embeddings and finds semantically similar documents, even when they don't share exact words. This makes it much easier to build applications that can understand the meaning of text, not just match keywords.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
