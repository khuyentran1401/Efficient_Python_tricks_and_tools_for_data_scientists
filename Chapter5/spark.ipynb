{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b25228f0",
   "metadata": {},
   "source": [
    "## PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc16c3",
   "metadata": {},
   "source": [
    "### Distributed Data Joining with Shuffle Joins in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb120452",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install 'pyspark[sql]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef45c6",
   "metadata": {},
   "source": [
    "Shuffle joins in PySpark distribute data across worker nodes, enabling parallel processing and improving performance compared to single-node joins. By dividing data into partitions and joining each partition simultaneously, shuffle joins can handle large datasets efficiently.\n",
    "\n",
    "![](../img/shuffle_join_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a95df",
   "metadata": {},
   "source": [
    "Here's an example of performing a shuffle join in PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62748cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ad00bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----------+------+\n",
      "| id|name|department|salary|\n",
      "+---+----+----------+------+\n",
      "|  1|John|     Sales|  5000|\n",
      "|  2|Jane| Marketing|  6000|\n",
      "+---+----+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "employees = spark.createDataFrame(\n",
    "    [(1, \"John\", \"Sales\"), (2, \"Jane\", \"Marketing\"), (3, \"Bob\", \"Engineering\")],\n",
    "    [\"id\", \"name\", \"department\"],\n",
    ")\n",
    "\n",
    "salaries = spark.createDataFrame([(1, 5000), (2, 6000), (4, 7000)], [\"id\", \"salary\"])\n",
    "\n",
    "# Perform an inner join using the join key \"id\"\n",
    "joined_df = employees.join(salaries, \"id\", \"inner\")\n",
    "\n",
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024e48ef",
   "metadata": {},
   "source": [
    "In this example, PySpark performs a shuffle join behind the scenes to combine the two DataFrames. The process involves partitioning the data based on the join key (\"id\"), shuffling the partitions across the worker nodes, performing local joins on each worker node, and finally merging the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e0e0a",
   "metadata": {},
   "source": [
    "### PySpark DataFrame Transformations: select vs withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322859d8",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install 'pyspark[sql]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbf287e",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/02 06:01:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1dbabe",
   "metadata": {},
   "source": [
    "PySpark's `select` and `withColumn` both can be used to add or modify existing columns. However, their behavior are different.\n",
    "\n",
    "To demonstrate this, let's start with creating a sample DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9561afc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+\n",
      "| name|age|         city|\n",
      "+-----+---+-------------+\n",
      "|Alice| 28|     New York|\n",
      "|  Bob| 35|San Francisco|\n",
      "+-----+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, upper\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"Alice\", 28, \"New York\"),\n",
    "    (\"Bob\", 35, \"San Francisco\"),\n",
    "]\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\", \"city\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eba50a",
   "metadata": {},
   "source": [
    "\n",
    "`select` only keeps specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "278dac0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|   upper_city|\n",
      "+-------------+\n",
      "|     NEW YORK|\n",
      "|SAN FRANCISCO|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_select = df.select(upper(col(\"city\")).alias(\"upper_city\"))\n",
    "df_select.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1dec31",
   "metadata": {},
   "source": [
    "`withColumn` retains all original columns plus the new/modified one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b0a9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------------+-------------+\n",
      "| name|age|         city|   upper_city|\n",
      "+-----+---+-------------+-------------+\n",
      "|Alice| 28|     New York|     NEW YORK|\n",
      "|  Bob| 35|San Francisco|SAN FRANCISCO|\n",
      "+-----+---+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_withColumn = df.withColumn('upper_city', upper(col('city')))\n",
    "df_withColumn.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc8623",
   "metadata": {},
   "source": [
    "### Spark DataFrame: Avoid Out-of-Memory Errors with Lazy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6809b440",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install 'pyspark[sql]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7e8e068",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Create a parquet file for testing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set the number of rows\n",
    "num_rows = 1_000_000\n",
    "\n",
    "# Create a categorical column with three categories as strings\n",
    "category_column = np.random.choice([\"a\", \"b\", \"c\"], size=num_rows)\n",
    "\n",
    "# Create two numerical columns\n",
    "num_col1 = np.random.randint(0, 100, num_rows)\n",
    "num_col2 = np.random.randint(0, 100, num_rows)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(\n",
    "    {\"cat\": category_column, \"val1\": num_col1, \"val2\": num_col2}\n",
    ")\n",
    "\n",
    "df.to_parquet(\"test_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bfb86b",
   "metadata": {},
   "source": [
    "Retrieving all rows from a large dataset into memory can cause out-of-memory errors. When creating a Spark DataFrame, computations are not executed until the `collect()` method is invoked. This allows you to reduce the size of the DataFrame through operations such as filtering or aggregating before bringing them into memory. \n",
    "\n",
    "As a result, you can manage memory usage more efficiently and avoid unnecessary computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8b69c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d04cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+\n",
      "|cat|val1|val2|\n",
      "+---+----+----+\n",
      "|  b|   0|  34|\n",
      "|  a|  58|  12|\n",
      "|  c|  24|  72|\n",
      "|  a|  20|  58|\n",
      "|  b|  13|  17|\n",
      "+---+----+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet('test_data.parquet')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01181341",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df = df.filter(df[\"val1\"] >= 50).groupBy(\"cat\").agg({\"val2\": \"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305ddc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(cat='c', avg(val2)=49.54095055783208),\n",
       " Row(cat='b', avg(val2)=49.46593810642427),\n",
       " Row(cat='a', avg(val2)=49.52092805080465)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c115794a",
   "metadata": {},
   "source": [
    "### Pandas-Friendly Big Data Processing with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1917668",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"pyspark[pandas_on_spark]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0190c8e",
   "metadata": {},
   "source": [
    "Spark enables scaling of your pandas workloads across multiple nodes. However, learning PySpark syntax can be daunting for pandas users. \n",
    "\n",
    "Pandas API on Spark enables leveraging Spark's capabilities for big data while retaining a familiar pandas-like syntax.\n",
    "\n",
    "The following code compares the syntax between PySpark and the Pandas API on Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631d204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19b336",
   "metadata": {},
   "source": [
    "Pandas API on Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspark.pandas as ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3136c2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "psdf = ps.DataFrame(\n",
    "    {\n",
    "        \"A\": [\"foo\", \"bar\", \"foo\"],\n",
    "        \"B\": [\"one\", \"one\", \"two\"],\n",
    "        \"C\": [0.1, 0.3, 0.5],\n",
    "        \"D\": [0.2, 0.4, 0.6],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313537cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foo</td>\n",
       "      <td>one</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bar</td>\n",
       "      <td>one</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foo</td>\n",
       "      <td>two</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C    D\n",
       "0  foo  one  0.1  0.2\n",
       "1  bar  one  0.3  0.4\n",
       "2  foo  two  0.5  0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "psdf.sort_values(by='B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e6506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>foo</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bar</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       C    D\n",
       "A            \n",
       "foo  0.6  0.8\n",
       "bar  0.3  0.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "psdf.groupby('A').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba075e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foo</td>\n",
       "      <td>two</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C    D\n",
       "2  foo  two  0.5  0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "psdf.query(\"C > 0.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83be61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C    D\n",
       "0  0.1  0.2\n",
       "1  0.3  0.4\n",
       "2  0.5  0.6"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "psdf[[\"C\", \"D\"]].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa28ac06",
   "metadata": {},
   "source": [
    "PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52727ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import abs\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20263c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd65b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_data = spark.createDataFrame([\n",
    "    (\"foo\", \"one\", 0.1, 0.2),\n",
    "    (\"bar\", \"one\", 0.3, 0.4),\n",
    "    (\"foo\", \"two\", 0.5, 0.6),\n",
    "], [\"A\", \"B\", \"C\", \"D\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "|foo|one|0.1|0.2|\n",
      "|bar|one|0.3|0.4|\n",
      "|foo|two|0.5|0.6|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.sort(col('B')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78b6c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|  A|sum(C)|sum(D)|\n",
      "+---+------+------+\n",
      "|foo|   0.6|   0.8|\n",
      "|bar|   0.3|   0.4|\n",
      "+---+------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_data.groupBy('A').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa63abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  A|  B|  C|  D|\n",
      "+---+---+---+---+\n",
      "|foo|two|0.5|0.6|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_data.filter(col('C') > 0.4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f340ef77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[C: double, D: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark_data.select(abs(spark_data[\"C\"]).alias(\"C\"), abs(spark_data[\"D\"]).alias(\"D\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d96783",
   "metadata": {},
   "source": [
    "### PySpark SQL: Enhancing Reusability with Parameterized Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d296b294",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install \"pyspark[sql]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc2bc2",
   "metadata": {},
   "source": [
    "In PySpark, parametrized queries enable the same query structure to be reused with different inputs, without rewriting the SQL.\n",
    "\n",
    "Additionally, they safeguard against SQL injection attacks by treating input data as parameters rather than as executable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd \n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5f3c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|item_id|price|\n",
      "+-------+-----+\n",
      "|      1|    4|\n",
      "|      2|    2|\n",
      "|      3|    5|\n",
      "|      4|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame\n",
    "item_price_pandas = pd.DataFrame({\"item_id\": [1, 2, 3, 4], \"price\": [4, 2, 5, 1]})\n",
    "item_price = spark.createDataFrame(item_price_pandas)\n",
    "item_price.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90976e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|item_id|price|\n",
      "+-------+-----+\n",
      "|      1|    4|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"SELECT item_id, price \n",
    "FROM {item_price} \n",
    "WHERE item_id = {id_val} \n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query, id_val=1, item_price=item_price).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44634ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|item_id|price|\n",
      "+-------+-----+\n",
      "|      2|    2|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query, id_val=2, item_price=item_price).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a4e7a",
   "metadata": {},
   "source": [
    "### Working with Arrays Made Easier in Spark 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cebbb84",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install \"pyspark[sql]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c70dd9",
   "metadata": {},
   "source": [
    "Spark 3.5 added new array helper functions that simplify the process of working with array data. Below are a few examples showcasing these new array functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803bd642",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "103201f9",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|customer|  orders|\n",
      "+--------+--------+\n",
      "|    Alex|[üçã, üçã]|\n",
      "|     Bob|    [üçä]|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [\n",
    "        Row(customer=\"Alex\", orders=[\"üçã\", \"üçã\"]),\n",
    "        Row(customer=\"Bob\", orders=[\"üçä\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32d3913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|customer|      orders|\n",
      "+--------+------------+\n",
      "|    Alex|[üçã, üçã, üçá]|\n",
      "|     Bob|    [üçä, üçá]|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    array_append,\n",
    "    array_prepend,\n",
    "    array_contains,\n",
    "    array_distinct,\n",
    ")\n",
    "\n",
    "df.withColumn(\"orders\", array_append(col(\"orders\"), \"üçá\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cbac4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|customer|      orders|\n",
      "+--------+------------+\n",
      "|    Alex|[üçá, üçã, üçã]|\n",
      "|     Bob|    [üçá, üçä]|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"orders\", array_prepend(col(\"orders\"), \"üçá\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac9780aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|customer|orders|\n",
      "+--------+------+\n",
      "|    Alex|  [üçã]|\n",
      "|     Bob|  [üçä]|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"orders\", array_distinct(col(\"orders\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74db4e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+\n",
      "|customer|  orders|has_üçã|\n",
      "+--------+--------+------+\n",
      "|    Alex|[üçã, üçã]|  true|\n",
      "|     Bob|    [üçä]| false|\n",
      "+--------+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"has_üçã\", array_contains(col(\"orders\"), \"üçã\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397c95a1",
   "metadata": {},
   "source": [
    "[View other array functions](https://bit.ly/4c0txD1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c6d76",
   "metadata": {},
   "source": [
    "### Simplify Complex SQL Queries with PySpark UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75b6174",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install \"pyspark[sql]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f909f3d",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4e6b5",
   "metadata": {},
   "source": [
    "SQL queries can often become complex and challenging to comprehend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b733f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|modified_name|\n",
      "+---+-------------+\n",
      "|  1|     John doe|\n",
      "|  2|   Jane smith|\n",
      "|  3|  Bob johnson|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(\n",
    "    [(1, \"John Doe\"), (2, \"Jane Smith\"), (3, \"Bob Johnson\")], [\"id\", \"name\"]\n",
    ")\n",
    "\n",
    "# Register the DataFrame as a temporary table or view\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# Complex SQL query\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT id, CONCAT(UPPER(SUBSTRING(name, 1, 1)), LOWER(SUBSTRING(name, 2))) AS modified_name\n",
    "    FROM df\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8056521a",
   "metadata": {},
   "source": [
    "Using PySpark UDFs simplifies complex SQL queries by encapsulating complex operations into a single function call, resulting in cleaner queries. UDFs also allow for the reuse of complex logic across different queries. \n",
    "\n",
    "In the code example below, we define a UDF called `modify_name` that converts the name to uppercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59804345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/30 14:36:24 WARN SimpleFunctionRegistry: The function modify_name replaced a previously registered function.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+\n",
      "| id|modified_name|\n",
      "+---+-------------+\n",
      "|  1|     John doe|\n",
      "|  2|   Jane smith|\n",
      "|  3|  Bob johnson|\n",
      "+---+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "# Define a UDF to modify the name\n",
    "@udf(returnType=StringType())\n",
    "def modify_name(name):\n",
    "    return name[0].upper() + name[1:].lower()\n",
    "\n",
    "spark.udf.register('modify_name', modify_name)\n",
    "\n",
    "# Apply the UDF in the spark.sql query\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT id, modify_name(name) AS modified_name\n",
    "    FROM df\n",
    "\"\"\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963cf67f",
   "metadata": {},
   "source": [
    "[Learn more about PySPark UDFs](https://bit.ly/3TEYPHh)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b6d13",
   "metadata": {},
   "source": [
    "### Leverage Spark UDFs for Reusable Complex Logic in SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef1fce5",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install \"pyspark[sql]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0230a6da",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71ec91f",
   "metadata": {},
   "source": [
    "Duplicated code in SQL queries can lead to inconsistencies if changes are made to one instance of the duplicated code but not to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f892003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+--------+\n",
      "|     name|price|quantity|category|\n",
      "+---------+-----+--------+--------+\n",
      "|Product 1| 10.0|       5|  Medium|\n",
      "|Product 2| 15.0|       3|    High|\n",
      "|Product 3|  8.0|       2|     Low|\n",
      "+---------+-----+--------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|     name|category|\n",
      "+---------+--------+\n",
      "|Product 1|  Medium|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [(\"Product 1\", 10.0, 5), (\"Product 2\", 15.0, 3), (\"Product 3\", 8.0, 2)],\n",
    "    [\"name\", \"price\", \"quantity\"],\n",
    ")\n",
    "\n",
    "# Use df within Spark SQL queries\n",
    "df.createOrReplaceTempView(\"products\")\n",
    "\n",
    "# Select Statement 1\n",
    "result1 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT name, price, quantity,\n",
    "        CASE\n",
    "            WHEN price < 10.0 THEN 'Low'\n",
    "            WHEN price >= 10.0 AND price < 15.0 THEN 'Medium'\n",
    "            ELSE 'High'\n",
    "        END AS category\n",
    "    FROM products\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Select Statement 2\n",
    "result2 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT name,\n",
    "        CASE\n",
    "            WHEN price < 10.0 THEN 'Low'\n",
    "            WHEN price >= 10.0 AND price < 15.0 THEN 'Medium'\n",
    "            ELSE 'High'\n",
    "        END AS category\n",
    "    FROM products\n",
    "    WHERE quantity > 3\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "result1.show()\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ba0e79",
   "metadata": {},
   "source": [
    "Spark UDFs (User-Defined Functions) can help address these issues by encapsulating complex logic that is reused across multiple SQL queries. \n",
    "\n",
    "In the code example above, we define a UDF `assign_category_label` that assigns category labels based on price. This UDF is then reused in two different SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37f4d9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/15 09:28:11 WARN SimpleFunctionRegistry: The function assign_category_label replaced a previously registered function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------+--------+\n",
      "|     name|price|quantity|category|\n",
      "+---------+-----+--------+--------+\n",
      "|Product 1| 10.0|       5|  Medium|\n",
      "|Product 2| 15.0|       3|    High|\n",
      "|Product 3|  8.0|       2|     Low|\n",
      "+---------+-----+--------+--------+\n",
      "\n",
      "+---------+--------+\n",
      "|     name|category|\n",
      "+---------+--------+\n",
      "|Product 1|  Medium|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define UDF to assign category label based on price\n",
    "@udf(returnType=StringType())\n",
    "def assign_category_label(price):\n",
    "    if price < 10.0:\n",
    "        return \"Low\"\n",
    "    elif price >= 10.0 and price < 15.0:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"High\"\n",
    "\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"assign_category_label\", assign_category_label)\n",
    "\n",
    "# Select Statement 1\n",
    "result1 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT name, price, quantity, assign_category_label(price) AS category\n",
    "    FROM products\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Select Statement 2\n",
    "result2 = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT name, assign_category_label(price) AS category\n",
    "    FROM products\n",
    "    WHERE quantity > 3\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "result1.show()\n",
    "result2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7e800",
   "metadata": {},
   "source": [
    "### Simplify Unit Testing of SQL Queries with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f400b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install ipytest \"pyspark[sql]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1bcfd44",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954ea695",
   "metadata": {},
   "source": [
    "Testing your SQL queries helps to ensure that they are correct and functioning as intended.\n",
    "\n",
    "PySpark enables users to parameterize queries, which simplifies unit testing of SQL queries. In this example, the `df` and `amount` variables are parameterized to verify whether the `actual_df` matches the `expected_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "14d313f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest -qq\n",
    "import pytest\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def query():\n",
    "    return \"SELECT * from {df} where price > {amount} AND name LIKE '%Product%';\"\n",
    "\n",
    "\n",
    "def test_query_return_correct_number_of_rows(query):\n",
    "\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Create a sample DataFrame\n",
    "    df = spark.createDataFrame(\n",
    "        [\n",
    "            (\"Product 1\", 10.0, 5),\n",
    "            (\"Product 2\", 15.0, 3),\n",
    "            (\"Product 3\", 8.0, 2),\n",
    "        ],\n",
    "        [\"name\", \"price\", \"quantity\"],\n",
    "    )\n",
    "\n",
    "    # Execute the query\n",
    "    actual_df = spark.sql(query, df=df, amount=10)\n",
    "\n",
    "    # Assert the result\n",
    "    expected_df = spark.createDataFrame(\n",
    "        [\n",
    "            (\"Product 2\", 15.0, 3),\n",
    "        ],\n",
    "        [\"name\", \"price\", \"quantity\"],\n",
    "    )\n",
    "    assertDataFrameEqual(actual_df, expected_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102a2ba",
   "metadata": {},
   "source": [
    "### Update Multiple Columns in Spark 3.3 and Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70283a1f",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U \"pyspark[sql]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6b1afd",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "327cc772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|first_name|age|\n",
      "+----------+---+\n",
      "|   John   | 35|\n",
      "|      Jane| 28|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [(\"   John   \", 35), (\"Jane\", 28)]\n",
    "columns = [\"first_name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b37c9a",
   "metadata": {},
   "source": [
    "Prior to PySpark 3.3, appending multiple columns to a Spark DataFrame required chaining multiple `withColumn` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e38d06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+\n",
      "|first_name|age|age_after_10_years|\n",
      "+----------+---+------------------+\n",
      "|      John| 35|                45|\n",
      "|      Jane| 28|                38|\n",
      "+----------+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Before Spark 3.3 \n",
    "new_df = (df\n",
    "          .withColumn(\"first_name\", trim(col(\"first_name\")))\n",
    "          .withColumn(\"age_after_10_years\", col(\"age\") + 10)\n",
    "         )\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc42fddd",
   "metadata": {},
   "source": [
    "In PySpark 3.3 and later, you can use the withColumns method in a dictionary style to append multiple columns to a DataFrame. This syntax is more user-friendly for pandas users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae122634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+------------------+\n",
      "|first_name|age|age_after_10_years|\n",
      "+----------+---+------------------+\n",
      "|      John| 35|                45|\n",
      "|      Jane| 28|                38|\n",
      "+----------+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.withColumns(\n",
    "    {\n",
    "        \"first_name\": trim(col(\"first_name\")),\n",
    "        \"age_after_10_years\": col(\"age\") + 10,\n",
    "    }\n",
    ")\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb930f8",
   "metadata": {},
   "source": [
    "### Vectorized Operations in PySpark: pandas_udf vs Standard UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e824b",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c80193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 10:51:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94329cf7",
   "metadata": {},
   "source": [
    "Standard UDF functions process data row-by-row, resulting in Python function call overhead. \n",
    "\n",
    "In contrast, pandas_udf uses Pandas' vectorized operations to process entire columns in a single operation, significantly improving performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4633f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|val1|\n",
      "+----+\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "| 3.0|\n",
      "| 4.0|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "data = [(1.0,), (2.0,), (3.0,), (4.0,)]\n",
    "df = spark.createDataFrame(data, [\"val1\"])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcf0cdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|val1|val2|\n",
      "+----+----+\n",
      "| 1.0| 2.0|\n",
      "| 2.0| 3.0|\n",
      "| 3.0| 4.0|\n",
      "| 4.0| 5.0|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Standard UDF\n",
    "@udf('double')\n",
    "def plus_one(val):\n",
    "    return val + 1\n",
    "\n",
    "# Apply the Standard UDF\n",
    "df.withColumn('val2', plus_one(df.val1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1ec8b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|val1|val2|\n",
      "+----+----+\n",
      "| 1.0| 2.0|\n",
      "| 2.0| 3.0|\n",
      "| 3.0| 4.0|\n",
      "| 4.0| 5.0|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Pandas UDF\n",
    "@pandas_udf(\"double\")\n",
    "def pandas_plus_one(val: pd.Series) -> pd.Series:\n",
    "    return val + 1\n",
    "\n",
    "\n",
    "# Apply the Pandas UDF\n",
    "df.withColumn(\"val2\", pandas_plus_one(df.val1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab94b5b",
   "metadata": {},
   "source": [
    "[Learn more about pandas_udf](https://bit.ly/4aRBNTX)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acbf836",
   "metadata": {},
   "source": [
    "### Optimizing PySpark Queries: DataFrame API or SQL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514d9516",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install \"pyspark[sql]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "910e70e1",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4046833",
   "metadata": {},
   "source": [
    "PySpark queries with different syntax (DataFrame API or parameterized SQL) can have the same performance, as the physical plan is identical. Here is an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c4c030c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  item|price|\n",
      "+------+-----+\n",
      "| apple|    4|\n",
      "|orange|    3|\n",
      "|banana|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "fruits = spark.createDataFrame(\n",
    "    [(\"apple\", 4), (\"orange\", 3), (\"banana\", 2)], [\"item\", \"price\"]\n",
    ")\n",
    "fruits.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f82fd1e",
   "metadata": {},
   "source": [
    "Use the DataFrame API to filter rows where the price is greater than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b693607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(price#80L) AND (price#80L > 3))\n",
      "+- *(1) Scan ExistingRDD[item#79,price#80L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fruits.where(col(\"price\") > 3).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d5dc7",
   "metadata": {},
   "source": [
    "Use the spark.sql() method to execute an equivalent SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b5500a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(price#80L) AND (price#80L > 3))\n",
      "+- *(1) Scan ExistingRDD[item#79,price#80L]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from {df} where price > 3\", df=fruits).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b2068",
   "metadata": {},
   "source": [
    "The physical plan for both queries is the same, indicating identical performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b065a14",
   "metadata": {},
   "source": [
    "Thus, the choice between DataFrame API and spark.sql() depends on the following:\n",
    "- **Familiarity**: Use spark.sql() if your team prefers SQL syntax. Use the DataFrame API if chained method calls are more intuitive for your team.\n",
    "- **Complexity of Transformations**: The DataFrame API is more flexible for complex manipulations, while SQL is more concise for simpler queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bb85d6",
   "metadata": {},
   "source": [
    "### Enhance Code Modularity and Reusability with Temporary Views in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c6792",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U 'pyspark[sql]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab976de",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/14 09:13:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac1a0a7",
   "metadata": {},
   "source": [
    "In PySpark, temporary views are virtual tables that can be queried using SQL, enabling code reusability and modularity.\n",
    "\n",
    "To demonstrate this, let's create a PySpark DataFrame called `orders_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cf261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1001, \"John Doe\", 500.0),\n",
    "    (1002, \"Jane Smith\", 750.0),\n",
    "    (1003, \"Bob Johnson\", 300.0),\n",
    "    (1004, \"Sarah Lee\", 400.0),\n",
    "    (1005, \"Tom Wilson\", 600.0),\n",
    "]\n",
    "\n",
    "columns = [\"customer_id\", \"customer_name\", \"revenue\"]\n",
    "orders_df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf288495",
   "metadata": {},
   "source": [
    "Next, create a temporary view called `orders` from the `orders_df` DataFrame using the `createOrReplaceTempView` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019a8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "orders_df.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2a924",
   "metadata": {},
   "source": [
    "With the temporary view created, we can perform various operations on it using SQL queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f88486c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Revenue:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|total_revenue|\n",
      "+-------------+\n",
      "|       2550.0|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "Number of Orders:\n",
      "+-----------+\n",
      "|order_count|\n",
      "+-----------+\n",
      "|          5|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform operations on the temporary view\n",
    "total_revenue = spark.sql(\"SELECT SUM(revenue) AS total_revenue FROM orders\")\n",
    "order_count = spark.sql(\"SELECT COUNT(*) AS order_count FROM orders\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Total Revenue:\")\n",
    "total_revenue.show()\n",
    "\n",
    "print(\"\\nNumber of Orders:\")\n",
    "order_count.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
